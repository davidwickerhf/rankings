"""
Split the network by article.

This script splits the citation network into subnetworks based on ECHR articles.
It takes the nodes and edges CSV files generated by load.py and creates separate
JSON files for each article.

Usage:
    python split.py --input_path EHCR --save_path split_networks [--articles 3 6 8]

Arguments:
    --input_path: Directory containing the nodes.csv and edges.csv files
    --save_path: Directory to save the split network files
    --articles: Optional list of specific articles to split by. If not provided,
               will split by all articles found in the data.

Example:
    # Split by all articles
    python analyze/split.py --input_path data/ECHR --output_path data/split  --min_cases 50
    python split.py --input_path EHCR --save_path split_networks

    # Split only articles 3, 6 and 8
    python split.py --input_path EHCR --save_path split_networks --articles 3 6 8

The script will create a subdirectory for each article containing:
- nodes.json: Nodes that cite the article
- edges.json: Edges between nodes that cite the article
"""

import pandas as pd
import os
import argparse
from typing import List, Set
import json

def get_unique_articles(df: pd.DataFrame) -> Set[str]:
    """
    Extract unique articles from the article column, splitting combined articles.
    
    Args:
        df: DataFrame containing an 'article' column
    Returns:
        Set of unique article numbers
    """
    all_articles = set()
    
    # Filter out rows where article is NaN
    df = df[df['article'].notna()]
    
    # Split articles and add to set
    for articles_str in df['article']:
        # Split on semicolon if multiple articles
        articles = articles_str.split(';')
        all_articles.update(articles)
    
    return all_articles

def filter_by_article(nodes_df: pd.DataFrame, edges_df: pd.DataFrame, article: str) -> tuple[pd.DataFrame, pd.DataFrame]:
    """
    Filter nodes and edges for a specific article.
    
    Args:
        nodes_df: DataFrame containing node information
        edges_df: DataFrame containing edge information
        article: Article number to filter by
    Returns:
        Tuple of filtered (nodes_df, edges_df)
    """
    # Reference the filter_article function from load.py
    filtered_nodes = nodes_df[nodes_df['article'].str.contains(article, na=False)]
    
    # Get list of valid ECLIs for this article
    valid_eclis = set(filtered_nodes['ecli'].values)
    
    # Filter edges to only include connections between nodes in this article
    filtered_edges = edges_df[edges_df['ecli'].isin(valid_eclis)].copy()
    
    # Filter references to only include valid ECLIs
    filtered_edges['references'] = filtered_edges['references'].apply(
        lambda refs: [ref for ref in refs if ref in valid_eclis]
    )
    
    return filtered_nodes, filtered_edges

def save_as_json(df: pd.DataFrame, filepath: str):
    """
    Save DataFrame as JSON in a format compatible with rankings.ipynb.
    
    Args:
        df: DataFrame to save
        filepath: Path where to save the JSON file
    """
    # Convert DataFrame to list of records
    records = df.to_dict(orient='records')
    
    # Handle special case for references column in edges
    if 'references' in df.columns:
        for record in records:
            # Ensure references is a list
            if isinstance(record['references'], str):
                record['references'] = eval(record['references'])
    
    # Save with proper formatting
    with open(filepath, 'w') as f:
        json.dump(records, f, indent=2)

def main():
    parser = argparse.ArgumentParser(description="Split ECHR network by articles")
    parser.add_argument("--input_path", type=str, required=True, help="Directory containing nodes.csv and edges.csv")
    parser.add_argument("--output_path", type=str, required=True, help="Directory to save article-specific networks")
    parser.add_argument("--min_cases", type=int, default=50, help="Minimum number of cases required for an article network")
    args = parser.parse_args()

    # Read input files
    nodes_df = pd.read_csv(os.path.join(args.input_path, 'nodes.csv'))
    edges_df = pd.read_csv(os.path.join(args.input_path, 'edges.csv'))
    
    # Convert string representation of list to actual list in edges_df
    edges_df['references'] = edges_df['references'].apply(eval)
    
    # Get unique articles
    unique_articles = get_unique_articles(nodes_df)
    print(f"Found {len(unique_articles)} unique articles")
    
    # Create output directory
    os.makedirs(args.output_path, exist_ok=True)
    
    # Process each article
    for article in unique_articles:
        # Filter data for this article
        article_nodes, article_edges = filter_by_article(nodes_df, edges_df, article)
        
        # Check if network meets minimum size requirement
        if len(article_nodes) >= args.min_cases:
            # Create article-specific directory
            article_dir = os.path.join(args.output_path, f"article_{article.replace('/', '_')}")
            os.makedirs(article_dir, exist_ok=True)
            
            # Save filtered data as JSON
            save_as_json(article_nodes, os.path.join(article_dir, 'nodes.json'))
            save_as_json(article_edges, os.path.join(article_dir, 'edges.json'))
            
            print(f"Created network for Article {article} with {len(article_nodes)} nodes")
        else:
            print(f"Skipping Article {article} - only {len(article_nodes)} cases (minimum: {args.min_cases})")

if __name__ == "__main__":
    main()