{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:188: SyntaxWarning: invalid escape sequence '\\+'\n",
      "<>:188: SyntaxWarning: invalid escape sequence '\\+'\n",
      "<>:188: SyntaxWarning: invalid escape sequence '\\+'\n",
      "<>:188: SyntaxWarning: invalid escape sequence '\\+'\n",
      "/var/folders/26/0sr3rdx53314n7l3fzj55k6r0000gn/T/ipykernel_42035/3685315357.py:188: SyntaxWarning: invalid escape sequence '\\+'\n",
      "  pattern = f\"(^|;|\\+){article}(-|$|;|\\+)\"\n",
      "/var/folders/26/0sr3rdx53314n7l3fzj55k6r0000gn/T/ipykernel_42035/3685315357.py:188: SyntaxWarning: invalid escape sequence '\\+'\n",
      "  pattern = f\"(^|;|\\+){article}(-|$|;|\\+)\"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from typing import List, Set, Optional\n",
    "import json\n",
    "from typing import Union\n",
    "\n",
    "def read_nodes_file(nodes_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Read nodes data from either CSV or JSON file.\n",
    "    \n",
    "    Args:\n",
    "        nodes_path: Path to the nodes file (either .csv or .json)\n",
    "        \n",
    "    Returns:\n",
    "        pandas DataFrame containing the nodes data\n",
    "    \"\"\"\n",
    "    file_extension = nodes_path.split('.')[-1].lower()\n",
    "    \n",
    "    if file_extension == 'csv':\n",
    "        nodes_df = pd.read_csv(nodes_path)\n",
    "    elif file_extension == 'json':\n",
    "        nodes_df = pd.read_json(nodes_path)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file format: {file_extension}. Please use CSV or JSON.\")\n",
    "        \n",
    "    return nodes_df\n",
    "\n",
    "def process_network(\n",
    "    nodes: Union[str, pd.DataFrame],\n",
    "    edges: Union[str, pd.DataFrame],\n",
    "    output_path: Optional[str] = None,\n",
    "    min_cases: int = 50,\n",
    "    specific_articles: Optional[List[str]] = None,\n",
    "    split_by_article: bool = True,\n",
    "    save_files: bool = False,\n",
    "    merge_subarticles: bool = False\n",
    ") -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Process and optionally split the citation network based on ECHR articles.\n",
    "    \n",
    "    Args:\n",
    "        nodes: Path to nodes CSV file or nodes DataFrame\n",
    "        edges: Path to edges CSV file or edges DataFrame\n",
    "        output_path: Directory to save the network files (required if save_files=True)\n",
    "        min_cases: Minimum number of cases required for an article network\n",
    "        specific_articles: Optional list of specific articles to split by\n",
    "        split_by_article: Whether to split the network by article\n",
    "        save_files: Whether to save the processed network(s) to files\n",
    "    \n",
    "    Returns:\n",
    "        tuple[pd.DataFrame, pd.DataFrame]: The processed (nodes_df, edges_df)\n",
    "    \"\"\"\n",
    "    # Validate arguments\n",
    "    if save_files and not output_path:\n",
    "        raise ValueError(\"output_path is required when save_files=True\")\n",
    "\n",
    "    # Handle input data\n",
    "    if isinstance(nodes, str):\n",
    "        print(f\"Reading nodes from {nodes}...\")\n",
    "        nodes_df = read_nodes_file(nodes)\n",
    "    else:\n",
    "        nodes_df = nodes.copy()\n",
    "        \n",
    "    if isinstance(edges, str):\n",
    "        print(f\"Reading edges from {edges}...\")\n",
    "        edges_df = read_nodes_file(edges)\n",
    "    else:\n",
    "        edges_df = edges.copy()\n",
    "    \n",
    "    # Convert string representation of list to actual list in edges_df\n",
    "    edges_df['references'] = edges_df['references'].apply(\n",
    "        lambda x: eval(x) if isinstance(x, str) else x\n",
    "    )\n",
    "    \n",
    "    if not split_by_article:\n",
    "        if save_files:\n",
    "            os.makedirs(output_path, exist_ok=True)\n",
    "            full_dir = os.path.join(output_path, 'full')\n",
    "            os.makedirs(full_dir, exist_ok=True)\n",
    "            save_as_json(nodes_df, os.path.join(full_dir, 'nodes.json'))\n",
    "            save_as_json(edges_df, os.path.join(full_dir, 'edges.json'))\n",
    "            print(f\"Saved processed network to {output_path}\")\n",
    "        return nodes_df, edges_df\n",
    "    \n",
    "    # Get unique articles\n",
    "    if merge_subarticles:\n",
    "        unique_articles = get_unique_articles_updated(nodes_df)\n",
    "    else:\n",
    "        unique_articles = get_unique_articles(nodes_df)\n",
    "\n",
    "    if specific_articles:\n",
    "        unique_articles = {art for art in unique_articles if art in specific_articles}\n",
    "    print(f\"Found {len(unique_articles)} unique articles\")\n",
    "    \n",
    "    if save_files:\n",
    "        # Create output directory\n",
    "        os.makedirs(output_path, exist_ok=True)\n",
    "        \n",
    "        # Process each article\n",
    "        for article in unique_articles:\n",
    "            # Filter data for this article\n",
    "            article_nodes, article_edges = filter_by_article(nodes_df, edges_df, article)\n",
    "            \n",
    "            # Check if network meets minimum size requirement\n",
    "            if len(article_nodes) >= min_cases:\n",
    "                # Create article-specific directory\n",
    "                article_dir = os.path.join(output_path, f\"article_{article.replace('/', '_')}\")\n",
    "                os.makedirs(article_dir, exist_ok=True)\n",
    "                \n",
    "                # Save filtered data as JSON\n",
    "                save_as_json(article_nodes, os.path.join(article_dir, 'nodes.json'))\n",
    "                save_as_json(article_edges, os.path.join(article_dir, 'edges.json'))\n",
    "                \n",
    "                print(f\"Created network for Article {article} with {len(article_nodes)} nodes\")\n",
    "            else:\n",
    "                print(f\"Skipping Article {article} - only {len(article_nodes)} cases (minimum: {min_cases})\")\n",
    "    \n",
    "    return nodes_df, edges_df\n",
    "\n",
    "def get_unique_articles(df: pd.DataFrame) -> Set[str]:\n",
    "    \"\"\"\n",
    "    Extract unique articles from the article column, splitting combined articles.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame containing an 'article' column\n",
    "    Returns:\n",
    "        Set of unique article numbers\n",
    "    \"\"\"\n",
    "    all_articles = set()\n",
    "    \n",
    "    # Filter out rows where article is NaN\n",
    "    df = df[df['article'].notna()]\n",
    "    \n",
    "    # Split articles and add to set\n",
    "    for articles_str in df['article']:\n",
    "        # Split on semicolon if multiple articles\n",
    "        articles = articles_str.split(';')\n",
    "        # Filter out empty strings and strip whitespace\n",
    "        articles = {art.strip() for art in articles if art.strip()}\n",
    "        all_articles.update(articles)\n",
    "    \n",
    "    return all_articles\n",
    "\n",
    "def get_unique_articles_updated(df: pd.DataFrame) -> Set[str]:\n",
    "    \"\"\"\n",
    "    Extract unique articles from the article column, treating sub-articles as their main article.\n",
    "    Also handles '+' separated articles.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame containing an 'article' column\n",
    "    Returns:\n",
    "        Set of unique article numbers\n",
    "    \"\"\"\n",
    "    all_articles = set()\n",
    "    \n",
    "    # Filter out rows where article is NaN\n",
    "    df = df[df['article'].notna()]\n",
    "    \n",
    "    # Split articles and add to set\n",
    "    for articles_str in df['article']:\n",
    "        # Split on semicolon if multiple articles\n",
    "        articles = articles_str.split(';')\n",
    "        # Split on plus if articles are combined\n",
    "        articles = [art.split('+') for art in articles]\n",
    "        # Flatten the list\n",
    "        articles = [item.strip() for sublist in articles for item in sublist]\n",
    "        # Filter out empty strings and strip whitespace\n",
    "        articles = {art.strip() for art in articles if art.strip()}\n",
    "        # Extract main article numbers (before any dash)\n",
    "        main_articles = {art.split('-')[0] for art in articles}\n",
    "        all_articles.update(main_articles)\n",
    "    \n",
    "    return all_articles\n",
    "\n",
    "def filter_by_article(nodes_df: pd.DataFrame, edges_df: pd.DataFrame, article) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Filter nodes and edges for a specific article.\n",
    "    Handles cases where articles are combined with '+'.\n",
    "    \n",
    "    Args:\n",
    "        nodes_df: DataFrame containing node information\n",
    "        edges_df: DataFrame containing edge information\n",
    "        article: Article number to filter by\n",
    "    Returns:\n",
    "        Tuple of filtered (nodes_df, edges_df)\n",
    "    \"\"\"\n",
    "    # Create pattern to match article as main or sub-article, including when combined with '+'\n",
    "    pattern = f\"(^|;|\\+){article}(-|$|;|\\+)\"\n",
    "    \n",
    "    # Reference the filter_article function from load.py\n",
    "    filtered_nodes = nodes_df[nodes_df['article'].str.contains(pattern, na=False, regex=True)]\n",
    "    \n",
    "    # Get list of valid ECLIs for this article\n",
    "    valid_eclis = set(filtered_nodes['ecli'].values)\n",
    "    \n",
    "    # Filter edges to only include connections between nodes in this article\n",
    "    filtered_edges = edges_df[edges_df['ecli'].isin(valid_eclis)].copy()\n",
    "    \n",
    "    # Filter references to only include valid ECLIs\n",
    "    filtered_edges['references'] = filtered_edges['references'].apply(\n",
    "        lambda refs: [ref for ref in refs if ref in valid_eclis]\n",
    "    )\n",
    "    \n",
    "    return filtered_nodes, filtered_edges\n",
    "\n",
    "def save_as_json(df: pd.DataFrame, filepath: str):\n",
    "    \"\"\"\n",
    "    Save DataFrame as JSON in a format compatible with rankings.ipynb.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame to save\n",
    "        filepath: Path where to save the JSON file\n",
    "    \"\"\"\n",
    "    # Convert DataFrame to list of records\n",
    "    records = df.to_dict(orient='records')\n",
    "    \n",
    "    # Handle special case for references column in edges\n",
    "    if 'references' in df.columns:\n",
    "        for record in records:\n",
    "            # Ensure references is a list\n",
    "            if isinstance(record['references'], str):\n",
    "                record['references'] = eval(record['references'])\n",
    "    \n",
    "    # Save with proper formatting\n",
    "    # Create directory if it doesn't exist\n",
    "    os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
    "    \n",
    "    with open(filepath, 'w') as f:\n",
    "        json.dump(records, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse and load unbalanced network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Balance network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_network_by_parameter(\n",
    "    nodes_df: pd.DataFrame, \n",
    "    edges_df: pd.DataFrame, \n",
    "    balance_parameter: str\n",
    ") -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Balance network by reducing all classes of a parameter to match the size of the smallest class.\n",
    "    \n",
    "    Args:\n",
    "        nodes_df: DataFrame containing node information\n",
    "        edges_df: DataFrame containing edge information\n",
    "        balance_parameter: Column name in nodes_df to balance by\n",
    "        \n",
    "    Returns:\n",
    "        tuple[pd.DataFrame, pd.DataFrame]: Balanced (nodes_df, edges_df)\n",
    "    \"\"\"\n",
    "    # Get class sizes\n",
    "    class_sizes = nodes_df[balance_parameter].value_counts()\n",
    "    min_size = class_sizes.min()\n",
    "    \n",
    "    print(f\"Balancing network by {balance_parameter}\")\n",
    "    print(f\"Original class distribution: {class_sizes.to_dict()}\")\n",
    "    print(f\"Target size per class: {min_size}\")\n",
    "    \n",
    "    # Create balanced nodes dataframe\n",
    "    balanced_nodes = pd.DataFrame()\n",
    "    for class_value in class_sizes.index:\n",
    "        # Get all nodes of this class\n",
    "        class_nodes = nodes_df[nodes_df[balance_parameter] == class_value]\n",
    "        \n",
    "        # Randomly sample to match minimum size\n",
    "        sampled_nodes = class_nodes.sample(n=min_size, random_state=42)\n",
    "        balanced_nodes = pd.concat([balanced_nodes, sampled_nodes])\n",
    "    \n",
    "    # Get valid ECLIs after balancing\n",
    "    valid_eclis = set(balanced_nodes['ecli'].values)\n",
    "    \n",
    "    # Filter edges to only include connections between remaining nodes\n",
    "    balanced_edges = edges_df[edges_df['ecli'].isin(valid_eclis)].copy()\n",
    "    \n",
    "    # Filter references to only include valid ECLIs\n",
    "    balanced_edges['references'] = balanced_edges['references'].apply(\n",
    "        lambda refs: [ref for ref in refs if ref in valid_eclis]\n",
    "    )\n",
    "    \n",
    "    print(f\"Final network size: {len(balanced_nodes)} nodes and {len(balanced_edges)} edges\")\n",
    "    \n",
    "    return balanced_nodes, balanced_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_total_edges(edges_df: pd.DataFrame) -> int:\n",
    "    \"\"\"\n",
    "    Count total number of edges in the network, considering multiple targets per entry.\n",
    "    \n",
    "    Args:\n",
    "        edges_df: DataFrame containing edges with 'target' column that may contain multiple targets\n",
    "        \n",
    "    Returns:\n",
    "        int: Total number of edges in the network\n",
    "    \"\"\"\n",
    "    total_edges = 0\n",
    "    \n",
    "    # Iterate through each row\n",
    "    for _, row in edges_df.iterrows():\n",
    "        # Handle references column\n",
    "        if isinstance(row['references'], str):\n",
    "            # Clean the string and convert to list\n",
    "            refs_str = row['references'].strip('[]').replace(\"'\", \"\").replace('\"', \"\")\n",
    "            # Split by comma and clean each ECLI\n",
    "            references = [ref.strip() for ref in refs_str.split(',') if ref.strip()]\n",
    "            # Count valid ECLI references\n",
    "            edge_count = len([ref for ref in references if ref.startswith('ECLI:')])\n",
    "            total_edges += edge_count\n",
    "        elif isinstance(row['references'], list):\n",
    "            # If references is already a list\n",
    "            edge_count = len([ref for ref in row['references'] if isinstance(ref, str) and ref.startswith('ECLI:')])\n",
    "            total_edges += edge_count\n",
    "            \n",
    "    return total_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading nodes from ../data/METADATA/nodes.csv...\n",
      "Reading edges from ../data/METADATA/edges.csv...\n",
      "Number of nodes: 27801\n",
      "Number of edges: 232002\n",
      "Balancing network by importance\n",
      "Original class distribution: {3.0: 19895, 2.0: 5756, 1.0: 2150}\n",
      "Target size per class: 2150\n",
      "Final network size: 6450 nodes and 6450 edges\n",
      "Balancing network by doctypebranch\n",
      "Original class distribution: {'CHAMBER': 20717, 'COMMITTEE': 6570, 'GRANDCHAMBER': 514}\n",
      "Target size per class: 514\n",
      "Final network size: 1542 nodes and 1542 edges\n",
      "Found 48 unique articles\n",
      "Skipping Article 33 - only 8 cases (minimum: 50)\n",
      "Created network for Article 39 with 106 nodes\n",
      "Skipping Article 27 - only 3 cases (minimum: 50)\n",
      "Skipping Article 4 - only 34 cases (minimum: 50)\n",
      "Skipping Article 32 - only 6 cases (minimum: 50)\n",
      "Created network for Article P7 with 85 nodes\n",
      "Skipping Article 30 - only 11 cases (minimum: 50)\n",
      "Created network for Article P1 with 1021 nodes\n",
      "Skipping Article 43 - only 11 cases (minimum: 50)\n",
      "Skipping Article P13 - only 1 cases (minimum: 50)\n",
      "Created network for Article 10 with 603 nodes\n",
      "Created network for Article 8 with 1126 nodes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/26/0sr3rdx53314n7l3fzj55k6r0000gn/T/ipykernel_42035/3685315357.py:191: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  filtered_nodes = nodes_df[nodes_df['article'].str.contains(pattern, na=False, regex=True)]\n",
      "/var/folders/26/0sr3rdx53314n7l3fzj55k6r0000gn/T/ipykernel_42035/3685315357.py:191: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  filtered_nodes = nodes_df[nodes_df['article'].str.contains(pattern, na=False, regex=True)]\n",
      "/var/folders/26/0sr3rdx53314n7l3fzj55k6r0000gn/T/ipykernel_42035/3685315357.py:191: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  filtered_nodes = nodes_df[nodes_df['article'].str.contains(pattern, na=False, regex=True)]\n",
      "/var/folders/26/0sr3rdx53314n7l3fzj55k6r0000gn/T/ipykernel_42035/3685315357.py:191: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  filtered_nodes = nodes_df[nodes_df['article'].str.contains(pattern, na=False, regex=True)]\n",
      "/var/folders/26/0sr3rdx53314n7l3fzj55k6r0000gn/T/ipykernel_42035/3685315357.py:191: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  filtered_nodes = nodes_df[nodes_df['article'].str.contains(pattern, na=False, regex=True)]\n",
      "/var/folders/26/0sr3rdx53314n7l3fzj55k6r0000gn/T/ipykernel_42035/3685315357.py:191: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  filtered_nodes = nodes_df[nodes_df['article'].str.contains(pattern, na=False, regex=True)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created network for Article 41 with 3595 nodes\n",
      "Created network for Article 37 with 229 nodes\n",
      "Created network for Article 13 with 948 nodes\n",
      "Created network for Article 1 with 66 nodes\n",
      "Created network for Article 2 with 427 nodes\n",
      "Created network for Article 9 with 125 nodes\n",
      "Created network for Article 38 with 114 nodes\n",
      "Created network for Article 11 with 213 nodes\n",
      "Skipping Article 44 - only 1 cases (minimum: 50)\n",
      "Skipping Article 26 - only 14 cases (minimum: 50)\n",
      "Skipping Article 56 - only 8 cases (minimum: 50)\n",
      "Skipping Article P6 - only 2 cases (minimum: 50)\n",
      "Skipping Article 17 - only 32 cases (minimum: 50)\n",
      "Skipping Article 19 - only 10 cases (minimum: 50)\n",
      "Skipping Article 57 - only 19 cases (minimum: 50)\n",
      "Skipping Article 25 - only 18 cases (minimum: 50)\n",
      "Created network for Article 35 with 1585 nodes\n",
      "Skipping Article 36 - only 39 cases (minimum: 50)\n",
      "Skipping Article P12 - only 10 cases (minimum: 50)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/26/0sr3rdx53314n7l3fzj55k6r0000gn/T/ipykernel_42035/3685315357.py:191: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  filtered_nodes = nodes_df[nodes_df['article'].str.contains(pattern, na=False, regex=True)]\n",
      "/var/folders/26/0sr3rdx53314n7l3fzj55k6r0000gn/T/ipykernel_42035/3685315357.py:191: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  filtered_nodes = nodes_df[nodes_df['article'].str.contains(pattern, na=False, regex=True)]\n",
      "/var/folders/26/0sr3rdx53314n7l3fzj55k6r0000gn/T/ipykernel_42035/3685315357.py:191: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  filtered_nodes = nodes_df[nodes_df['article'].str.contains(pattern, na=False, regex=True)]\n",
      "/var/folders/26/0sr3rdx53314n7l3fzj55k6r0000gn/T/ipykernel_42035/3685315357.py:191: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  filtered_nodes = nodes_df[nodes_df['article'].str.contains(pattern, na=False, regex=True)]\n",
      "/var/folders/26/0sr3rdx53314n7l3fzj55k6r0000gn/T/ipykernel_42035/3685315357.py:191: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  filtered_nodes = nodes_df[nodes_df['article'].str.contains(pattern, na=False, regex=True)]\n",
      "/var/folders/26/0sr3rdx53314n7l3fzj55k6r0000gn/T/ipykernel_42035/3685315357.py:191: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  filtered_nodes = nodes_df[nodes_df['article'].str.contains(pattern, na=False, regex=True)]\n",
      "/var/folders/26/0sr3rdx53314n7l3fzj55k6r0000gn/T/ipykernel_42035/3685315357.py:191: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  filtered_nodes = nodes_df[nodes_df['article'].str.contains(pattern, na=False, regex=True)]\n",
      "/var/folders/26/0sr3rdx53314n7l3fzj55k6r0000gn/T/ipykernel_42035/3685315357.py:191: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  filtered_nodes = nodes_df[nodes_df['article'].str.contains(pattern, na=False, regex=True)]\n",
      "/var/folders/26/0sr3rdx53314n7l3fzj55k6r0000gn/T/ipykernel_42035/3685315357.py:191: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  filtered_nodes = nodes_df[nodes_df['article'].str.contains(pattern, na=False, regex=True)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created network for Article 5 with 981 nodes\n",
      "Created network for Article 29 with 557 nodes\n",
      "Created network for Article P4 with 80 nodes\n",
      "Skipping Article 52 - only 2 cases (minimum: 50)\n",
      "Created network for Article 18 with 62 nodes\n",
      "Created network for Article 6 with 2987 nodes\n",
      "Created network for Article 46 with 204 nodes\n",
      "Created network for Article 14 with 528 nodes\n",
      "Skipping Article 28 - only 2 cases (minimum: 50)\n",
      "Skipping Article 53 - only 4 cases (minimum: 50)\n",
      "Created network for Article 34 with 531 nodes\n",
      "Skipping Article 12 - only 26 cases (minimum: 50)\n",
      "Skipping Article 16 - only 2 cases (minimum: 50)\n",
      "Created network for Article 7 with 92 nodes\n",
      "Skipping Article 58 - only 14 cases (minimum: 50)\n",
      "Skipping Article 15 - only 29 cases (minimum: 50)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/26/0sr3rdx53314n7l3fzj55k6r0000gn/T/ipykernel_42035/3685315357.py:191: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  filtered_nodes = nodes_df[nodes_df['article'].str.contains(pattern, na=False, regex=True)]\n",
      "/var/folders/26/0sr3rdx53314n7l3fzj55k6r0000gn/T/ipykernel_42035/3685315357.py:191: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  filtered_nodes = nodes_df[nodes_df['article'].str.contains(pattern, na=False, regex=True)]\n",
      "/var/folders/26/0sr3rdx53314n7l3fzj55k6r0000gn/T/ipykernel_42035/3685315357.py:191: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  filtered_nodes = nodes_df[nodes_df['article'].str.contains(pattern, na=False, regex=True)]\n",
      "/var/folders/26/0sr3rdx53314n7l3fzj55k6r0000gn/T/ipykernel_42035/3685315357.py:191: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  filtered_nodes = nodes_df[nodes_df['article'].str.contains(pattern, na=False, regex=True)]\n",
      "/var/folders/26/0sr3rdx53314n7l3fzj55k6r0000gn/T/ipykernel_42035/3685315357.py:191: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  filtered_nodes = nodes_df[nodes_df['article'].str.contains(pattern, na=False, regex=True)]\n",
      "/var/folders/26/0sr3rdx53314n7l3fzj55k6r0000gn/T/ipykernel_42035/3685315357.py:191: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  filtered_nodes = nodes_df[nodes_df['article'].str.contains(pattern, na=False, regex=True)]\n",
      "/var/folders/26/0sr3rdx53314n7l3fzj55k6r0000gn/T/ipykernel_42035/3685315357.py:191: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  filtered_nodes = nodes_df[nodes_df['article'].str.contains(pattern, na=False, regex=True)]\n",
      "/var/folders/26/0sr3rdx53314n7l3fzj55k6r0000gn/T/ipykernel_42035/3685315357.py:191: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  filtered_nodes = nodes_df[nodes_df['article'].str.contains(pattern, na=False, regex=True)]\n",
      "/var/folders/26/0sr3rdx53314n7l3fzj55k6r0000gn/T/ipykernel_42035/3685315357.py:191: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  filtered_nodes = nodes_df[nodes_df['article'].str.contains(pattern, na=False, regex=True)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created network for Article 3 with 1082 nodes\n",
      "Found 44 unique articles\n",
      "Skipping Article 33 - only 5 cases (minimum: 50)\n",
      "Skipping Article 39 - only 25 cases (minimum: 50)\n",
      "Skipping Article 4 - only 5 cases (minimum: 50)\n",
      "Skipping Article 30 - only 11 cases (minimum: 50)\n",
      "Skipping Article P7 - only 31 cases (minimum: 50)\n",
      "Skipping Article 32 - only 1 cases (minimum: 50)\n",
      "Created network for Article P1 with 286 nodes\n",
      "Skipping Article 43 - only 11 cases (minimum: 50)\n",
      "Created network for Article 10 with 142 nodes\n",
      "Created network for Article 8 with 230 nodes\n",
      "Created network for Article 41 with 544 nodes\n",
      "Created network for Article 13 with 229 nodes\n",
      "Created network for Article 37 with 78 nodes\n",
      "Skipping Article 1 - only 31 cases (minimum: 50)\n",
      "Created network for Article 2 with 87 nodes\n",
      "Skipping Article 9 - only 33 cases (minimum: 50)\n",
      "Skipping Article 38 - only 19 cases (minimum: 50)\n",
      "Created network for Article 11 with 66 nodes\n",
      "Skipping Article 26 - only 2 cases (minimum: 50)\n",
      "Skipping Article 56 - only 2 cases (minimum: 50)\n",
      "Skipping Article P6 - only 1 cases (minimum: 50)\n",
      "Skipping Article 17 - only 5 cases (minimum: 50)\n",
      "Skipping Article 19 - only 7 cases (minimum: 50)\n",
      "Skipping Article 57 - only 1 cases (minimum: 50)\n",
      "Skipping Article 36 - only 20 cases (minimum: 50)\n",
      "Created network for Article 35 with 268 nodes\n",
      "Skipping Article 25 - only 2 cases (minimum: 50)\n",
      "Skipping Article P12 - only 2 cases (minimum: 50)\n",
      "Created network for Article 5 with 261 nodes\n",
      "Created network for Article 29 with 74 nodes\n",
      "Skipping Article P4 - only 15 cases (minimum: 50)\n",
      "Skipping Article 18 - only 16 cases (minimum: 50)\n",
      "Created network for Article 6 with 730 nodes\n",
      "Created network for Article 46 with 51 nodes\n",
      "Created network for Article 14 with 105 nodes\n",
      "Skipping Article 28 - only 2 cases (minimum: 50)\n",
      "Skipping Article 53 - only 2 cases (minimum: 50)\n",
      "Created network for Article 34 with 104 nodes\n",
      "Skipping Article 12 - only 7 cases (minimum: 50)\n",
      "Skipping Article 16 - only 1 cases (minimum: 50)\n",
      "Skipping Article 7 - only 28 cases (minimum: 50)\n",
      "Skipping Article 58 - only 1 cases (minimum: 50)\n",
      "Skipping Article 15 - only 8 cases (minimum: 50)\n",
      "Created network for Article 3 with 272 nodes\n",
      "Found 48 unique articles\n",
      "Skipping Article 33 - only 10 cases (minimum: 50)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/26/0sr3rdx53314n7l3fzj55k6r0000gn/T/ipykernel_42035/3685315357.py:191: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  filtered_nodes = nodes_df[nodes_df['article'].str.contains(pattern, na=False, regex=True)]\n",
      "/var/folders/26/0sr3rdx53314n7l3fzj55k6r0000gn/T/ipykernel_42035/3685315357.py:191: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  filtered_nodes = nodes_df[nodes_df['article'].str.contains(pattern, na=False, regex=True)]\n",
      "/var/folders/26/0sr3rdx53314n7l3fzj55k6r0000gn/T/ipykernel_42035/3685315357.py:191: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  filtered_nodes = nodes_df[nodes_df['article'].str.contains(pattern, na=False, regex=True)]\n",
      "/var/folders/26/0sr3rdx53314n7l3fzj55k6r0000gn/T/ipykernel_42035/3685315357.py:191: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  filtered_nodes = nodes_df[nodes_df['article'].str.contains(pattern, na=False, regex=True)]\n",
      "/var/folders/26/0sr3rdx53314n7l3fzj55k6r0000gn/T/ipykernel_42035/3685315357.py:191: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  filtered_nodes = nodes_df[nodes_df['article'].str.contains(pattern, na=False, regex=True)]\n",
      "/var/folders/26/0sr3rdx53314n7l3fzj55k6r0000gn/T/ipykernel_42035/3685315357.py:191: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  filtered_nodes = nodes_df[nodes_df['article'].str.contains(pattern, na=False, regex=True)]\n",
      "/var/folders/26/0sr3rdx53314n7l3fzj55k6r0000gn/T/ipykernel_42035/3685315357.py:191: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  filtered_nodes = nodes_df[nodes_df['article'].str.contains(pattern, na=False, regex=True)]\n",
      "/var/folders/26/0sr3rdx53314n7l3fzj55k6r0000gn/T/ipykernel_42035/3685315357.py:191: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  filtered_nodes = nodes_df[nodes_df['article'].str.contains(pattern, na=False, regex=True)]\n",
      "/var/folders/26/0sr3rdx53314n7l3fzj55k6r0000gn/T/ipykernel_42035/3685315357.py:191: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  filtered_nodes = nodes_df[nodes_df['article'].str.contains(pattern, na=False, regex=True)]\n",
      "/var/folders/26/0sr3rdx53314n7l3fzj55k6r0000gn/T/ipykernel_42035/3685315357.py:191: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  filtered_nodes = nodes_df[nodes_df['article'].str.contains(pattern, na=False, regex=True)]\n",
      "/var/folders/26/0sr3rdx53314n7l3fzj55k6r0000gn/T/ipykernel_42035/3685315357.py:191: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  filtered_nodes = nodes_df[nodes_df['article'].str.contains(pattern, na=False, regex=True)]\n",
      "/var/folders/26/0sr3rdx53314n7l3fzj55k6r0000gn/T/ipykernel_42035/3685315357.py:191: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  filtered_nodes = nodes_df[nodes_df['article'].str.contains(pattern, na=False, regex=True)]\n",
      "/var/folders/26/0sr3rdx53314n7l3fzj55k6r0000gn/T/ipykernel_42035/3685315357.py:191: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  filtered_nodes = nodes_df[nodes_df['article'].str.contains(pattern, na=False, regex=True)]\n",
      "/var/folders/26/0sr3rdx53314n7l3fzj55k6r0000gn/T/ipykernel_42035/3685315357.py:191: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  filtered_nodes = nodes_df[nodes_df['article'].str.contains(pattern, na=False, regex=True)]\n",
      "/var/folders/26/0sr3rdx53314n7l3fzj55k6r0000gn/T/ipykernel_42035/3685315357.py:191: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  filtered_nodes = nodes_df[nodes_df['article'].str.contains(pattern, na=False, regex=True)]\n",
      "/var/folders/26/0sr3rdx53314n7l3fzj55k6r0000gn/T/ipykernel_42035/3685315357.py:191: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  filtered_nodes = nodes_df[nodes_df['article'].str.contains(pattern, na=False, regex=True)]\n",
      "/var/folders/26/0sr3rdx53314n7l3fzj55k6r0000gn/T/ipykernel_42035/3685315357.py:191: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  filtered_nodes = nodes_df[nodes_df['article'].str.contains(pattern, na=False, regex=True)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created network for Article 39 with 852 nodes\n",
      "Skipping Article 27 - only 4 cases (minimum: 50)\n",
      "Created network for Article 4 with 56 nodes\n",
      "Skipping Article 32 - only 7 cases (minimum: 50)\n",
      "Created network for Article P7 with 380 nodes\n",
      "Skipping Article 30 - only 12 cases (minimum: 50)\n",
      "Created network for Article P1 with 4746 nodes\n",
      "Skipping Article 43 - only 12 cases (minimum: 50)\n",
      "Skipping Article P13 - only 1 cases (minimum: 50)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/26/0sr3rdx53314n7l3fzj55k6r0000gn/T/ipykernel_42035/3685315357.py:191: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  filtered_nodes = nodes_df[nodes_df['article'].str.contains(pattern, na=False, regex=True)]\n",
      "/var/folders/26/0sr3rdx53314n7l3fzj55k6r0000gn/T/ipykernel_42035/3685315357.py:191: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  filtered_nodes = nodes_df[nodes_df['article'].str.contains(pattern, na=False, regex=True)]\n",
      "/var/folders/26/0sr3rdx53314n7l3fzj55k6r0000gn/T/ipykernel_42035/3685315357.py:191: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  filtered_nodes = nodes_df[nodes_df['article'].str.contains(pattern, na=False, regex=True)]\n",
      "/var/folders/26/0sr3rdx53314n7l3fzj55k6r0000gn/T/ipykernel_42035/3685315357.py:191: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  filtered_nodes = nodes_df[nodes_df['article'].str.contains(pattern, na=False, regex=True)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created network for Article 10 with 1691 nodes\n",
      "Created network for Article 8 with 3046 nodes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/26/0sr3rdx53314n7l3fzj55k6r0000gn/T/ipykernel_42035/3685315357.py:191: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  filtered_nodes = nodes_df[nodes_df['article'].str.contains(pattern, na=False, regex=True)]\n",
      "/var/folders/26/0sr3rdx53314n7l3fzj55k6r0000gn/T/ipykernel_42035/3685315357.py:191: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  filtered_nodes = nodes_df[nodes_df['article'].str.contains(pattern, na=False, regex=True)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created network for Article 41 with 10100 nodes\n",
      "Created network for Article 13 with 3990 nodes\n",
      "Created network for Article 1 with 102 nodes\n",
      "Created network for Article 37 with 1254 nodes\n",
      "Created network for Article 2 with 1388 nodes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/26/0sr3rdx53314n7l3fzj55k6r0000gn/T/ipykernel_42035/3685315357.py:191: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  filtered_nodes = nodes_df[nodes_df['article'].str.contains(pattern, na=False, regex=True)]\n",
      "/var/folders/26/0sr3rdx53314n7l3fzj55k6r0000gn/T/ipykernel_42035/3685315357.py:191: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  filtered_nodes = nodes_df[nodes_df['article'].str.contains(pattern, na=False, regex=True)]\n",
      "/var/folders/26/0sr3rdx53314n7l3fzj55k6r0000gn/T/ipykernel_42035/3685315357.py:191: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  filtered_nodes = nodes_df[nodes_df['article'].str.contains(pattern, na=False, regex=True)]\n",
      "/var/folders/26/0sr3rdx53314n7l3fzj55k6r0000gn/T/ipykernel_42035/3685315357.py:191: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  filtered_nodes = nodes_df[nodes_df['article'].str.contains(pattern, na=False, regex=True)]\n",
      "/var/folders/26/0sr3rdx53314n7l3fzj55k6r0000gn/T/ipykernel_42035/3685315357.py:191: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  filtered_nodes = nodes_df[nodes_df['article'].str.contains(pattern, na=False, regex=True)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created network for Article 9 with 243 nodes\n",
      "Created network for Article 38 with 594 nodes\n",
      "Created network for Article 11 with 854 nodes\n",
      "Skipping Article 44 - only 1 cases (minimum: 50)\n",
      "Skipping Article 26 - only 25 cases (minimum: 50)\n",
      "Skipping Article 56 - only 8 cases (minimum: 50)\n",
      "Skipping Article P6 - only 10 cases (minimum: 50)\n",
      "Created network for Article 17 with 67 nodes\n",
      "Skipping Article 19 - only 13 cases (minimum: 50)\n",
      "Skipping Article 57 - only 30 cases (minimum: 50)\n",
      "Skipping Article 25 - only 21 cases (minimum: 50)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/26/0sr3rdx53314n7l3fzj55k6r0000gn/T/ipykernel_42035/3685315357.py:191: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  filtered_nodes = nodes_df[nodes_df['article'].str.contains(pattern, na=False, regex=True)]\n",
      "/var/folders/26/0sr3rdx53314n7l3fzj55k6r0000gn/T/ipykernel_42035/3685315357.py:191: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  filtered_nodes = nodes_df[nodes_df['article'].str.contains(pattern, na=False, regex=True)]\n",
      "/var/folders/26/0sr3rdx53314n7l3fzj55k6r0000gn/T/ipykernel_42035/3685315357.py:191: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  filtered_nodes = nodes_df[nodes_df['article'].str.contains(pattern, na=False, regex=True)]\n",
      "/var/folders/26/0sr3rdx53314n7l3fzj55k6r0000gn/T/ipykernel_42035/3685315357.py:191: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  filtered_nodes = nodes_df[nodes_df['article'].str.contains(pattern, na=False, regex=True)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created network for Article 35 with 3632 nodes\n",
      "Skipping Article 36 - only 47 cases (minimum: 50)\n",
      "Skipping Article P12 - only 20 cases (minimum: 50)\n",
      "Created network for Article 5 with 4302 nodes\n",
      "Created network for Article P4 with 199 nodes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/26/0sr3rdx53314n7l3fzj55k6r0000gn/T/ipykernel_42035/3685315357.py:191: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  filtered_nodes = nodes_df[nodes_df['article'].str.contains(pattern, na=False, regex=True)]\n",
      "/var/folders/26/0sr3rdx53314n7l3fzj55k6r0000gn/T/ipykernel_42035/3685315357.py:191: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  filtered_nodes = nodes_df[nodes_df['article'].str.contains(pattern, na=False, regex=True)]\n",
      "/var/folders/26/0sr3rdx53314n7l3fzj55k6r0000gn/T/ipykernel_42035/3685315357.py:191: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  filtered_nodes = nodes_df[nodes_df['article'].str.contains(pattern, na=False, regex=True)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created network for Article 29 with 2521 nodes\n",
      "Skipping Article 52 - only 2 cases (minimum: 50)\n",
      "Created network for Article 18 with 118 nodes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/26/0sr3rdx53314n7l3fzj55k6r0000gn/T/ipykernel_42035/3685315357.py:191: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  filtered_nodes = nodes_df[nodes_df['article'].str.contains(pattern, na=False, regex=True)]\n",
      "/var/folders/26/0sr3rdx53314n7l3fzj55k6r0000gn/T/ipykernel_42035/3685315357.py:191: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  filtered_nodes = nodes_df[nodes_df['article'].str.contains(pattern, na=False, regex=True)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created network for Article 6 with 14811 nodes\n",
      "Created network for Article 46 with 368 nodes\n",
      "Created network for Article 14 with 1030 nodes\n",
      "Skipping Article 28 - only 3 cases (minimum: 50)\n",
      "Skipping Article 53 - only 6 cases (minimum: 50)\n",
      "Created network for Article 34 with 1181 nodes\n",
      "Skipping Article 12 - only 35 cases (minimum: 50)\n",
      "Skipping Article 16 - only 2 cases (minimum: 50)\n",
      "Created network for Article 7 with 170 nodes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/26/0sr3rdx53314n7l3fzj55k6r0000gn/T/ipykernel_42035/3685315357.py:191: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  filtered_nodes = nodes_df[nodes_df['article'].str.contains(pattern, na=False, regex=True)]\n",
      "/var/folders/26/0sr3rdx53314n7l3fzj55k6r0000gn/T/ipykernel_42035/3685315357.py:191: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  filtered_nodes = nodes_df[nodes_df['article'].str.contains(pattern, na=False, regex=True)]\n",
      "/var/folders/26/0sr3rdx53314n7l3fzj55k6r0000gn/T/ipykernel_42035/3685315357.py:191: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  filtered_nodes = nodes_df[nodes_df['article'].str.contains(pattern, na=False, regex=True)]\n",
      "/var/folders/26/0sr3rdx53314n7l3fzj55k6r0000gn/T/ipykernel_42035/3685315357.py:191: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  filtered_nodes = nodes_df[nodes_df['article'].str.contains(pattern, na=False, regex=True)]\n",
      "/var/folders/26/0sr3rdx53314n7l3fzj55k6r0000gn/T/ipykernel_42035/3685315357.py:191: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  filtered_nodes = nodes_df[nodes_df['article'].str.contains(pattern, na=False, regex=True)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping Article 58 - only 32 cases (minimum: 50)\n",
      "Skipping Article 15 - only 45 cases (minimum: 50)\n",
      "Created network for Article 3 with 4606 nodes\n"
     ]
    }
   ],
   "source": [
    "# Parse full network\n",
    "nodes_path = '../data/METADATA/nodes.csv'\n",
    "edges_path = '../data/METADATA/edges.csv'\n",
    "output_root = '../networks/merged-article-edges'\n",
    "\n",
    "\n",
    "all_nodes_df, all_edges_df = process_network(\n",
    "    nodes=nodes_path,\n",
    "    edges=edges_path,\n",
    "    min_cases=50,\n",
    "    split_by_article=False,\n",
    "    save_files=False,\n",
    ")\n",
    "\n",
    "# Merge importance levels of 1 and 2 together (assign value 1), transpose 3 to 2, and 4 to 3\n",
    "all_nodes_df['importance'] = all_nodes_df['importance'].replace({1: 1, 2: 1, 3: 2, 4: 3})\n",
    "\n",
    "print(f\"Number of nodes: {len(all_nodes_df)}\")\n",
    "print(f\"Number of edges: {count_total_edges(all_edges_df)}\")\n",
    "\n",
    "# Balance network by importance\n",
    "balanced_nodes_importance, balanced_edges_importance = balance_network_by_parameter(\n",
    "    nodes_df=all_nodes_df,\n",
    "    edges_df=all_edges_df,\n",
    "    balance_parameter='importance'\n",
    ")\n",
    "\n",
    "# Balance network by doctypebranch\n",
    "balanced_nodes_doctypebranch, balanced_edges_doctypebranch = balance_network_by_parameter(\n",
    "    nodes_df=all_nodes_df,\n",
    "    edges_df=all_edges_df,\n",
    "    balance_parameter='doctypebranch'\n",
    ")\n",
    "\n",
    "# Save balanced networks (full)\n",
    "save_as_json(all_nodes_df, os.path.join(output_root, 'full-unbalanced/nodes.json'))\n",
    "save_as_json(all_edges_df, os.path.join(output_root, 'full-unbalanced/edges.json'))\n",
    "\n",
    "save_as_json(balanced_nodes_importance, os.path.join(output_root, 'full-balanced-importance/nodes.json'))\n",
    "save_as_json(balanced_edges_importance, os.path.join(output_root, 'full-balanced-importance/edges.json'))\n",
    "\n",
    "save_as_json(balanced_nodes_doctypebranch, os.path.join(output_root, 'full-balanced-doctypebranch/nodes.json'))\n",
    "save_as_json(balanced_edges_doctypebranch, os.path.join(output_root, 'full-balanced-doctypebranch/edges.json'))\n",
    "\n",
    "# Split the balanced networks\n",
    "_, _ = process_network(\n",
    "    nodes=balanced_nodes_importance,\n",
    "    edges=balanced_edges_importance,\n",
    "    min_cases=50,\n",
    "    split_by_article=True,\n",
    "    save_files=True,\n",
    "    merge_subarticles=True,\n",
    "    output_path=os.path.join(output_root, 'split-balanced-importance')\n",
    ")\n",
    "\n",
    "_, _ = process_network(\n",
    "    nodes=balanced_nodes_doctypebranch,\n",
    "    edges=balanced_edges_doctypebranch,\n",
    "    min_cases=50,\n",
    "    split_by_article=True,\n",
    "    save_files=True,\n",
    "    merge_subarticles=True,\n",
    "    output_path=os.path.join(output_root, 'split-balanced-doctypebranch')\n",
    ")\n",
    "\n",
    "# Split the unbalanced networks\n",
    "_, _ = process_network(\n",
    "    nodes=all_nodes_df,\n",
    "    edges=all_edges_df,\n",
    "    min_cases=50,\n",
    "    split_by_article=True,\n",
    "    merge_subarticles=True,\n",
    "    save_files=True,\n",
    "    output_path=os.path.join(output_root, 'split-unbalanced')\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
