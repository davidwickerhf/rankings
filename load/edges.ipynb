{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confirm AppNums Count\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows in metadata: 34090\n",
      "Rows with application numbers: 24934\n",
      "Total application numbers: 391297\n",
      "Unique application numbers: 107460\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def count_application_numbers(metadata_file):\n",
    "    # Read the metadata CSV\n",
    "    df = pd.read_csv(metadata_file, low_memory=False)\n",
    "    print(f\"Total rows in metadata: {len(df)}\")\n",
    "    \n",
    "    # Count rows with non-null extractedappno\n",
    "    has_appno = df['extractedappno'].notna()\n",
    "    print(f\"Rows with application numbers: {has_appno.sum()}\")\n",
    "    \n",
    "    # Count total unique application numbers\n",
    "    total_appnos = 0\n",
    "    unique_appnos = set()\n",
    "    \n",
    "    for appno in df[has_appno]['extractedappno']:\n",
    "        if isinstance(appno, str):\n",
    "            # Split by semicolon as they're semicolon-separated\n",
    "            appnos = appno.split(';')\n",
    "            total_appnos += len(appnos)\n",
    "            unique_appnos.update(appnos)\n",
    "    \n",
    "    print(f\"Total application numbers: {total_appnos}\")\n",
    "    print(f\"Unique application numbers: {len(unique_appnos)}\")\n",
    "\n",
    "# Usage\n",
    "count_application_numbers('../data/METADATA/echr_metadata.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confirm count of nodes and edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/26/0sr3rdx53314n7l3fzj55k6r0000gn/T/ipykernel_93543/3870497256.py:167: DtypeWarning: Columns (10,17) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  nodes_df = pd.read_csv('../data/METADATA/optimized/nodes.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Edge Count Statistics:\n",
      "Total unique edges: 232002\n",
      "Duplicate edges found: 0\n",
      "Rows with NaN references: 0\n",
      "Rows with empty references []: 6660\n",
      "Rows containing NaN within references list: 0\n",
      "Total number of edges: 232002\n",
      "\n",
      "Network Statistics:\n",
      "Nodes: 27802\n",
      "Original Edges: 232002\n",
      "Duplicate Edges Removed: 0\n",
      "Invalid Source Edges Removed: 0\n",
      "Invalid Target Edges Removed: 0\n",
      "Final Edges: 232002\n",
      "Isolated Nodes: 4496\n",
      "Network Density: 0.000300\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict, List, Set, Tuple\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "def count_total_edges(edges_df: pd.DataFrame) -> int:\n",
    "    \"\"\"\n",
    "    Count total number of edges in the network, considering multiple targets per entry.\n",
    "    \"\"\"\n",
    "    total_edges = 0\n",
    "    nan_rows = 0\n",
    "    empty_refs = 0\n",
    "    nan_in_refs = 0\n",
    "    seen_edges = set()  # Track unique edges\n",
    "    duplicate_count = 0\n",
    "    examples_shown = 0  # Counter for duplicate examples\n",
    "    \n",
    "    for _, row in edges_df.iterrows():\n",
    "        refs = row['references']\n",
    "            \n",
    "        # Handle string representation of list\n",
    "        if isinstance(refs, str):\n",
    "            try:\n",
    "                refs_list = eval(refs)\n",
    "            except:\n",
    "                print(f\"Warning: Could not process references: {refs}\")\n",
    "                continue\n",
    "        else:\n",
    "            refs_list = refs\n",
    "            \n",
    "        # Handle empty list\n",
    "        if not refs_list:\n",
    "            empty_refs += 1\n",
    "            continue\n",
    "            \n",
    "        source = row['ecli']\n",
    "        # Process each reference\n",
    "        for ref in refs_list:\n",
    "            if isinstance(ref, str) and not pd.isna(ref) and ref.startswith('ECLI:'):\n",
    "                edge = (source, ref)\n",
    "                if edge in seen_edges:\n",
    "                    duplicate_count += 1\n",
    "                    if examples_shown < 2:  # Show first 2 duplicates found\n",
    "                        print(f\"\\nDuplicate edge found:\")\n",
    "                        print(f\"Source: {source}\")\n",
    "                        print(f\"Target: {ref}\")\n",
    "                        examples_shown += 1\n",
    "                else:\n",
    "                    seen_edges.add(edge)\n",
    "                    total_edges += 1\n",
    "        \n",
    "        # Track if we found any NaN values in this reference list\n",
    "        if any(pd.isna(ref) for ref in refs_list):\n",
    "            nan_in_refs += 1\n",
    "    \n",
    "    # Print statistics\n",
    "    print(\"\\nEdge Count Statistics:\")\n",
    "    print(f\"Total unique edges: {total_edges}\")\n",
    "    print(f\"Duplicate edges found: {duplicate_count}\")\n",
    "    print(f\"Rows with NaN references: {nan_rows}\")\n",
    "    print(f\"Rows with empty references []: {empty_refs}\")\n",
    "    print(f\"Rows containing NaN within references list: {nan_in_refs}\")\n",
    "    \n",
    "    return total_edges\n",
    "\n",
    "def analyze_network(nodes_df: pd.DataFrame, edges_df: pd.DataFrame) -> Tuple[pd.DataFrame, dict]:\n",
    "    \"\"\"\n",
    "    Analyze and clean the network by:\n",
    "    1. Counting and removing duplicate edges\n",
    "    2. Removing edges with non-existent sources/targets\n",
    "    3. Generating network statistics\n",
    "    \"\"\"\n",
    "    # Convert nodes to set for O(1) lookup\n",
    "    valid_nodes = set(nodes_df['ecli'].unique())\n",
    "    \n",
    "    # Statistics dictionary\n",
    "    stats = {\n",
    "        'original_edges': 0,\n",
    "        'duplicate_edges': 0,\n",
    "        'invalid_source_edges': 0,\n",
    "        'invalid_target_edges': 0,\n",
    "        'final_edges': 0,\n",
    "        'nodes': len(valid_nodes),\n",
    "        'isolated_nodes': 0\n",
    "    }\n",
    "    \n",
    "    # Convert string representation of lists to actual lists, handling NaN values\n",
    "    def safe_eval(x):\n",
    "        if pd.isna(x):\n",
    "            return []\n",
    "        try:\n",
    "            return eval(x)\n",
    "        except:\n",
    "            return []\n",
    "            \n",
    "    edges_df['references'] = edges_df['references'].apply(safe_eval)\n",
    "    \n",
    "    # Count original edges\n",
    "    stats['original_edges'] = sum(len(refs) for refs in edges_df['references'])\n",
    "    \n",
    "    # Create set of all unique edges and track invalid edges\n",
    "    unique_edges = set()\n",
    "    cleaned_edges = []\n",
    "    \n",
    "    for _, row in edges_df.iterrows():\n",
    "        source = row['ecli']\n",
    "        if source not in valid_nodes:\n",
    "            stats['invalid_source_edges'] += len(row['references'])\n",
    "            continue\n",
    "            \n",
    "        valid_refs = []\n",
    "        for target in row['references']:\n",
    "            # Check for invalid targets\n",
    "            if target not in valid_nodes:\n",
    "                stats['invalid_target_edges'] += 1\n",
    "                continue\n",
    "                \n",
    "            # Check for duplicates\n",
    "            edge = (source, target)\n",
    "            if edge in unique_edges:\n",
    "                stats['duplicate_edges'] += 1\n",
    "                continue\n",
    "                \n",
    "            unique_edges.add(edge)\n",
    "            valid_refs.append(target)\n",
    "        \n",
    "        if valid_refs:\n",
    "            cleaned_edges.append({\n",
    "                'ecli': source,\n",
    "                'references': valid_refs\n",
    "            })\n",
    "    \n",
    "    # Create cleaned DataFrame\n",
    "    cleaned_df = pd.DataFrame(cleaned_edges) if cleaned_edges else pd.DataFrame(columns=['ecli', 'references'])\n",
    "    stats['final_edges'] = len(unique_edges)\n",
    "    \n",
    "    # Count isolated nodes (nodes with no edges)\n",
    "    if not cleaned_df.empty:\n",
    "        connected_nodes = set(cleaned_df['ecli']).union(\n",
    "            *[set(refs) for refs in cleaned_df['references']]\n",
    "        )\n",
    "    else:\n",
    "        connected_nodes = set()\n",
    "    stats['isolated_nodes'] = len(valid_nodes - connected_nodes)\n",
    "    \n",
    "    # Calculate network density\n",
    "    possible_edges = len(valid_nodes) * (len(valid_nodes) - 1)  # Directed graph\n",
    "    stats['density'] = stats['final_edges'] / possible_edges if possible_edges > 0 else 0\n",
    "    \n",
    "    # Print statistics\n",
    "    print(\"\\nNetwork Statistics:\")\n",
    "    print(f\"Nodes: {stats['nodes']}\")\n",
    "    print(f\"Original Edges: {stats['original_edges']}\")\n",
    "    print(f\"Duplicate Edges Removed: {stats['duplicate_edges']}\")\n",
    "    print(f\"Invalid Source Edges Removed: {stats['invalid_source_edges']}\")\n",
    "    print(f\"Invalid Target Edges Removed: {stats['invalid_target_edges']}\")\n",
    "    print(f\"Final Edges: {stats['final_edges']}\")\n",
    "    print(f\"Isolated Nodes: {stats['isolated_nodes']}\")\n",
    "    print(f\"Network Density: {stats['density']:.6f}\")\n",
    "    \n",
    "    return cleaned_df, stats\n",
    "\n",
    "\n",
    "# Load data\n",
    "edges_df = pd.read_csv('../data/METADATA/optimized/edges.csv')\n",
    "nodes_df = pd.read_csv('../data/METADATA/optimized/nodes.csv')\n",
    "\n",
    "# Calculate total edges\n",
    "total_edges = count_total_edges(edges_df)\n",
    "print(f\"Total number of edges: {total_edges}\")\n",
    "\n",
    "cleaned_edges, stats = analyze_network(nodes_df, edges_df)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate updated count of nodes and edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import dateparser\n",
    "from echr_extractor.clean_ref import clean_pattern\n",
    "\n",
    "\n",
    "\n",
    "def open_metadata(PATH_metadata):\n",
    "    \"\"\"\n",
    "    Finds the ECHR metadata file and loads it into a dataframe\n",
    "    \n",
    "    param filename_metadata: string with path to metadata\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(PATH_metadata)  # change hard coded path\n",
    "        print(df.columns)\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        print(\"File not found. Please check the path to the metadata file.\")\n",
    "        return False\n",
    "\n",
    "def concat_metadata(df):\n",
    "    agg_func = {'itemid' : 'first', 'appno' : 'first', 'article' : 'first', 'conclusion' : 'first' , 'docname' : 'first' , 'doctype' : 'first',\n",
    "                'doctypebranch' : 'first', 'ecli' : 'first', 'importance' : 'first', 'judgementdate' : 'first', 'languageisocode' : ', '.join, 'originatingbody' : 'first',\n",
    "                'violation' : 'first', 'nonviolation' : 'first', 'extractedappno' : 'first', 'scl' : 'first'}\n",
    "    new_df = df.groupby('ecli').agg(agg_func)\n",
    "    print(new_df)\n",
    "    return new_df\n",
    "\n",
    "def get_language_from_metadata(df):\n",
    "    df = concat_metadata(df)\n",
    "    df.to_json('langisocode-nodes.json', orient=\"records\")\n",
    "\n",
    "def metadata_to_nodesedgeslist(df):\n",
    "    \"\"\"\n",
    "    Returns a dataframe where column 'article' only contains a certain article\n",
    "\n",
    "    param df: the complete dataframe from the metadata\n",
    "    \"\"\"\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def retrieve_nodes_list(df):\n",
    "    \"\"\"\n",
    "    Returns a dataframe where 'ecli' is moved to the first column.\n",
    "    \n",
    "    param df: the dataframe after article filter\n",
    "    \"\"\"\n",
    "    df = metadata_to_nodesedgeslist(df)\n",
    "    # Instead of dropping first column blindly, keep appno\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # Move ecli to first position\n",
    "    col = df_copy.pop(\"ecli\")\n",
    "    df_copy.insert(0, col.name, col)\n",
    "    \n",
    "    return df_copy\n",
    "\n",
    "\n",
    "def retrieve_edges_list(df, df_unfiltered):\n",
    "    \"\"\"\n",
    "    Returns a dataframe consisting of 2 columns 'ecli' and 'reference' which\n",
    "    indicate a reference link between cases.\n",
    "\n",
    "    params:\n",
    "    df -- the node list extracted from the metadata\n",
    "    df_unfiltered -- the complete dataframe from the metadata\n",
    "    \"\"\"\n",
    "    edges = pd.DataFrame(columns=['ecli', 'references'])\n",
    "\n",
    "    count = 0\n",
    "    tot_num_refs = 0\n",
    "    missing_cases = []\n",
    "    for index, item in df.iterrows():\n",
    "        eclis = []\n",
    "        app_number = []\n",
    "        extracted_appnos = []\n",
    "        if item.extractedappno is not np.nan:\n",
    "            extracted_appnos = item.extractedappno.split(';') \n",
    "\n",
    "        if item.scl is not np.nan:\n",
    "            \"\"\"\n",
    "            Split the references from the scl column i nto a list of references.\n",
    "\n",
    "            Example:\n",
    "            references in string: \"Ali v. Switzerland, 5 August 1998, § 32, Reports of Judgments and \n",
    "            Decisions 1998-V;Sevgi Erdogan v. Turkey (striking out), no. 28492/95, 29 April 2003\"\n",
    "\n",
    "            [\"Ali v. Switzerland, 5 August 1998, § 32, Reports of Judgments and \n",
    "            Decisions 1998-V\", \"Sevgi Erdogan v. Turkey (striking out), no. \n",
    "            28492/95, 29 April 2003\"]\n",
    "            \"\"\"\n",
    "            ref_list = item.scl.split(';')\n",
    "            new_ref_list = []\n",
    "            for ref in ref_list:\n",
    "                ref = re.sub('\\n', '', ref)\n",
    "                new_ref_list.append(ref)\n",
    "\n",
    "            tot_num_refs = tot_num_refs + len(ref_list)\n",
    "\n",
    "            for ref in new_ref_list:\n",
    "                app_number = re.findall(\"[0-9]{3,5}\\/[0-9]{2}\", ref) ################\n",
    "                if len(extracted_appnos) > 0:\n",
    "                    app_number = app_number + extracted_appnos\n",
    "                # app_number = app_number + extracted_appnos\n",
    "                app_number = set(app_number)\n",
    "                \n",
    "                if len(app_number) > 0:\n",
    "                    # get dataframe with all possible cases by application number\n",
    "                    if len(app_number) > 1:\n",
    "                        app_number = [';'.join(app_number)]\n",
    "                    case = lookup_app_number(app_number, df_unfiltered)\n",
    "                else: # if no application number in reference\n",
    "                    # get dataframe with all possible cases by casename\n",
    "                    case = lookup_casename(ref, df_unfiltered)\n",
    "\n",
    "                if len(case) == 0:\n",
    "                    case = lookup_casename(ref, df_unfiltered)\n",
    "\n",
    "                components = ref.split(',')\n",
    "                # get the year of case\n",
    "                year_from_ref = get_year_from_ref(components)\n",
    "\n",
    "                # remove cases in different language than reference\n",
    "                for id, it in case.iterrows():\n",
    "                    if 'v.' in components[0]:\n",
    "                        lang = 'ENG'\n",
    "                    else:\n",
    "                        lang = 'FRE'\n",
    "\n",
    "                    if lang not in it.languageisocode:\n",
    "                        case = case[case['languageisocode'].str.contains(lang, regex=False, flags=re.IGNORECASE)]\n",
    "\n",
    "                for id, i in case.iterrows():\n",
    "                    if i.judgementdate is np.nan:\n",
    "                        continue\n",
    "                    date = dateparser.parse(i.judgementdate)\n",
    "                    year_from_case = date.year\n",
    "\n",
    "                    if year_from_case - year_from_ref == 0:\n",
    "                        case = case[case['judgementdate'].str.contains(str(year_from_ref), regex=False, flags=re.IGNORECASE)]\n",
    "\n",
    "                #case = metadata_to_nodesedgeslist(case)\n",
    "\n",
    "                if len(case) > 0:\n",
    "                    if len(case) > 3:\n",
    "                        print(\"stop\")\n",
    "                    for _,row in case.iterrows():\n",
    "                        eclis.append(row.ecli)\n",
    "                else:\n",
    "                    count = count + 1\n",
    "                    missing_cases.append(ref)\n",
    "\n",
    "            eclis = set(eclis)\n",
    "\n",
    "            #add ecli to edges list\n",
    "            if len(eclis) == 0:\n",
    "                continue\n",
    "            else:\n",
    "                edges = pd.concat(\n",
    "                    [edges, pd.DataFrame.from_records([{'ecli': item.ecli, 'references': list(eclis)}])])\n",
    "\n",
    "    print(\"num missed cases: \", count)\n",
    "    print(\"total num of refs: \", tot_num_refs)\n",
    "    missing_cases_set = set(missing_cases)\n",
    "    missing_cases = list(missing_cases_set)\n",
    "    \n",
    "    # Store missing references\n",
    "    missing_df = pd.DataFrame(missing_cases)\n",
    "    edges = edges.groupby('ecli', as_index=False).agg({'references' : 'sum'})\n",
    "    return edges, missing_df\n",
    "\n",
    "def lookup_app_number(pattern, df):\n",
    "    \"\"\"\n",
    "    Returns a list with rows containing the cases linked to the found app numbers.\n",
    "    \"\"\"\n",
    "    print(f\"Looking up app numbers: {pattern}\")\n",
    "    row = df.loc[df['appno'].isin(pattern)]\n",
    "\n",
    "    if row.empty:\n",
    "        return pd.DataFrame()\n",
    "    elif row.shape[0] > 1:\n",
    "        return row\n",
    "    else:\n",
    "        return row\n",
    "\n",
    "\n",
    "def lookup_casename(ref, df):\n",
    "    \"\"\"\n",
    "    Process the reference for lookup in metadata.\n",
    "    Returns the rows corresponding to the cases.\n",
    "\n",
    "    - Example of the processing (2 variants) -\n",
    "\n",
    "    Original reference from scl:\n",
    "    - Hentrich v. France, 22 September 1994, § 42, Series A no. 296-A\n",
    "    - Eur. Court H.R. James and Others judgment of 21 February 1986,\n",
    "    Series A no. 98, p. 46, para. 81\n",
    "\n",
    "    Split on ',' and take first item:\n",
    "    Hentrich v. France\n",
    "    Eur. Court H.R. James and Others judgment of 21 February 1986\n",
    "\n",
    "    If certain pattern from CLEAN_REF in case name, then remove:\n",
    "    Eur. Court H.R. James and Others judgment of 21 February 1986 -->\n",
    "        James and Others\n",
    "\n",
    "    Change name to upper case and add additional text to match metadata:\n",
    "    Hentrich v. France --> CASE OF HENTRICH V. FRANCE\n",
    "    James and Others --> CASE OF JAMES AND OTHERS\n",
    "    \"\"\"\n",
    "    name = get_casename(ref)\n",
    "    \n",
    "    # DEV note: In case, add more patterns to clean_ref.py in future\n",
    "    patterns = clean_pattern\n",
    "\n",
    "    uptext = name.upper()\n",
    "\n",
    "    if 'NO.' in uptext:\n",
    "        uptext = uptext.replace('NO.', 'No.')\n",
    "\n",
    "    if 'BV' in uptext:\n",
    "        uptext = uptext.replace('BV', 'B.V.')\n",
    "\n",
    "    if 'v.' in name:\n",
    "        uptext = uptext.replace('V.', 'v.')\n",
    "        lang = 'ENG'\n",
    "    else:\n",
    "        uptext = uptext.replace('C.', 'c.')\n",
    "        lang = 'FRE'\n",
    "\n",
    "    for pattern in patterns:\n",
    "        uptext = re.sub(pattern, '', uptext)\n",
    "\n",
    "    uptext = re.sub(r'\\[.*', \"\", uptext)\n",
    "    uptext = uptext.strip()\n",
    "    row = df[df['docname'].str.contains(uptext, regex=False, flags=re.IGNORECASE)]\n",
    "\n",
    "    # if len(row) == 0:\n",
    "    #     print(\"no cases matched: \", name)\n",
    "\n",
    "    return row\n",
    "\n",
    "def get_casename(ref):\n",
    "    count = 0\n",
    "    if 'v.' in ref:\n",
    "        slice_at_versus = ref.split('v.')  # skip if typo (count how many)\n",
    "    elif 'c.' in ref:\n",
    "        slice_at_versus = ref.split('c.')\n",
    "    else:\n",
    "        count = count + 1\n",
    "        name = ref.split(',')\n",
    "        return name[0]\n",
    "\n",
    "    num_commas = slice_at_versus[0].count(',')\n",
    "\n",
    "    if num_commas > 0:\n",
    "        num_commas = num_commas + 1\n",
    "        name = \",\".join(ref.split(\",\", num_commas)[:num_commas])\n",
    "    else:\n",
    "        name = ref.split(',')\n",
    "        return name[0]\n",
    "    return name\n",
    "\n",
    "def get_year_from_ref(ref):\n",
    "    for component in ref:\n",
    "        if '§' in component:\n",
    "            continue\n",
    "        component = re.sub('judgment of ', \"\", component)\n",
    "        if dateparser.parse(component) is not None:\n",
    "            date = dateparser.parse(component)\n",
    "        elif (\"ECHR\" in component or \"CEDH\" in component):\n",
    "            if (\"ECHR\" in component or \"CEDH\" in component):\n",
    "                date = re.sub('ECHR ', '', component)\n",
    "                date = re.sub('CEDH ', '', date)\n",
    "                date = date.strip()\n",
    "                date = re.sub('-.*', '', date)\n",
    "                date = re.sub('\\s.*', '', date)\n",
    "                date = dateparser.parse(date)\n",
    "   \n",
    "    try:\n",
    "        return date.year\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "\n",
    "\n",
    "def echr_nodes_edges(metadata_path):\n",
    "    \"\"\"\n",
    "    Create nodes and edges list for the ECHR data.\n",
    "    \"\"\"\n",
    "    print('\\n--- COLLECTING METADATA ---\\n')\n",
    "    data = open_metadata(metadata_path)\n",
    "\n",
    "    print('\\n--- EXTRACTING NODES LIST ---\\n')\n",
    "    nodes = retrieve_nodes_list(data)\n",
    "    print(f\"Number of nodes: {len(nodes)}\")\n",
    "    # get_language_from_metadata(nodes)\n",
    "\n",
    "    print('\\n--- EXTRACTING EDGES LIST ---\\n')\n",
    "    edges, missing_df = retrieve_edges_list(nodes, data)\n",
    "\n",
    "    return nodes, edges, missing_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes, edges, missing_df = echr_nodes_edges('../data/METADATA/echr_metadata.csv')\n",
    "\n",
    "print(f\"Number of nodes: {len(nodes)}\")\n",
    "print(f\"Number of edges: {len(edges)}\")\n",
    "print(f\"Number of missing cases: {len(missing_df)}\")\n",
    "\n",
    "# Save to CSV files\n",
    "print(\"\\n--- SAVING TO CSV ---\\n\")\n",
    "nodes.to_csv('../data/METADATA/nodes.csv', index=False)\n",
    "edges.to_csv('../data/METADATA/edges.csv', index=False) \n",
    "missing_df.to_csv('../data/METADATA/missing_cases.csv', index=False)\n",
    "\n",
    "print(\"Files saved successfully to ../data/METADATA/\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
