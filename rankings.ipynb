{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing Centrality Metrics\n",
    "\n",
    "This notebook analyzes and compares different centrality metrics for a citation network.\n",
    "\n",
    "It loads node and edge data from JSON files, calculates various centrality metrics (like degree, betweenness, closeness etc.), and compares them against ground truth measures like importance and document type.\n",
    "\n",
    "The notebook defines constants for:\n",
    "- Input data file paths\n",
    "- Centrality metrics to analyze \n",
    "- Ground truth measures to compare against\n",
    "\n",
    "It also defines TypedDict classes to type the network statistics and results.\n",
    "\n",
    "**Note:** This is an analysis notebook. To modify the code that generates the network and calculates centralities, please refer to the Main section.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import datetime\n",
    "import dateutil.parser\n",
    "import statistics\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from scipy.stats import spearmanr\n",
    "import seaborn as sns\n",
    "import os\n",
    "from typing import Dict, List, Union, TypedDict, Tuple\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetworkStats(TypedDict):\n",
    "    n_nodes: int\n",
    "    n_edges: int\n",
    "\n",
    "class BestCentralities(TypedDict):\n",
    "    high: str\n",
    "    low: str\n",
    "\n",
    "class AnalysisResults(TypedDict):\n",
    "    network_stats: NetworkStats\n",
    "    correlations: Dict[str, Dict[str, Dict[tuple[str, str], float]]]  # ground_truth -> composite_function -> (centrality, ground_truth) -> correlation\n",
    "    best_centralities: Dict[str, BestCentralities]  # ground_truth -> best centralities\n",
    "    composite_rankings: Dict[str, Dict[str, Dict[str, float]]]  # ground_truth -> composite_function -> ecli -> rank\n",
    "    dataframe: pd.DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert doctypebranch to a numeric value\n",
    "def categorise_total_branch_numerically(branches: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Convert branch categorisation from strings into numbers.\n",
    "    \n",
    "    :param branches: The column containing branch data, categorized with strings.\n",
    "    :return: A pandas Series with numerical categorization.\n",
    "    \"\"\"\n",
    "    mapping = {\n",
    "        \"GRANDCHAMBER\": 1,\n",
    "        \"CHAMBER\": 2,\n",
    "        \"COMMITTEE\": 3,\n",
    "    }\n",
    "    \n",
    "    # Convert to uppercase to ensure consistent matching\n",
    "    branches = branches.str.upper()\n",
    "    \n",
    "    # Print any values that don't match our mapping\n",
    "    unmapped = set(branches.unique()) - set(mapping.keys())\n",
    "    if unmapped:\n",
    "        print(f\"Warning: Found unmapped values: {unmapped}\")\n",
    "    \n",
    "    return branches.map(mapping).fillna(0)  # Filling NaN with 0\n",
    "\n",
    "def prep_data(df: pd.DataFrame, include: list) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Prepare the dataset by selecting the appropriate columns and filtering out rows with uncomputed metric values.\n",
    "    \n",
    "    :param df: The DataFrame to process.\n",
    "    :param include: Columns to include.\n",
    "    :return: The processed DataFrame.\n",
    "    \"\"\"\n",
    "    headers = include + ['ecli']  # Ensure essential columns are included\n",
    "    headers = list(set(headers))  # Removing duplicates\n",
    "\n",
    "    data = df[headers]\n",
    "\n",
    "    # Filter out rows with uncomputed metric values (-2)\n",
    "    metric_column = include[-1]\n",
    "    data = data[data[metric_column] >= -1]\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Centrality Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_disruptions_new(graph: nx.Graph) -> dict:\n",
    "    \"\"\"\n",
    "    Calculate the disruption score for each node in the graph.\n",
    "\n",
    "    Args:\n",
    "        graph (nx.Graph): The input directed graph.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with nodes as keys and their disruption scores as values.\n",
    "    \"\"\"\n",
    "    disruptions = {}\n",
    "    for node in graph.nodes:\n",
    "        i, j, k = 0, 0, 0\n",
    "\n",
    "        # count j\n",
    "        for in_node in graph.predecessors(node):\n",
    "            for out_node in graph.successors(node):\n",
    "                if graph.has_edge(in_node, out_node):\n",
    "                    j += 1\n",
    "                    break\n",
    "\n",
    "        # count i\n",
    "        i = graph.in_degree(node) - j\n",
    "\n",
    "        # count k\n",
    "        for out_node in graph.successors(node):\n",
    "            for in_out_node in graph.predecessors(out_node):\n",
    "                if in_out_node != node and not graph.has_edge(in_out_node, node):\n",
    "                    k += 1\n",
    "\n",
    "        try:\n",
    "            disruptions[node] = (i - j) / (i + j + k)\n",
    "        except ZeroDivisionError:\n",
    "            disruptions[node] = np.nan\n",
    "\n",
    "    return disruptions\n",
    "\n",
    "\n",
    "# Function to calculate centrality measures\n",
    "def calculate_centrality_measures(graph):\n",
    "    degree_centrality = nx.degree_centrality(graph)\n",
    "    in_degree_centrality = nx.in_degree_centrality(graph)\n",
    "    out_degree_centrality = nx.out_degree_centrality(graph)\n",
    "    betweenness_centrality = nx.betweenness_centrality(graph, normalized=True)\n",
    "    closeness_centrality = nx.closeness_centrality(graph)\n",
    "    \n",
    "    # Additional centrality measures\n",
    "    core_number = nx.core_number(graph)\n",
    "    eigenvector_centrality = nx.eigenvector_centrality(graph)\n",
    "    pagerank = nx.pagerank(graph)\n",
    "    # current_flow_betweenness_centrality = nx.current_flow_betweenness_centrality(graph)\n",
    "    harmonic_centrality = nx.harmonic_centrality(graph)\n",
    "    \n",
    "    # For HITS, we get both hub and authority scores\n",
    "    hits_hub, hits_authority = nx.hits(graph)\n",
    "    \n",
    "    # Trophic level calculation\n",
    "    # Note: NetworkX doesn't have a built-in function for trophic level, so this is a placeholder\n",
    "    # trophic_level = {node: 0 for node in graph.nodes}  # Placeholder implementation\n",
    "    \n",
    "    # Relative in-degree centrality\n",
    "    total_nodes = len(graph.nodes)\n",
    "    relative_in_degree_centrality = {node: in_degree / total_nodes for node, in_degree in in_degree_centrality.items()}\n",
    "    \n",
    "    # Placeholder for Forest Closeness and Disruption as these are not standard in NetworkX\n",
    "    # forest_closeness = {node: 0 for node in graph.nodes}  # Placeholder implementation\n",
    "    disruption = calculate_disruptions_new(graph)  # Placeholder implementation\n",
    "\n",
    "    return {\n",
    "        'degree_centrality': degree_centrality,\n",
    "        'in_degree_centrality': in_degree_centrality,\n",
    "        'out_degree_centrality': out_degree_centrality,\n",
    "        'betweenness_centrality': betweenness_centrality,\n",
    "        'closeness_centrality': closeness_centrality,\n",
    "        'core_number': core_number,\n",
    "        'relative_in_degree_centrality': relative_in_degree_centrality,\n",
    "        'eigenvector_centrality': eigenvector_centrality,\n",
    "        'pagerank': pagerank,\n",
    "        # 'current_flow_betweenness_centrality': current_flow_betweenness_centrality,\n",
    "        'hits_hub': hits_hub,\n",
    "        'hits_authority': hits_authority,\n",
    "        'harmonic_centrality': harmonic_centrality,\n",
    "        'disruption': disruption,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Composite Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script analyzes the relationship between various centrality measures and ground truth scores\n",
    "for legal cases. It aims to find the best centrality measures for predicting high and low relevance\n",
    "scores, create a composite ranking, and evaluate its performance against individual centrality measures.\n",
    "\n",
    "The main steps are:\n",
    "1. Plot error bars for centrality measures vs. ground truth scores\n",
    "2. Find the best centrality measures for predicting high and low scores\n",
    "3. Create a composite ranking using the best measures\n",
    "4. Calculate correlations between rankings and ground truth scores\n",
    "5. Visualize and save the results\n",
    "\"\"\"\n",
    "\n",
    "def plot_error_bars(df, centrality, ground_truth):\n",
    "    \"\"\"\n",
    "    Plot error bars for a given centrality measure against a ground truth score.\n",
    "\n",
    "    Args:\n",
    "    df (pd.DataFrame): The dataframe containing the data\n",
    "    centrality (str): The name of the centrality measure column\n",
    "    ground_truth (str): The name of the ground truth score column\n",
    "\n",
    "    This function visualizes the relationship between a centrality measure and a ground truth score,\n",
    "    showing the mean centrality value for each ground truth score category along with error bars\n",
    "    representing the standard deviation.\n",
    "    \"\"\"\n",
    "    # Prepare data\n",
    "    include = [ground_truth, centrality]\n",
    "    data = prep_data(df, include)\n",
    "\n",
    "    x_header = centrality\n",
    "    y_header = ground_truth\n",
    "    x, y = list(data[x_header]), list(data[y_header])\n",
    "    categories = list(set(y))\n",
    "    categories.sort()\n",
    "    num_categories, num_instances = len(categories), len(x)\n",
    "    y_instances = [[] for _ in range(num_categories)]\n",
    "    for category_no in range(num_categories):\n",
    "        for instance_no in range(num_instances):\n",
    "            if y[instance_no] == categories[category_no]:\n",
    "                y_instances[category_no].append(x[instance_no])\n",
    "    x = [statistics.mean(y_instances[category_no]) for category_no in range(num_categories)]\n",
    "    y = categories\n",
    "\n",
    "    # Draw graph\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    title = f\"{centrality.capitalize()} vs Average {y_header.capitalize()}\"\n",
    "    plt.suptitle(title, fontsize=22)\n",
    "    plt.xlabel(f\"{centrality.capitalize()}\", fontsize=22)\n",
    "    plt.ylabel(f\"{y_header.capitalize()}\", fontsize=22)\n",
    "    plt.yticks(categories, fontsize=16)\n",
    "\n",
    "    # Calculate error bars\n",
    "    stds = [statistics.stdev(y_instances[category_no]) for category_no in range(num_categories)]\n",
    "    plt.errorbar(x, y, xerr=stds, fmt='o')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def find_best_centralities(df, centralities, ground_truth):\n",
    "    \"\"\"\n",
    "    Find the best centrality measures for predicting high and low ground truth scores.\n",
    "    TODO: Include considerations for multiple ground truth scores\n",
    "\n",
    "    Args:\n",
    "    df (pd.DataFrame): The dataframe containing the data\n",
    "    centralities (list): List of centrality measure column names\n",
    "    ground_truth (str): The name of the ground truth score column\n",
    "\n",
    "    Returns:\n",
    "    tuple: (best_high, best_low) - the names of the best centrality measures for high and low scores\n",
    "\n",
    "    This function calculates the Spearman correlation between each centrality measure and the ground truth,\n",
    "    using 1 - |correlation| as an error metric. The centrality with the lowest error is chosen as best_high,\n",
    "    and the second-lowest (excluding best_high) is chosen as best_low.\n",
    "    \"\"\"\n",
    "    errors = {}\n",
    "    \n",
    "    for centrality in centralities:\n",
    "        # Calculate correlation across the full range\n",
    "        corr, _ = stats.spearmanr(df[centrality], df[ground_truth])\n",
    "        errors[centrality] = 1 - abs(corr)  # Use 1 - |correlation| as error\n",
    "    \n",
    "    best_high = min(errors, key=errors.get)\n",
    "    \n",
    "    # Remove the best_high centrality from consideration for best_low\n",
    "    errors.pop(best_high, None)\n",
    "    \n",
    "    best_low = min(errors, key=errors.get)\n",
    "    \n",
    "    return best_high, best_low\n",
    "\n",
    "def rank_cases(df, centrality):\n",
    "    \"\"\"\n",
    "    Rank cases based on a given centrality measure.\n",
    "\n",
    "    Args:\n",
    "    df (pd.DataFrame): The dataframe containing the data\n",
    "    centrality (str): The name of the centrality measure column\n",
    "\n",
    "    Returns:\n",
    "    pd.Series: A series of rankings for each case\n",
    "\n",
    "    This function ranks the cases in descending order of the centrality measure,\n",
    "    with the highest centrality receiving rank 1.\n",
    "    \"\"\"\n",
    "    return df[centrality].rank(ascending=False)\n",
    "\n",
    "def create_treashold_composite_ranking(df, high_centrality, low_centrality, ground_truth):\n",
    "    \"\"\"\n",
    "    Create a composite ranking based on a threshold approach using two centrality measures.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame containing the centrality measures\n",
    "        high_centrality: Centrality measure that performs best for high scores\n",
    "        low_centrality: Centrality measure that performs best for low scores\n",
    "        ground_truth: Name of the ground truth column\n",
    "        \n",
    "    Returns:\n",
    "        Series containing the composite ranking\n",
    "    \"\"\"\n",
    "    # Normalize both centrality measures to [0,1] range\n",
    "    high_normalized = (df[high_centrality] - df[high_centrality].min()) / (df[high_centrality].max() - df[high_centrality].min())\n",
    "    low_normalized = (df[low_centrality] - df[low_centrality].min()) / (df[low_centrality].max() - df[low_centrality].min())\n",
    "    \n",
    "    # Find optimal threshold by testing different values\n",
    "    thresholds = np.linspace(0, 1, 100)\n",
    "    best_correlation = -1\n",
    "    optimal_threshold = 0\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        # Create composite ranking using current threshold\n",
    "        composite = np.where(\n",
    "            high_normalized > threshold,\n",
    "            high_normalized,  # Use high centrality measure\n",
    "            low_normalized    # Use low centrality measure\n",
    "        )\n",
    "        \n",
    "        # Calculate correlation with ground truth\n",
    "        correlation = abs(spearmanr(composite, df[ground_truth])[0])\n",
    "        \n",
    "        # Update if better correlation found\n",
    "        if correlation > best_correlation:\n",
    "            best_correlation = correlation\n",
    "            optimal_threshold = threshold\n",
    "    \n",
    "    # Create final composite ranking using optimal threshold\n",
    "    final_composite = np.where(\n",
    "        high_normalized > optimal_threshold,\n",
    "        high_normalized,\n",
    "        low_normalized\n",
    "    )\n",
    "    \n",
    "    print(f\"Optimal threshold found: {optimal_threshold:.3f}\")\n",
    "    return final_composite\n",
    "\n",
    "def create_composite_ranking(df, high_centrality, low_centrality, weight):\n",
    "    \"\"\"\n",
    "    Create a composite ranking using two centrality measures.\n",
    "\n",
    "    Args:\n",
    "    df (pd.DataFrame): The dataframe containing the data\n",
    "    high_centrality (str): The name of the centrality measure best for high scores\n",
    "    low_centrality (str): The name of the centrality measure best for low scores\n",
    "    weight (float): The weight given to the high_centrality ranking (0-1)\n",
    "\n",
    "    Returns:\n",
    "    pd.Series: A series of composite rankings for each case\n",
    "\n",
    "    This function creates a weighted average of the rankings from two centrality measures,\n",
    "    allowing for a balance between predicting high and low ground truth scores.\n",
    "    \"\"\"\n",
    "    high_ranks = rank_cases(df, high_centrality)\n",
    "    low_ranks = rank_cases(df, low_centrality)\n",
    "    return weight * high_ranks + (1 - weight) * low_ranks\n",
    "\n",
    "def find_optimal_weight(df, high_centrality, low_centrality, ground_truth):\n",
    "    \"\"\"\n",
    "    Find the optimal weight for creating a composite ranking.\n",
    "\n",
    "    Args:\n",
    "    df (pd.DataFrame): The dataframe containing the data\n",
    "    high_centrality (str): The name of the centrality measure best for high scores\n",
    "    low_centrality (str): The name of the centrality measure best for low scores\n",
    "    ground_truth (str): The name of the ground truth score column\n",
    "\n",
    "    Returns:\n",
    "    float: The optimal weight (0-1) for the high_centrality ranking\n",
    "\n",
    "    This function tests different weights to find the one that produces the composite ranking\n",
    "    with the highest correlation to the ground truth scores.\n",
    "    \"\"\"\n",
    "    best_corr = -1\n",
    "    best_weight = 0\n",
    "    for weight in np.arange(0, 1.01, 0.01):\n",
    "        composite = create_composite_ranking(df, high_centrality, low_centrality, weight)\n",
    "        corr, _ = stats.spearmanr(composite, df[ground_truth])\n",
    "        if abs(corr) > best_corr:\n",
    "            best_corr = abs(corr)\n",
    "            best_weight = weight\n",
    "    return best_weight\n",
    "\n",
    "def calculate_correlations(df, centralities, ground_truths, composite_ranking):\n",
    "    \"\"\"\n",
    "    Calculate correlations between rankings and ground truth scores.\n",
    "\n",
    "    Args:\n",
    "    df (pd.DataFrame): The dataframe containing the data\n",
    "    centralities (list): List of centrality measure column names\n",
    "    ground_truths (list): List of ground truth score column names\n",
    "    composite_ranking (str): The name of the composite ranking column\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary of correlation coefficients\n",
    "\n",
    "    This function calculates Spearman correlations between the rankings of each centrality measure\n",
    "    (including the composite ranking) and each ground truth score.\n",
    "    \"\"\"\n",
    "    correlations = {}\n",
    "    \n",
    "    for centrality in centralities:\n",
    "        centrality_ranking = rank_cases(df, centrality)\n",
    "        for ground_truth in ground_truths:\n",
    "            corr, _ = stats.spearmanr(centrality_ranking, df[ground_truth])\n",
    "            correlations[(centrality, ground_truth)] = corr\n",
    "    \n",
    "    for ground_truth in ground_truths:\n",
    "        corr, _ = stats.spearmanr(df[composite_ranking], df[ground_truth])\n",
    "        correlations[('composite', ground_truth)] = corr\n",
    "    \n",
    "    return correlations\n",
    "\n",
    "def plot_correlations(correlations, ground_truth, output_file, best_high, best_low):\n",
    "    \"\"\"\n",
    "    Plot correlations between rankings and ground truth scores.\n",
    "\n",
    "    Args:\n",
    "    correlations (dict): Dictionary of correlation coefficients\n",
    "    ground_truth (str): The name of the current ground truth score\n",
    "    output_file (str): The name of the output file for the plot\n",
    "    best_high (str): The name of the best centrality for high scores\n",
    "    best_low (str): The name of the best centrality for low scores\n",
    "\n",
    "    This function creates a bar plot showing the correlations between each centrality measure\n",
    "    (including the composite ranking) and the ground truth scores.\n",
    "    \"\"\"\n",
    "    centralities = list(set([k[0] for k in correlations.keys() if k[0] != 'composite']))\n",
    "    ground_truths = list(set([k[1] for k in correlations.keys()]))\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    # Add text to show which centralities were used for the composite ranking\n",
    "    ax.text(0.02, 0.98, f\"Composite: {best_high} (high) + {best_low} (low)\", \n",
    "            transform=ax.transAxes, ha='left', va='top', \n",
    "            bbox=dict(facecolor='white', edgecolor='gray', alpha=0.8))\n",
    "    \n",
    "    x = np.arange(len(ground_truths))\n",
    "    width = 0.8 / (len(centralities) + 1)\n",
    "    \n",
    "    for i, centrality in enumerate(centralities + ['composite']):\n",
    "        offset = width * i - 0.4 + width/2\n",
    "        rects = ax.bar(x + offset, [correlations[(centrality, gt)] for gt in ground_truths], width, label=centrality)\n",
    "    \n",
    "    ax.set_ylabel('Correlation Coefficient')\n",
    "    ax.set_title(f'Correlations between Rankings and Ground Truths (optimized for {ground_truth})')\n",
    "    ax.set_xticks(x, ground_truths)\n",
    "    ax.legend(loc='best')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_file)\n",
    "    plt.close()\n",
    "\n",
    "def save_correlations_to_csv(correlations, ground_truth, output_file, best_high, best_low):\n",
    "    \"\"\"\n",
    "    Save correlation results to a CSV file.\n",
    "\n",
    "    Args:\n",
    "    correlations (dict): Dictionary of correlation coefficients\n",
    "    ground_truth (str): The name of the current ground truth score\n",
    "    output_file (str): The name of the output CSV file\n",
    "    best_high (str): The name of the best centrality for high scores\n",
    "    best_low (str): The name of the best centrality for low scores\n",
    "\n",
    "    This function saves the correlation results to a CSV file for further analysis or reporting.\n",
    "    \"\"\"\n",
    "    df_correlations = pd.DataFrame(correlations.items(), columns=['Pair', 'Correlation'])\n",
    "    df_correlations[['Centrality', 'Ground Truth']] = pd.DataFrame(df_correlations['Pair'].tolist(), index=df_correlations.index)\n",
    "    df_correlations = df_correlations.drop('Pair', axis=1)\n",
    "    df_correlations.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze Network Function\n",
    "> The `analyze_network()` function performs comprehensive network analysis using various centrality measures and composite rankings.\n",
    "\n",
    "## Input Parameters\n",
    "\n",
    "- `nodes_df`: Pandas DataFrame containing node information, including ground truth scores and node attributes\n",
    "- `edges_df`: Pandas DataFrame containing edge information (connections between nodes)  \n",
    "- `ground_truths`: List of column names in nodes_df that contain ground truth scores to analyze\n",
    "- `centralities`: List of centrality measures to calculate (e.g. degree, betweenness, etc.)\n",
    "- `composite_functions`: List of functions that combine multiple centrality measures into composite rankings\n",
    "- `output_path`: Directory path where analysis outputs will be saved\n",
    "\n",
    "## Processing Steps\n",
    "\n",
    "1. Creates output directory if it doesn't exist\n",
    "2. Makes a copy of the input nodes DataFrame\n",
    "3. For each ground truth measure:\n",
    "   - Calculates an inverted version (max value - original value)\n",
    "   - Stores inverted versions with \"_inverted\" suffix\n",
    "4. Cleans data by:\n",
    "   - Dropping rows with missing ECLI identifiers\n",
    "   - Converting doctypebranch to numeric values if present\n",
    "5. Calculates centrality measures specified\n",
    "6. Creates composite rankings using provided functions\n",
    "7. Computes correlations between:\n",
    "   - Individual centrality measures and ground truths\n",
    "   - Composite rankings and ground truths\n",
    "\n",
    "## Return Value\n",
    "\n",
    "Returns an `AnalysisResults` dictionary containing:\n",
    "- `network_stats`: Basic statistics about the network (nodes, edges, density etc.)\n",
    "- `correlations`: Correlation coefficients between rankings and ground truths\n",
    "- `best_centralities`: Best performing centrality measures for each ground truth\n",
    "- `composite_rankings`: Results of composite ranking calculations\n",
    "- `dataframe`: Final processed DataFrame with all measures included\n",
    "\n",
    "## Output Files\n",
    "\n",
    "Saves various analysis results to the specified output directory, including:\n",
    "- Correlation plots\n",
    "- CSV files with detailed results\n",
    "- Network statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_network(nodes_df: pd.DataFrame, \n",
    "                edges_df: pd.DataFrame, \n",
    "                ground_truths: list, \n",
    "                centralities: list,\n",
    "                composite_functions: list,\n",
    "                output_path: str) -> AnalysisResults:\n",
    "    \"\"\"\n",
    "    Analyze a network using various centrality measures and composite rankings.\n",
    "    \n",
    "    Args:\n",
    "        nodes_df: DataFrame containing node information\n",
    "        edges_df: DataFrame containing edge information\n",
    "        ground_truths: List of ground truth column names to analyze\n",
    "        centralities: List of centrality measures to calculate\n",
    "        composite_functions: List of composite ranking functions to use\n",
    "        output_path: Path where to save output files\n",
    "        \n",
    "    Returns:\n",
    "        AnalysisResults: Dictionary containing:\n",
    "        - network_stats: Basic network statistics\n",
    "        - correlations: Correlation results for each ground truth and composite function\n",
    "        - best_centralities: Best performing centralities for each ground truth\n",
    "        - composite_rankings: Composite ranking results\n",
    "        - dataframe: The final processed dataframe with all measures\n",
    "    \"\"\"\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    \n",
    "    # Make a copy of nodes dataframe\n",
    "    total_df = nodes_df.copy()\n",
    "    \n",
    "    # Invert ground truth values\n",
    "    ground_truths_inverted = []\n",
    "    for truth in ground_truths:\n",
    "        max_value = total_df[truth].max()\n",
    "        inverted_col = f'{truth}_inverted'\n",
    "        total_df[inverted_col] = max_value - total_df[truth]\n",
    "        ground_truths_inverted.append(inverted_col)\n",
    "    \n",
    "    # Drop rows with missing ecli\n",
    "    total_df = total_df.dropna(subset=['ecli'])\n",
    "    \n",
    "    # Convert doctypebranch to numeric if it exists\n",
    "    if 'doctypebranch' in total_df.columns:\n",
    "        total_df['doctypebranch'] = categorise_total_branch_numerically(total_df['doctypebranch'])\n",
    "    \n",
    "    # Create graph\n",
    "    G = nx.DiGraph()\n",
    "    \n",
    "    # Add nodes with attributes\n",
    "    for idx, row in total_df.iterrows():\n",
    "        node_attrs = {truth: row[truth] for truth in ground_truths if truth in row}\n",
    "        G.add_node(row['ecli'], **node_attrs)\n",
    "    \n",
    "    # Add edges between existing nodes\n",
    "    valid_nodes = set(total_df['ecli'].values)\n",
    "    for idx, row in edges_df.iterrows():\n",
    "        source = row['ecli']\n",
    "        targets = row['references']\n",
    "        if source in valid_nodes:\n",
    "            for target in targets:\n",
    "                if target and target in valid_nodes:\n",
    "                    G.add_edge(source, target)\n",
    "    \n",
    "    # Remove self-loops\n",
    "    G.remove_edges_from(nx.selfloop_edges(G))\n",
    "    \n",
    "    # Calculate centrality measures\n",
    "    centrality_measures = calculate_centrality_measures(G)\n",
    "    centrality_df = pd.DataFrame(centrality_measures)\n",
    "    \n",
    "    # Merge centrality measures with total_df\n",
    "    total_df = pd.merge(total_df, centrality_df, left_on='ecli', right_index=True, how='left')\n",
    "    \n",
    "    # Plot initial correlations\n",
    "    numeric_cols = total_df.select_dtypes(include=[float, int]).columns\n",
    "    for centrality in centralities:\n",
    "        if centrality in numeric_cols:\n",
    "            correlation_results = total_df[ground_truths_inverted + [centrality]].corr()[centrality][ground_truths_inverted]\n",
    "            \n",
    "            # Plot correlation results\n",
    "            plt.figure()\n",
    "            correlation_results.plot(kind='bar', \n",
    "                                title=f'Correlation of {centrality} with Ground Truths')\n",
    "            plt.ylabel('Correlation Coefficient')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f'{output_path}/correlation_{centrality}.png')\n",
    "            plt.close()\n",
    "    \n",
    "    # Run composite rankings analysis\n",
    "    for ground_truth in ground_truths:\n",
    "        for composite_function in composite_functions:\n",
    "            print(f\"\\nAnalyzing ground truth: {ground_truth} with {composite_function}\")\n",
    "            \n",
    "            # Find best centralities and create composite ranking\n",
    "            best_high, best_low = find_best_centralities(total_df, centralities, ground_truth)\n",
    "            \n",
    "            if composite_function == 'weight_composite_ranking':\n",
    "                optimal_weight = find_optimal_weight(total_df, best_high, best_low, ground_truth)\n",
    "                composite_ranking = create_composite_ranking(total_df, best_high, best_low, optimal_weight)\n",
    "            else:\n",
    "                composite_ranking = create_treashold_composite_ranking(total_df, best_high, best_low, ground_truth)\n",
    "            \n",
    "            # Add composite ranking to dataframe\n",
    "            ranking_col = f'composite_ranking_{ground_truth}_{composite_function}'\n",
    "            total_df[ranking_col] = composite_ranking\n",
    "            \n",
    "            # Calculate and save correlations\n",
    "            correlations = calculate_correlations(total_df, centralities, ground_truths, ranking_col)\n",
    "            \n",
    "            # Save results\n",
    "            plot_correlations(correlations, ground_truth, \n",
    "                            f'{output_path}/correlations_plot_{ground_truth}_{composite_function}.png',\n",
    "                            best_high, best_low)\n",
    "            \n",
    "            save_correlations_to_csv(correlations, ground_truth,\n",
    "                                f'{output_path}/correlations_{ground_truth}_{composite_function}.csv',\n",
    "                                best_high, best_low)\n",
    "            \n",
    "    # Prepare structured results\n",
    "    results = {\n",
    "        'network_stats': {\n",
    "            'n_nodes': len(total_df),\n",
    "            'n_edges': len(edges_df)\n",
    "        },\n",
    "        'correlations': {},\n",
    "        'best_centralities': {},\n",
    "        'composite_rankings': {},\n",
    "        'dataframe': total_df  # Include the final dataframe\n",
    "    }\n",
    "    \n",
    "    # Store correlations and rankings for each ground truth and composite function\n",
    "    for ground_truth in ground_truths:\n",
    "        results['correlations'][ground_truth] = {}\n",
    "        results['best_centralities'][ground_truth] = {}\n",
    "        results['composite_rankings'][ground_truth] = {}\n",
    "        \n",
    "        for composite_function in composite_functions:\n",
    "            ranking_col = f'composite_ranking_{ground_truth}_{composite_function}'\n",
    "            \n",
    "            # Store correlations\n",
    "            correlations = calculate_correlations(total_df, centralities, ground_truths, ranking_col)\n",
    "            results['correlations'][ground_truth][composite_function] = correlations\n",
    "            \n",
    "            # Store best centralities\n",
    "            best_high, best_low = find_best_centralities(total_df, centralities, ground_truth)\n",
    "            results['best_centralities'][ground_truth] = {\n",
    "                'high': best_high,\n",
    "                'low': best_low\n",
    "            }\n",
    "            \n",
    "            # Store composite rankings\n",
    "            results['composite_rankings'][ground_truth][composite_function] = total_df[ranking_col].to_dict()\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare Function\n",
    "This section implements network comparison functionality to analyze and compare results across different networks.\n",
    "\n",
    "The main function `compare_networks()` takes results from multiple network analyses and performs the following comparisons:\n",
    "\n",
    "1. Correlation Comparisons:\n",
    "   - Compares how different centrality measures correlate with ground truth metrics across networks\n",
    "   - Creates comparison tables showing correlation values for each network\n",
    "   - Saves correlation comparisons to CSV files\n",
    "\n",
    "2. Ranking Comparisons: \n",
    "   - Analyzes how centrality measures rank relative to each other in different networks\n",
    "   - Converts absolute correlation values to rankings\n",
    "   - Shows which centrality measures perform consistently well across networks\n",
    "   - Saves ranking comparisons to CSV files\n",
    "\n",
    "The comparisons are performed for each combination of:\n",
    "- Ground truth metrics (e.g., PageRank, degree centrality)\n",
    "- Composite ranking functions (different ways of combining centrality measures)\n",
    "\n",
    "This allows us to:\n",
    "- Identify which centrality measures work best across different network types\n",
    "- Understand how network structure affects centrality measure performance\n",
    "- Compare the effectiveness of different composite ranking approaches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_networks(network_results: Dict[str, AnalysisResults], output_path: str) -> Tuple[Dict[str, Dict[str, pd.DataFrame]], Dict[str, Dict[str, Dict[str, pd.DataFrame]]]]:\n",
    "    \"\"\"\n",
    "    Compare results across different networks.\n",
    "    \n",
    "    Args:\n",
    "        network_results: Dictionary with network names as keys and their analysis results as values.\n",
    "        output_path: Path to save comparison results.\n",
    "        \n",
    "    Returns:\n",
    "        A tuple containing:\n",
    "        - correlation_comparisons: A dictionary of correlation DataFrames for each ground truth and composite function.\n",
    "        - ranking_comparisons: A dictionary containing ranking DataFrames and rank correlations for each ground truth and composite function.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    \n",
    "    # Compare correlations across networks\n",
    "    correlation_comparisons = {}\n",
    "    ranking_comparisons = {}\n",
    "    \n",
    "    # Get all ground truths and composite functions from first network\n",
    "    first_network = list(network_results.values())[0]\n",
    "    ground_truths = list(first_network['correlations'].keys())\n",
    "    composite_functions = list(first_network['correlations'][ground_truths[0]].keys())\n",
    "    \n",
    "    for ground_truth in ground_truths:\n",
    "        correlation_comparisons[ground_truth] = {}\n",
    "        ranking_comparisons[ground_truth] = {}\n",
    "        \n",
    "        for composite_function in composite_functions:\n",
    "            # Compare absolute correlations\n",
    "            network_correlations = {\n",
    "                network_name: results['correlations'][ground_truth][composite_function]\n",
    "                for network_name, results in network_results.items()\n",
    "            }\n",
    "            \n",
    "            # Create correlation comparison DataFrame\n",
    "            corr_df = pd.DataFrame(network_correlations).round(3)\n",
    "            corr_df.to_csv(f'{output_path}/correlations_{ground_truth}_{composite_function}.csv')\n",
    "            \n",
    "            # Compare relative rankings of centrality measures\n",
    "            centrality_ranks = {}\n",
    "            for network_name, correlations in network_correlations.items():\n",
    "                # Convert correlations to ranks\n",
    "                centrality_corrs = {k[0]: abs(v) for k, v in correlations.items() \n",
    "                                if k[0] != 'composite'}\n",
    "                ranks = pd.Series(centrality_corrs).rank(ascending=False)\n",
    "                centrality_ranks[network_name] = ranks\n",
    "            \n",
    "            # Create ranking comparison DataFrame\n",
    "            rank_df = pd.DataFrame(centrality_ranks).round(3)\n",
    "            rank_df.to_csv(f'{output_path}/ranks_{ground_truth}_{composite_function}.csv')\n",
    "            \n",
    "            # Calculate rank correlation between networks\n",
    "            rank_correlation = rank_df.corr(method='spearman')\n",
    "            rank_correlation.to_csv(f'{output_path}/rank_correlation_{ground_truth}_{composite_function}.csv')\n",
    "            \n",
    "            # Store results\n",
    "            correlation_comparisons[ground_truth][composite_function] = corr_df\n",
    "            ranking_comparisons[ground_truth][composite_function] = {\n",
    "                'centrality_ranks': rank_df,\n",
    "                'rank_correlation': rank_correlation\n",
    "            }\n",
    "    \n",
    "    return correlation_comparisons, ranking_comparisons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Centralities of interest\n",
    "CENTRALITIES = ['degree_centrality', 'in_degree_centrality', 'out_degree_centrality', \n",
    "                'betweenness_centrality', 'closeness_centrality', 'core_number', \n",
    "                'relative_in_degree_centrality', 'eigenvector_centrality', \n",
    "                'pagerank', 'hits_hub', 'hits_authority', 'harmonic_centrality', 'disruption']\n",
    "\n",
    "# Ground truths of interest\n",
    "GROUND_TRUTHS = ['importance', 'doctypebranch']\n",
    "GROUND_TRUTHS_INVERTED = ['importance_inverted',  'doctypebranch_inverted']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze multiple networks\n",
    "network_results = {}\n",
    "networks = {\n",
    "    'network1': {'nodes': nodes_df1, 'edges': edges_df1},\n",
    "    'network2': {'nodes': nodes_df2, 'edges': edges_df2},\n",
    "    # ... more networks\n",
    "}\n",
    "\n",
    "for network_name, data in networks.items():\n",
    "    results = analyze_network(\n",
    "        nodes_df=data['nodes'],\n",
    "        edges_df=data['edges'],\n",
    "        ground_truths=GROUND_TRUTHS,\n",
    "        centralities=CENTRALITIES,\n",
    "        composite_functions=['weight_composite_ranking', 'treashold_composite_ranking'],\n",
    "        output_path=f'results/{network_name}'\n",
    "    )\n",
    "    network_results[network_name] = results\n",
    "\n",
    "# Compare results across networks\n",
    "correlations, rankings = compare_networks(network_results, 'results/comparisons')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
