{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing Centrality Metrics\n",
    "\n",
    "This notebook analyzes and compares different centrality metrics for a citation network.\n",
    "\n",
    "It loads node and edge data from JSON files, calculates various centrality metrics (like degree, betweenness, closeness etc.), and compares them against ground truth measures like importance and document type.\n",
    "\n",
    "The notebook defines constants for:\n",
    "- Input data file paths\n",
    "- Centrality metrics to analyze \n",
    "- Ground truth measures to compare against\n",
    "\n",
    "It also defines TypedDict classes to type the network statistics and results.\n",
    "\n",
    "**Note:** This is an analysis notebook. To modify the code that generates the network and calculates centralities, please refer to the Main section.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import datetime\n",
    "import dateutil.parser\n",
    "import statistics\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from scipy.stats import spearmanr\n",
    "import seaborn as sns\n",
    "import os\n",
    "import json\n",
    "from typing import Dict, List, Union, TypedDict, Tuple\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetworkStats(TypedDict):\n",
    "    n_nodes: int\n",
    "    n_edges: int\n",
    "\n",
    "class BestCentralities(TypedDict):\n",
    "    high: str\n",
    "    low: str\n",
    "\n",
    "class AnalysisResults(TypedDict):\n",
    "    network_stats: NetworkStats\n",
    "    correlations: Dict[str, Dict[str, Dict[tuple[str, str], float]]]  # ground_truth -> composite_function -> (centrality, ground_truth) -> correlation\n",
    "    best_centralities: Dict[str, BestCentralities]  # ground_truth -> best centralities\n",
    "    composite_rankings: Dict[str, Dict[str, Dict[str, float]]]  # ground_truth -> composite_function -> ecli -> rank\n",
    "    dataframe: pd.DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert doctypebranch to a numeric value\n",
    "def categorise_total_branch_numerically(branches: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Convert branch categorisation from strings into numbers.\n",
    "    \n",
    "    :param branches: The column containing branch data, categorized with strings.\n",
    "    :return: A pandas Series with numerical categorization.\n",
    "    \"\"\"\n",
    "    mapping = {\n",
    "        \"GRANDCHAMBER\": 1,\n",
    "        \"CHAMBER\": 2,\n",
    "        \"COMMITTEE\": 3,\n",
    "    }\n",
    "    \n",
    "    # Convert to uppercase to ensure consistent matching\n",
    "    branches = branches.str.upper()\n",
    "    \n",
    "    # Print any values that don't match our mapping\n",
    "    unmapped = set(branches.unique()) - set(mapping.keys())\n",
    "    if unmapped:\n",
    "        print(f\"Warning: Found unmapped values: {unmapped}\")\n",
    "    \n",
    "    return branches.map(mapping)\n",
    "\n",
    "def prep_data(df: pd.DataFrame, include: list) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Prepare the dataset by selecting the appropriate columns and filtering out rows with uncomputed metric values.\n",
    "    \n",
    "    :param df: The DataFrame to process.\n",
    "    :param include: Columns to include.\n",
    "    :return: The processed DataFrame.\n",
    "    \"\"\"\n",
    "    headers = include + ['ecli']  # Ensure essential columns are included\n",
    "    headers = list(set(headers))  # Removing duplicates\n",
    "\n",
    "    data = df[headers]\n",
    "\n",
    "    # Filter out rows with uncomputed metric values (-2)\n",
    "    metric_column = include[-1]\n",
    "    data = data[data[metric_column] >= -1]\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Centrality Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_disruptions_new(graph: nx.Graph) -> dict:\n",
    "    \"\"\"\n",
    "    Calculate the disruption score for each node in the graph.\n",
    "\n",
    "    Args:\n",
    "        graph (nx.Graph): The input directed graph.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with nodes as keys and their disruption scores as values.\n",
    "    \"\"\"\n",
    "    disruptions = {}\n",
    "    for node in graph.nodes:\n",
    "        i, j, k = 0, 0, 0\n",
    "\n",
    "        # count j - papers that cite both the current paper and its references\n",
    "        for in_node in graph.predecessors(node):\n",
    "            for out_node in graph.successors(node):\n",
    "                if graph.has_edge(in_node, out_node):\n",
    "                    j += 1\n",
    "                    break\n",
    "\n",
    "        # count i - papers that only cite the current paper\n",
    "        i = graph.in_degree(node) - j\n",
    "\n",
    "        # count k - papers that only cite papers cited by the current paper\n",
    "        for out_node in graph.successors(node):\n",
    "            for in_out_node in graph.predecessors(out_node):\n",
    "                if in_out_node != node and not graph.has_edge(in_out_node, node):\n",
    "                    k += 1\n",
    "\n",
    "        # Calculate disruption index with better edge case handling\n",
    "        denominator = i + j + k\n",
    "        if denominator == 0:\n",
    "            # If paper has no citations and cites no one, assign neutral disruption\n",
    "            disruptions[node] = 0.0\n",
    "        else:\n",
    "            disruptions[node] = (i - j) / denominator\n",
    "\n",
    "    # Verify we have at least some non-zero values\n",
    "    values = list(disruptions.values())\n",
    "    if all(v == 0 for v in values):\n",
    "        print(\"Warning: All disruption values are 0\")\n",
    "    if len(set(values)) == 1:\n",
    "        print(f\"Warning: All disruption values are identical: {values[0]}\")\n",
    "\n",
    "    return disruptions\n",
    "\n",
    "\n",
    "# Function to calculate centrality measures\n",
    "def calculate_centrality_measures(graph):\n",
    "    \"\"\"Calculate various centrality measures for the graph.\"\"\"\n",
    "    measures = {\n",
    "        'degree_centrality': nx.degree_centrality(graph),\n",
    "        'in_degree_centrality': nx.in_degree_centrality(graph),\n",
    "        'out_degree_centrality': nx.out_degree_centrality(graph),\n",
    "        'betweenness_centrality': nx.betweenness_centrality(graph),\n",
    "        'closeness_centrality': nx.closeness_centrality(graph),\n",
    "        'core_number': nx.core_number(graph),\n",
    "        'relative_in_degree_centrality': {node: degree/len(graph) \n",
    "                                        for node, degree in graph.in_degree()},\n",
    "        'harmonic_centrality': nx.harmonic_centrality(graph)\n",
    "    }\n",
    "    \n",
    "    # Handle potentially failing measures with try-except\n",
    "    try:\n",
    "        measures['eigenvector_centrality'] = nx.eigenvector_centrality(graph, max_iter=1000)\n",
    "    except (nx.PowerIterationFailedConvergence, nx.NetworkXError):\n",
    "        # Fill with zeros if calculation fails\n",
    "        measures['eigenvector_centrality'] = {node: 0.0 for node in graph.nodes()}\n",
    "    \n",
    "    try:\n",
    "        measures['pagerank'] = nx.pagerank(graph)\n",
    "    except:\n",
    "        measures['pagerank'] = {node: 0.0 for node in graph.nodes()}\n",
    "    \n",
    "    try:\n",
    "        hub_dict, authority_dict = nx.hits(graph)\n",
    "        measures['hits_hub'] = hub_dict\n",
    "        measures['hits_authority'] = authority_dict\n",
    "    except:\n",
    "        measures['hits_hub'] = {node: 0.0 for node in graph.nodes()}\n",
    "        measures['hits_authority'] = {node: 0.0 for node in graph.nodes()}\n",
    "    \n",
    "    try:\n",
    "        measures['disruption'] = calculate_disruptions_new(graph)\n",
    "        # Check if disruption values make sense (between -1 and 1)\n",
    "        disruption_values = list(measures['disruption'].values())\n",
    "        if not all(-1 <= v <= 1 for v in disruption_values if not np.isnan(v)):\n",
    "            raise ValueError(\"Disruption values outside valid range [-1,1]\")\n",
    "        \n",
    "        # Check if all disruption values are identical (excluding NaN)\n",
    "        non_nan_values = [v for v in disruption_values if not np.isnan(v)]\n",
    "        if len(non_nan_values) > 0 and all(v == non_nan_values[0] for v in non_nan_values):\n",
    "            raise ValueError(\"All disruption values are identical\")\n",
    "    except Exception as e:\n",
    "        measures['disruption'] = {node: 0.0 for node in graph.nodes()}\n",
    "        \n",
    "    return measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Composite Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script analyzes the relationship between various centrality measures and ground truth scores\n",
    "for legal cases. It aims to find the best centrality measures for predicting high and low relevance\n",
    "scores, create a composite ranking, and evaluate its performance against individual centrality measures.\n",
    "\n",
    "The main steps are:\n",
    "1. Plot error bars for centrality measures vs. ground truth scores\n",
    "2. Find the best centrality measures for predicting high and low scores\n",
    "3. Create a composite ranking using the best measures\n",
    "4. Calculate correlations between rankings and ground truth scores\n",
    "5. Visualize and save the results\n",
    "\"\"\"\n",
    "\n",
    "def plot_error_bars(df, centrality, ground_truth):\n",
    "    \"\"\"\n",
    "    Plot error bars for a given centrality measure against a ground truth score.\n",
    "\n",
    "    Args:\n",
    "    df (pd.DataFrame): The dataframe containing the data\n",
    "    centrality (str): The name of the centrality measure column\n",
    "    ground_truth (str): The name of the ground truth score column\n",
    "\n",
    "    This function visualizes the relationship between a centrality measure and a ground truth score,\n",
    "    showing the mean centrality value for each ground truth score category along with error bars\n",
    "    representing the standard deviation.\n",
    "    \"\"\"\n",
    "    # Prepare data\n",
    "    include = [ground_truth, centrality]\n",
    "    data = prep_data(df, include)\n",
    "\n",
    "    x_header = centrality\n",
    "    y_header = ground_truth\n",
    "    x, y = list(data[x_header]), list(data[y_header])\n",
    "    categories = list(set(y))\n",
    "    categories.sort()\n",
    "    num_categories, num_instances = len(categories), len(x)\n",
    "    y_instances = [[] for _ in range(num_categories)]\n",
    "    for category_no in range(num_categories):\n",
    "        for instance_no in range(num_instances):\n",
    "            if y[instance_no] == categories[category_no]:\n",
    "                y_instances[category_no].append(x[instance_no])\n",
    "    x = [statistics.mean(y_instances[category_no]) for category_no in range(num_categories)]\n",
    "    y = categories\n",
    "\n",
    "    # Draw graph\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    title = f\"{centrality.capitalize()} vs Average {y_header.capitalize()}\"\n",
    "    plt.suptitle(title, fontsize=22)\n",
    "    plt.xlabel(f\"{centrality.capitalize()}\", fontsize=22)\n",
    "    plt.ylabel(f\"{y_header.capitalize()}\", fontsize=22)\n",
    "    plt.yticks(categories, fontsize=16)\n",
    "\n",
    "    # Calculate error bars\n",
    "    stds = [statistics.stdev(y_instances[category_no]) for category_no in range(num_categories)]\n",
    "    plt.errorbar(x, y, xerr=stds, fmt='o')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def find_best_centralities(df, centralities, ground_truth):\n",
    "    \"\"\"\n",
    "    Find the best centrality measures for predicting high and low ground truth scores.\n",
    "    TODO: Include considerations for multiple ground truth scores\n",
    "\n",
    "    Args:\n",
    "    df (pd.DataFrame): The dataframe containing the data\n",
    "    centralities (list): List of centrality measure column names\n",
    "    ground_truth (str): The name of the ground truth score column\n",
    "\n",
    "    Returns:\n",
    "    tuple: (best_high, best_low) - the names of the best centrality measures for high and low scores\n",
    "\n",
    "    This function calculates the Spearman correlation between each centrality measure and the ground truth,\n",
    "    using 1 - |correlation| as an error metric. The centrality with the lowest error is chosen as best_high,\n",
    "    and the second-lowest (excluding best_high) is chosen as best_low.\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO For COURTBRANCH: When selecting the optimal metric for the lower class (less importance) we weight it against the middle class.\n",
    "\n",
    "    errors = {}\n",
    "    \n",
    "    for centrality in centralities:\n",
    "        # Calculate correlation across the full range\n",
    "        corr, _ = stats.spearmanr(df[centrality], df[ground_truth])\n",
    "        errors[centrality] = 1 - abs(corr)  # Use 1 - |correlation| as error\n",
    "    \n",
    "    best_high = min(errors, key=errors.get)\n",
    "    \n",
    "    # Remove the best_high centrality from consideration for best_low\n",
    "    errors.pop(best_high, None)\n",
    "    \n",
    "    best_low = min(errors, key=errors.get)\n",
    "    \n",
    "    return best_high, best_low\n",
    "\n",
    "\n",
    "def find_best_centralities_updated(df, centralities, ground_truth):\n",
    "    \"\"\"\n",
    "    Find the best centrality measures for predicting high and low ground truth scores.\n",
    "    TODO: Include considerations for multiple ground truth scores\n",
    "\n",
    "    Args:\n",
    "    df (pd.DataFrame): The dataframe containing the data\n",
    "    centralities (list): List of centrality measure column names\n",
    "    ground_truth (str): The name of the ground truth score column\n",
    "\n",
    "    Returns:\n",
    "    tuple: (best_high, best_low) - the names of the best centrality measures for high and low scores\n",
    "\n",
    "    This function calculates the Spearman correlation between each centrality measure and the ground truth,\n",
    "    using 1 - |correlation| as an error metric. The centrality with the lowest error is chosen as best_high,\n",
    "    and the second-lowest (excluding best_high) is chosen as best_low.\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO For COURTBRANCH: When selecting the optimal metric for the lower class (less importance) we weight it against the middle class.\n",
    "\n",
    "    errors = {}\n",
    "    \n",
    "    for centrality in centralities:\n",
    "        # Calculate correlation across the full range\n",
    "        corr, _ = stats.spearmanr(df[centrality], df[ground_truth])\n",
    "        errors[centrality] = 1 - abs(corr)  # Use 1 - |correlation| as error\n",
    "    \n",
    "    best_high = min(errors, key=errors.get)\n",
    "    \n",
    "    # Remove the best_high centrality from consideration for best_low\n",
    "    errors.pop(best_high, None)\n",
    "    \n",
    "    best_low = min(errors, key=errors.get)\n",
    "    \n",
    "    return best_high, best_low\n",
    "\n",
    "def rank_cases(df, centrality):\n",
    "    \"\"\"\n",
    "    Rank cases based on a given centrality measure.\n",
    "\n",
    "    Args:\n",
    "    df (pd.DataFrame): The dataframe containing the data\n",
    "    centrality (str): The name of the centrality measure column\n",
    "\n",
    "    Returns:\n",
    "    pd.Series: A series of rankings for each case\n",
    "\n",
    "    This function ranks the cases in descending order of the centrality measure,\n",
    "    with the highest centrality receiving rank 1.\n",
    "    \"\"\"\n",
    "    return df[centrality].rank(ascending=False)\n",
    "\n",
    "def create_treashold_composite_ranking(df, high_centrality, low_centrality, ground_truth):\n",
    "    \"\"\"\n",
    "    Create a composite ranking based on a threshold approach using two centrality measures.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame containing the centrality measures\n",
    "        high_centrality: Centrality measure that performs best for high scores\n",
    "        low_centrality: Centrality measure that performs best for low scores\n",
    "        ground_truth: Name of the ground truth column\n",
    "        \n",
    "    Returns:\n",
    "        Series containing the composite ranking\n",
    "    \"\"\"\n",
    "    # Normalize both centrality measures to [0,1] range\n",
    "    high_normalized = (df[high_centrality] - df[high_centrality].min()) / (df[high_centrality].max() - df[high_centrality].min())\n",
    "    low_normalized = (df[low_centrality] - df[low_centrality].min()) / (df[low_centrality].max() - df[low_centrality].min())\n",
    "    \n",
    "    # Find optimal threshold by testing different values\n",
    "    thresholds = np.linspace(0, 1, 100)\n",
    "    best_correlation = -1\n",
    "    optimal_threshold = 0\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        # Create composite ranking using current threshold\n",
    "        composite = np.where(\n",
    "            high_normalized > threshold,\n",
    "            high_normalized,  # Use high centrality measure\n",
    "            low_normalized    # Use low centrality measure\n",
    "        )\n",
    "        \n",
    "        # Calculate correlation with ground truth\n",
    "        correlation = abs(spearmanr(composite, df[ground_truth])[0])\n",
    "        \n",
    "        # Update if better correlation found\n",
    "        if correlation > best_correlation:\n",
    "            best_correlation = correlation\n",
    "            optimal_threshold = threshold\n",
    "    \n",
    "    # Create final composite ranking using optimal threshold\n",
    "    final_composite = np.where(\n",
    "        high_normalized > optimal_threshold,\n",
    "        high_normalized,\n",
    "        low_normalized\n",
    "    )\n",
    "    \n",
    "    print(f\"Optimal threshold found: {optimal_threshold:.6f}\")\n",
    "    return final_composite, optimal_threshold\n",
    "\n",
    "def create_composite_ranking(df, high_centrality, low_centrality, weight):\n",
    "    \"\"\"\n",
    "    Create a composite ranking using two centrality measures.\n",
    "    The weight is the weight given to the high_centrality ranking (0-1), different from 0 and 1.\n",
    "\n",
    "    Args:\n",
    "    df (pd.DataFrame): The dataframe containing the data\n",
    "    high_centrality (str): The name of the centrality measure best for high scores\n",
    "    low_centrality (str): The name of the centrality measure best for low scores\n",
    "    weight (float): The weight given to the high_centrality ranking (0-1)\n",
    "\n",
    "    Returns:\n",
    "    pd.Series: A series of composite rankings for each case\n",
    "\n",
    "    This function creates a weighted average of the rankings from two centrality measures,\n",
    "    allowing for a balance between predicting high and low ground truth scores.\n",
    "    \"\"\"\n",
    "    high_ranks = rank_cases(df, high_centrality)\n",
    "    low_ranks = rank_cases(df, low_centrality)\n",
    "    return weight * high_ranks + (1 - weight) * low_ranks\n",
    "\n",
    "def find_optimal_weight(df, high_centrality, low_centrality, ground_truth):\n",
    "    \"\"\"\n",
    "    Find the optimal weight for creating a composite ranking. \n",
    "    The weight is the weight given to the high_centrality ranking (0-1), different from 0 and 1.\n",
    "    Args:\n",
    "    df (pd.DataFrame): The dataframe containing the data\n",
    "    high_centrality (str): The name of the centrality measure best for high scores\n",
    "    low_centrality (str): The name of the centrality measure best for low scores\n",
    "    ground_truth (str): The name of the ground truth score column\n",
    "\n",
    "    Returns:\n",
    "    float: The optimal weight (0-1) for the high_centrality ranking\n",
    "\n",
    "    This function tests different weights to find the one that produces the composite ranking\n",
    "    with the highest correlation to the ground truth scores.\n",
    "    \"\"\"\n",
    "    best_corr = -1\n",
    "    best_weight = 0.5  # Initialize to middle value\n",
    "    for weight in np.arange(0.01, 1.00, 0.01):  # Exclude 0 and 1\n",
    "        composite = create_composite_ranking(df, high_centrality, low_centrality, weight)\n",
    "        corr, _ = stats.spearmanr(composite, df[ground_truth])\n",
    "        if abs(corr) > best_corr:\n",
    "            best_corr = abs(corr)\n",
    "            best_weight = weight\n",
    "    return best_weight\n",
    "\n",
    "def calculate_correlations(df, centralities, ground_truths, composite_ranking=None):\n",
    "    \"\"\"\n",
    "    Calculate correlations between rankings and ground truth scores.\n",
    "\n",
    "    Args:\n",
    "    df (pd.DataFrame): The dataframe containing the data\n",
    "    centralities (list): List of centrality measure column names\n",
    "    ground_truths (list): List of ground truth score column names\n",
    "    composite_ranking (str): The name of the composite ranking column\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary of correlation coefficients with string keys\n",
    "\n",
    "    This function calculates Spearman correlations between the rankings of each centrality measure\n",
    "    (including the composite ranking) and each ground truth score.\n",
    "    \"\"\"\n",
    "    correlations = {}\n",
    "    \n",
    "    for centrality in centralities:\n",
    "        centrality_ranking = rank_cases(df, centrality)\n",
    "        for ground_truth in ground_truths:\n",
    "            # Check if arrays have variation before calculating correlation\n",
    "            if df[centrality].nunique() > 1 and df[ground_truth].nunique() > 1:\n",
    "                corr, _ = stats.spearmanr(centrality_ranking, df[ground_truth])\n",
    "                correlations[(centrality, ground_truth)] = corr\n",
    "            else:\n",
    "                correlations[(centrality, ground_truth)] = np.nan\n",
    "    \n",
    "    if composite_ranking is not None:\n",
    "        for ground_truth in ground_truths:\n",
    "            if df[composite_ranking].nunique() > 1 and df[ground_truth].nunique() > 1:\n",
    "                corr, _ = stats.spearmanr(df[composite_ranking], df[ground_truth])\n",
    "                correlations[('composite', ground_truth)] = corr\n",
    "            else:\n",
    "                correlations[('composite', ground_truth)] = np.nan\n",
    "    \n",
    "    return correlations\n",
    "\n",
    "def calculate_centrality_correlations(df, centralities, ground_truths):\n",
    "    \"\"\"\n",
    "    Calculate correlations between rankings and ground truth scores.\n",
    "    \"\"\"\n",
    "    correlations = {}\n",
    "    for centrality in centralities:\n",
    "        for ground_truth in ground_truths:\n",
    "            # Check if arrays have variation before calculating correlation\n",
    "            if df[centrality].nunique() > 1 and df[ground_truth].nunique() > 1:\n",
    "                corr, _ = stats.spearmanr(df[centrality], df[ground_truth])\n",
    "                correlations[(centrality, ground_truth)] = corr\n",
    "            else:\n",
    "                correlations[(centrality, ground_truth)] = np.nan\n",
    "    return correlations\n",
    "\n",
    "def plot_correlations(correlations, ground_truth, output_file, best_high, best_low, composite_param):\n",
    "    \"\"\"\n",
    "    Plot correlations between rankings and ground truth scores.\n",
    "\n",
    "    Args:\n",
    "    correlations (dict): Dictionary of correlation coefficients\n",
    "    ground_truth (str): The name of the current ground truth score\n",
    "    output_file (str): The name of the output file for the plot\n",
    "    best_high (str): The name of the best centrality for high scores\n",
    "    best_low (str): The name of the best centrality for low scores\n",
    "\n",
    "    This function creates a bar plot showing the correlations between each centrality measure\n",
    "    (including the composite ranking) and the ground truth scores.\n",
    "    \"\"\"\n",
    "    centralities = list(set([k[0] for k in correlations.keys() if k[0] != 'composite']))\n",
    "    ground_truths = list(set([k[1] for k in correlations.keys()]))\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    # Add text to show which centralities were used for the composite ranking\n",
    "    text = f\"Composite: {best_high} (high) + {best_low} (low)\"\n",
    "    if composite_param is not None:\n",
    "        text += f\"\\nComposite Param: {composite_param}\"\n",
    "    ax.text(0.02, 0.98, text,\n",
    "            transform=ax.transAxes, ha='left', va='top', \n",
    "            bbox=dict(facecolor='white', edgecolor='gray', alpha=0.6))\n",
    "    \n",
    "    x = np.arange(len(ground_truths))\n",
    "    width = 0.8 / (len(centralities) + 1)\n",
    "    \n",
    "    for i, centrality in enumerate(centralities + ['composite']):\n",
    "        offset = width * i - 0.4 + width/2\n",
    "        rects = ax.bar(x + offset, [correlations[(centrality, gt)] for gt in ground_truths], width, label=centrality)\n",
    "    \n",
    "    ax.set_ylabel('Correlation Coefficient')\n",
    "    ax.set_title(f'Correlations between Rankings and Ground Truths (optimized for {ground_truth})')\n",
    "    ax.set_xticks(x, ground_truths)\n",
    "    ax.legend(loc='best')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_file)\n",
    "    plt.close()\n",
    "\n",
    "def plot_centrality_vs_ground_truth(df, centrality, ground_truth, output_path):\n",
    "    \"\"\"\n",
    "    Plot centrality measure against ground truth metrics with error bars.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing the data\n",
    "        centrality (str): Name of the centrality measure to plot\n",
    "        ground_truths (list): List of ground truth measures to compare against\n",
    "        output_path (str): Path to save the output plots\n",
    "    \"\"\"\n",
    "    # Prepare data\n",
    "    data = df[[ground_truth, centrality]].copy()\n",
    "    data = data[data[centrality] != -2]  # Remove uncomputed values\n",
    "    \n",
    "    # Group by ground truth value and calculate statistics\n",
    "    grouped_stats = data.groupby(ground_truth)[centrality].agg(['mean', 'std']).reset_index()\n",
    "    \n",
    "    # Create the plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Plot points and error bars\n",
    "    plt.errorbar(grouped_stats['mean'], \n",
    "                grouped_stats[ground_truth],\n",
    "                xerr=grouped_stats['std'],\n",
    "                fmt='o',  # Changed from 'o-' to 'o' to remove connecting line\n",
    "                capsize=5,\n",
    "                capthick=1,\n",
    "                elinewidth=1,\n",
    "                markersize=8)\n",
    "    \n",
    "    # Customize the plot\n",
    "    plt.title(f'{centrality} vs. Average {ground_truth}', fontsize=16)\n",
    "    plt.xlabel(centrality.replace('_', ' ').title(), fontsize=16)\n",
    "    plt.ylabel(ground_truth.replace('_', ' ').title(), fontsize=16)\n",
    "    plt.yticks(grouped_stats[ground_truth], fontsize=16)\n",
    "    \n",
    "    # Add grid\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Adjust layout and save\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{output_path}/{centrality}_{ground_truth}_error_bars.png', \n",
    "                bbox_inches='tight', \n",
    "                dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "def save_correlations_to_csv(correlations, ground_truth, output_file, best_high, best_low):\n",
    "    \"\"\"\n",
    "    Save correlation results to a CSV file.\n",
    "\n",
    "    Args:\n",
    "    correlations (dict): Dictionary of correlation coefficients\n",
    "    ground_truth (str): The name of the current ground truth score\n",
    "    output_file (str): The name of the output CSV file\n",
    "    best_high (str): The name of the best centrality for high scores\n",
    "    best_low (str): The name of the best centrality for low scores\n",
    "\n",
    "    This function saves the correlation results to a CSV file for further analysis or reporting.\n",
    "    \"\"\"\n",
    "    df_correlations = pd.DataFrame(correlations.items(), columns=['Pair', 'Correlation'])\n",
    "    df_correlations[['Centrality', 'Ground Truth']] = pd.DataFrame(df_correlations['Pair'].tolist(), index=df_correlations.index)\n",
    "    df_correlations = df_correlations.drop('Pair', axis=1)\n",
    "    df_correlations.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze Network Function\n",
    "> The `analyze_network()` function performs comprehensive network analysis using various centrality measures and composite rankings.\n",
    "\n",
    "## Input Parameters\n",
    "\n",
    "- `nodes_df`: Pandas DataFrame containing node information, including ground truth scores and node attributes\n",
    "- `edges_df`: Pandas DataFrame containing edge information (connections between nodes)  \n",
    "- `ground_truths`: List of column names in nodes_df that contain ground truth scores to analyze\n",
    "- `centralities`: List of centrality measures to calculate (e.g. degree, betweenness, etc.)\n",
    "- `composite_functions`: List of functions that combine multiple centrality measures into composite rankings\n",
    "- `output_path`: Directory path where analysis outputs will be saved\n",
    "\n",
    "## Processing Steps\n",
    "\n",
    "1. Creates output directory if it doesn't exist\n",
    "2. Makes a copy of the input nodes DataFrame\n",
    "3. For each ground truth measure:\n",
    "   - Calculates an inverted version (max value - original value)\n",
    "   - Stores inverted versions with \"_inverted\" suffix\n",
    "4. Cleans data by:\n",
    "   - Dropping rows with missing ECLI identifiers\n",
    "   - Converting doctypebranch to numeric values if present\n",
    "5. Calculates centrality measures specified\n",
    "6. Creates composite rankings using provided functions\n",
    "7. Computes correlations between:\n",
    "   - Individual centrality measures and ground truths\n",
    "   - Composite rankings and ground truths\n",
    "\n",
    "## Return Value\n",
    "\n",
    "Returns an `AnalysisResults` dictionary containing:\n",
    "- `network_stats`: Basic statistics about the network (nodes, edges, density etc.)\n",
    "- `correlations`: Correlation coefficients between rankings and ground truths\n",
    "- `best_centralities`: Best performing centrality measures for each ground truth\n",
    "- `composite_rankings`: Results of composite ranking calculations\n",
    "- `dataframe`: Final processed DataFrame with all measures included\n",
    "\n",
    "## Output Files\n",
    "\n",
    "Saves various analysis results to the specified output directory, including:\n",
    "- Correlation plots\n",
    "- CSV files with detailed results\n",
    "- Network statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_network(nodes_df: pd.DataFrame, \n",
    "                   edges_df: pd.DataFrame, \n",
    "                   ground_truths: List[str],\n",
    "                   centralities: List[str],\n",
    "                   composite_functions: List[str],\n",
    "                   output_path: str,\n",
    "                   network_name: str = None) -> Dict:\n",
    "    \"\"\"\n",
    "    Analyze network using various centrality measures and composite rankings.\n",
    "    \n",
    "    Args:\n",
    "        nodes_df: DataFrame containing node information\n",
    "        edges_df: DataFrame containing edge information\n",
    "        ground_truths: List of ground truth scores to analyze\n",
    "        centralities: List of centrality measures to calculate\n",
    "        composite_functions: List of functions for composite rankings\n",
    "        output_path: Directory path for saving outputs\n",
    "        network_name: Name of the network being analyzed (for tracking performance)\n",
    "    \n",
    "    Returns:\n",
    "        Dict containing analysis results\n",
    "    \"\"\"\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "    # Make a copy of nodes dataframe\n",
    "    total_df = nodes_df.copy()\n",
    "\n",
    "    # Convert doctypebranch to numeric if it exists\n",
    "    if 'doctypebranch' in total_df.columns:\n",
    "        total_df['doctypebranch'] = categorise_total_branch_numerically(total_df['doctypebranch'])\n",
    "        # Remove rows with Nan doctypebranch\n",
    "        total_df = total_df.dropna(subset=['doctypebranch'])\n",
    "\n",
    "    # Convert ground truth columns to numeric\n",
    "    for truth in ground_truths:\n",
    "        total_df[truth] = pd.to_numeric(total_df[truth], errors='coerce')\n",
    "    \n",
    "    # Invert ground truth values\n",
    "    ground_truths_inverted = []\n",
    "    for truth in ground_truths:\n",
    "        max_value = total_df[truth].max()\n",
    "        inverted_col = f'{truth}_inverted'\n",
    "        total_df[inverted_col] = max_value - total_df[truth]\n",
    "        ground_truths_inverted.append(inverted_col)\n",
    "    \n",
    "    # Drop rows with missing ecli\n",
    "    total_df = total_df.dropna(subset=['ecli'])\n",
    "    \n",
    "    # Create graph\n",
    "    G = nx.DiGraph()\n",
    "    \n",
    "    # Add nodes with attributes\n",
    "    for idx, row in total_df.iterrows():\n",
    "        node_attrs = {truth: row[truth] for truth in ground_truths if truth in row}\n",
    "        G.add_node(row['ecli'], **node_attrs)\n",
    "    \n",
    "    # Add edges between existing nodes\n",
    "    valid_nodes = set(total_df['ecli'].values)\n",
    "    for idx, row in edges_df.iterrows():\n",
    "        source = row['ecli']\n",
    "        targets = row['references']\n",
    "        if source in valid_nodes:\n",
    "            for target in targets:\n",
    "                if target and target in valid_nodes:\n",
    "                    G.add_edge(source, target)\n",
    "    \n",
    "    # Remove self-loops\n",
    "    G.remove_edges_from(nx.selfloop_edges(G))\n",
    "\n",
    "    # Calculate network statistics\n",
    "    network_stats = {\n",
    "        'num_nodes': len(nodes_df),\n",
    "        'num_edges': len(edges_df),\n",
    "        'density': nx.density(G),\n",
    "        'average_degree': sum(dict(G.degree()).values()) / G.number_of_nodes(),\n",
    "    }\n",
    "    \n",
    "    # Initialize analysis dictionary\n",
    "    analysis = {\n",
    "        gt: {\n",
    "            'centrality_counts': {\n",
    "                'high': defaultdict(int),\n",
    "                'low': defaultdict(int),\n",
    "                'best_overall': defaultdict(int)\n",
    "            },\n",
    "            'combination_counts': defaultdict(int),\n",
    "            'composite_performance': {\n",
    "                'outperformed_count': 0,\n",
    "                'best_combinations': []\n",
    "            },\n",
    "            'network_performance': {},  # Track performance by network\n",
    "            'correlations': defaultdict(dict),  # Store all correlations\n",
    "            'optimal_weights': defaultdict(float),  # Store optimal weights\n",
    "            'class_distribution': {},  # Store class distribution\n",
    "            'composite_rankings': {},  # Store composite rankings\n",
    "            'best_centralities': {}  # Store best centralities\n",
    "        } for gt in ground_truths\n",
    "    }\n",
    "    \n",
    "    # Calculate centrality measures\n",
    "    centrality_measures = calculate_centrality_measures(G)\n",
    "    centrality_df = pd.DataFrame(centrality_measures)\n",
    "    \n",
    "    # Merge centrality measures with total_df\n",
    "    total_df = pd.merge(total_df, centrality_df, left_on='ecli', right_index=True, how='left')\n",
    "    \n",
    "    # Plot initial correlations\n",
    "    numeric_cols = total_df.select_dtypes(include=[float, int]).columns\n",
    "    \n",
    "    # Analyze each ground truth score\n",
    "    for ground_truth in ground_truths:\n",
    "        print(f\"\\nAnalyzing ground truth: {ground_truth}\")\n",
    "\n",
    "        for centrality in centralities:\n",
    "            if centrality in numeric_cols:\n",
    "                plot_centrality_vs_ground_truth(total_df, centrality, ground_truth, output_path)\n",
    "        print(f\"Plotted {len(centralities)} centrality vs ground truth plots\")\n",
    "        \n",
    "        # Calculate class distribution\n",
    "        class_distribution = nodes_df[ground_truth].value_counts().to_dict()\n",
    "        analysis[ground_truth]['class_distribution'] = class_distribution\n",
    "        \n",
    "        # Calculate correlations for all centrality measures\n",
    "        centrality_correlations = {}\n",
    "        for centrality in centralities:\n",
    "            corr, p_value = stats.spearmanr(total_df[centrality], total_df[ground_truth])\n",
    "            centrality_correlations[centrality] = abs(corr)\n",
    "            analysis[ground_truth]['correlations'][centrality] = {\n",
    "                'correlation': corr,\n",
    "                'p_value': p_value,\n",
    "                'abs_correlation': abs(corr)\n",
    "            }\n",
    "        \n",
    "        # Find best centralities\n",
    "        best_high = max(centrality_correlations.items(), key=lambda x: x[1])[0]\n",
    "        \n",
    "        # Remove best_high from consideration for best_low\n",
    "        low_correlations = {k: v for k, v in centrality_correlations.items() if k != best_high}\n",
    "        best_low = max(low_correlations.items(), key=lambda x: x[1])[0]\n",
    "        \n",
    "        # Find best overall centrality\n",
    "        best_centrality = max(centrality_correlations.items(), key=lambda x: x[1])[0]\n",
    "        \n",
    "        # Store best centralities\n",
    "        analysis[ground_truth]['best_centralities'] = {\n",
    "            'high': best_high,\n",
    "            'low': best_low,\n",
    "            'overall': best_centrality\n",
    "        }\n",
    "        \n",
    "        print(f\"Best high correlation: {best_high}\")\n",
    "        print(f\"Best low correlation: {best_low}\")\n",
    "        \n",
    "        # Update centrality counts\n",
    "        analysis[ground_truth]['centrality_counts']['high'][best_high] += 1\n",
    "        analysis[ground_truth]['centrality_counts']['low'][best_low] += 1\n",
    "        analysis[ground_truth]['centrality_counts']['best_overall'][best_centrality] += 1\n",
    "        \n",
    "        # Record network-specific performance if network_name is provided\n",
    "        if network_name:\n",
    "            analysis[ground_truth]['network_performance'][network_name] = {\n",
    "                'high': best_high,\n",
    "                'low': best_low,\n",
    "                'best_overall': best_centrality,\n",
    "                'correlations': centrality_correlations.copy()\n",
    "            }\n",
    "        \n",
    "        # Create and analyze composite rankings\n",
    "        for composite_function in composite_functions:\n",
    "            combination = f\"{best_high}_{best_low}\"\n",
    "            \n",
    "            # Create composite ranking based on function type\n",
    "            if composite_function == 'weight_composite_ranking':\n",
    "                # Find optimal weight and create composite ranking\n",
    "                optimal_weight = find_optimal_weight(total_df, best_high, best_low, ground_truth)\n",
    "                composite_ranking = create_composite_ranking(total_df, best_high, best_low, optimal_weight)\n",
    "                \n",
    "                # Store parameters\n",
    "                analysis[ground_truth]['composite_rankings'][f'{composite_function}_param'] = optimal_weight\n",
    "            else:\n",
    "                composite_ranking, threshold = create_treashold_composite_ranking(total_df, best_high, best_low, ground_truth)\n",
    "                analysis[ground_truth]['composite_rankings'][f'{composite_function}_param'] = threshold\n",
    "\n",
    "            # Add composite ranking to dataframe\n",
    "            ranking_col = f'composite_ranking_{ground_truth}_{composite_function}'\n",
    "            total_df[ranking_col] = composite_ranking\n",
    "            \n",
    "            # Calculate correlations and store them\n",
    "            correlations = calculate_correlations(total_df, centralities, ground_truths, ranking_col)\n",
    "            analysis[ground_truth]['correlations'][composite_function] = correlations\n",
    "            \n",
    "            # Store composite rankings\n",
    "            analysis[ground_truth]['composite_rankings'][composite_function] = total_df[ranking_col].to_dict()\n",
    "            \n",
    "            # Calculate correlation for composite ranking\n",
    "            composite_corr, p_value = stats.spearmanr(composite_ranking, total_df[ground_truth])\n",
    "            \n",
    "            # Store composite ranking correlation\n",
    "            analysis[ground_truth]['correlations']['composite'] = {\n",
    "                'correlation': composite_corr,\n",
    "                'p_value': p_value,\n",
    "                'abs_correlation': abs(composite_corr)\n",
    "            }\n",
    "            \n",
    "            # Check if composite ranking outperformed individual centralities\n",
    "            if abs(composite_corr) > max(centrality_correlations.values()):\n",
    "                analysis[ground_truth]['composite_performance']['outperformed_count'] += 1\n",
    "                analysis[ground_truth]['composite_performance']['best_combinations'].append({\n",
    "                    'combination': f\"{best_high}_{best_low}\",\n",
    "                    'correlation': abs(composite_corr),\n",
    "                    'network': network_name if network_name else 'unknown',\n",
    "                    'optimal_weight': optimal_weight if composite_function == 'weight_composite_ranking' else threshold\n",
    "                })\n",
    "            \n",
    "             # Save visualization outputs\n",
    "            plot_correlations(\n",
    "                correlations, \n",
    "                ground_truth, \n",
    "                f'{output_path}/correlations_plot_{ground_truth}_{composite_function}.png',\n",
    "                best_high, \n",
    "                best_low, \n",
    "                analysis[ground_truth]['composite_rankings'][f'{composite_function}_param']\n",
    "            )\n",
    "            \n",
    "            save_correlations_to_csv(\n",
    "                correlations,\n",
    "                ground_truth,\n",
    "                f'{output_path}/correlations_{ground_truth}_{composite_function}.csv',\n",
    "                best_high,\n",
    "                best_low\n",
    "            )\n",
    "        \n",
    "    correlations = calculate_centrality_correlations(total_df, centralities, ground_truths)\n",
    "\n",
    "    # Save complete analysis results\n",
    "    analysis_results = {\n",
    "        'network_stats': network_stats,\n",
    "        'ground_truths': ground_truths,\n",
    "        'centralities': centralities,\n",
    "        'composite_functions': composite_functions,\n",
    "        'centrality_correlations': correlations,\n",
    "        'ground_truth_analysis': {\n",
    "            gt: {\n",
    "                'correlations': analysis[gt]['correlations'],\n",
    "                'best_centralities': analysis[gt]['best_centralities'],\n",
    "                'composite_rankings': analysis[gt].get('composite_rankings', {}),\n",
    "                'composite_performance': analysis[gt]['composite_performance'],\n",
    "                'optimal_weights': analysis[gt].get('optimal_weights', {}),\n",
    "                'class_distribution': analysis[gt]['class_distribution'],\n",
    "                'centrality_counts': analysis[gt]['centrality_counts'],\n",
    "                'combination_counts': analysis[gt]['combination_counts'],\n",
    "                'network_performance': analysis[gt].get('network_performance', {})\n",
    "            } for gt in ground_truths\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return analysis_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare Function\n",
    "This section implements network comparison functionality to analyze and compare results across different networks.\n",
    "\n",
    "The main function `compare_networks()` takes results from multiple network analyses and performs the following comparisons:\n",
    "\n",
    "1. Correlation Comparisons:\n",
    "   - Compares how different centrality measures correlate with ground truth metrics across networks\n",
    "   - Creates comparison tables showing correlation values for each network\n",
    "   - Saves correlation comparisons to CSV files\n",
    "\n",
    "2. Ranking Comparisons: \n",
    "   - Analyzes how centrality measures rank relative to each other in different networks\n",
    "   - Converts absolute correlation values to rankings\n",
    "   - Shows which centrality measures perform consistently well across networks\n",
    "   - Saves ranking comparisons to CSV files\n",
    "\n",
    "The comparisons are performed for each combination of:\n",
    "- Ground truth metrics (e.g., PageRank, degree centrality)\n",
    "- Composite ranking functions (different ways of combining centrality measures)\n",
    "\n",
    "This allows us to:\n",
    "- Identify which centrality measures work best across different network types\n",
    "- Understand how network structure affects centrality measure performance\n",
    "- Compare the effectiveness of different composite ranking approaches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_networks(network_results: Dict[str, Dict], output_path: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Compare centrality measure performance across different networks.\n",
    "    \n",
    "    Args:\n",
    "        network_results: Dictionary mapping network names to their analysis results\n",
    "        output_path: Path to save comparison results\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing aggregated analysis results per ground truth\n",
    "    \"\"\"\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    \n",
    "    # Get ground truths from first network\n",
    "    first_network = list(network_results.values())[0]\n",
    "    ground_truths = list(first_network['ground_truth_analysis'].keys())\n",
    "    \n",
    "    analysis = {gt: {\n",
    "        'centrality_counts': {\n",
    "            'high': defaultdict(int),      # Times selected as best high\n",
    "            'low': defaultdict(int),       # Times selected as best low\n",
    "            'best_overall': defaultdict(int)  # Times had best correlation\n",
    "        },\n",
    "        'combination_counts': defaultdict(int),  # Times each high+low pair was selected\n",
    "        'composite_performance': {\n",
    "            'outperformed_count': 0,  # Times composite ranking beat individual centralities\n",
    "            'best_combinations': []    # Combinations that achieved best performance\n",
    "        }\n",
    "    } for gt in ground_truths}\n",
    "\n",
    "    # Add centrality correlations to analysis\n",
    "    analysis['centrality_correlations'] = {network_name: results['centrality_correlations'] for network_name, results in network_results.items()}\n",
    "    \n",
    "    # Analyze each network's results\n",
    "    for network_name, results in network_results.items():\n",
    "        for ground_truth in ground_truths:\n",
    "            gt_analysis = results['ground_truth_analysis'][ground_truth]\n",
    "            \n",
    "            # Get best centralities\n",
    "            best_high = gt_analysis['best_centralities']['high']\n",
    "            best_low = gt_analysis['best_centralities']['low']\n",
    "            best_overall = gt_analysis['best_centralities']['overall']\n",
    "            \n",
    "            # Count individual centrality selections\n",
    "            analysis[ground_truth]['centrality_counts']['high'][best_high] += 1\n",
    "            analysis[ground_truth]['centrality_counts']['low'][best_low] += 1\n",
    "            analysis[ground_truth]['centrality_counts']['best_overall'][best_overall] += 1\n",
    "            \n",
    "            # Count combination selections\n",
    "            combination = f\"{best_high}+{best_low}\"\n",
    "            analysis[ground_truth]['combination_counts'][combination] += 1\n",
    "            \n",
    "            # Track composite performance if available\n",
    "            composite_perf = gt_analysis['composite_performance']\n",
    "            if composite_perf['outperformed_count'] > 0:\n",
    "                analysis[ground_truth]['composite_performance']['outperformed_count'] += 1\n",
    "                for combo in composite_perf['best_combinations']:\n",
    "                    combo['network'] = network_name\n",
    "                    analysis[ground_truth]['composite_performance']['best_combinations'].append(combo)\n",
    "    \n",
    "    # Save results\n",
    "    for ground_truth in ground_truths:\n",
    "        # Save analysis results\n",
    "        results_file = f'{output_path}/network_analysis_{ground_truth}.json'\n",
    "        with open(results_file, 'w') as f:\n",
    "            json.dump(analysis[ground_truth], f, indent=4)\n",
    "        \n",
    "        # Create centrality summary DataFrame\n",
    "        centrality_summary = pd.DataFrame([{\n",
    "            'centrality': cent,\n",
    "            'times_best_high': analysis[ground_truth]['centrality_counts']['high'][cent],\n",
    "            'times_best_low': analysis[ground_truth]['centrality_counts']['low'][cent],\n",
    "            'times_best_overall': analysis[ground_truth]['centrality_counts']['best_overall'][cent]\n",
    "        } for cent in set(\n",
    "            list(analysis[ground_truth]['centrality_counts']['high'].keys()) +\n",
    "            list(analysis[ground_truth]['centrality_counts']['low'].keys()) +\n",
    "            list(analysis[ground_truth]['centrality_counts']['best_overall'].keys())\n",
    "        )])\n",
    "        \n",
    "        centrality_summary.to_csv(\n",
    "            f'{output_path}/centrality_summary_{ground_truth}.csv', \n",
    "            index=False\n",
    "        )\n",
    "        \n",
    "        # Create combination summary DataFrame\n",
    "        combination_summary = pd.DataFrame([{\n",
    "            'combination': comb,\n",
    "            'times_selected': count\n",
    "        } for comb, count in analysis[ground_truth]['combination_counts'].items()])\n",
    "        \n",
    "        combination_summary.to_csv(\n",
    "            f'{output_path}/combination_summary_{ground_truth}.csv',\n",
    "            index=False\n",
    "        )\n",
    "    \n",
    "    return analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_network_centrality_comparison(analysis: Dict, output_path: str):\n",
    "    # Get all unique centralities and networks\n",
    "    all_centralities = set()\n",
    "    all_networks = set()\n",
    "    \n",
    "    # Count occurrences for each centrality in each network and ground truth\n",
    "    network_counts = {}\n",
    "    \n",
    "    # Extract networks from centrality_correlations\n",
    "    if 'centrality_correlations' in analysis:\n",
    "        all_networks.update(analysis['centrality_correlations'].keys())\n",
    "        \n",
    "        # Generate colors dynamically based on number of networks\n",
    "        import colorsys\n",
    "        \n",
    "        def get_colors(n):\n",
    "            colors = {}\n",
    "            for i, network in enumerate(sorted(all_networks)):\n",
    "                # Generate evenly spaced hues\n",
    "                hue = i / n\n",
    "                # Create saturated color for importance\n",
    "                rgb = colorsys.hsv_to_rgb(hue, 0.8, 0.9)\n",
    "                main_color = f'#{int(rgb[0]*255):02x}{int(rgb[1]*255):02x}{int(rgb[2]*255):02x}'\n",
    "                # Create lighter version for doctypebranch\n",
    "                rgb_light = colorsys.hsv_to_rgb(hue, 0.5, 0.9)\n",
    "                light_color = f'#{int(rgb_light[0]*255):02x}{int(rgb_light[1]*255):02x}{int(rgb_light[2]*255):02x}'\n",
    "                colors[network] = (main_color, light_color)\n",
    "            return colors\n",
    "        \n",
    "        network_colors = get_colors(len(all_networks))\n",
    "        \n",
    "        # Initialize network_counts for each network\n",
    "        for network_name in all_networks:\n",
    "            network_counts[network_name] = {\n",
    "                'importance': defaultdict(int),\n",
    "                'doctypebranch': defaultdict(int)\n",
    "            }\n",
    "    \n",
    "    # For each ground truth, get the network performance data\n",
    "    for key in analysis:\n",
    "        if key != 'centrality_correlations':\n",
    "            network_performance = analysis[key].get('centrality_counts', {})\n",
    "            \n",
    "            # Process high performers\n",
    "            high_performers = network_performance.get('high', {})\n",
    "            for centrality, count in high_performers.items():\n",
    "                all_centralities.add(centrality)\n",
    "                for network in all_networks:\n",
    "                    network_counts[network][key][centrality] += count\n",
    "            \n",
    "            # Process low performers\n",
    "            low_performers = network_performance.get('low', {})\n",
    "            for centrality, count in low_performers.items():\n",
    "                all_centralities.add(centrality)\n",
    "                for network in all_networks:\n",
    "                    network_counts[network][key][centrality] += count\n",
    "    \n",
    "    if not all_networks or not all_centralities:\n",
    "        print(\"No data to plot: empty networks or centralities\")\n",
    "        return\n",
    "    \n",
    "    # Create the plot\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    centralities = sorted(list(all_centralities))\n",
    "    x = np.arange(len(centralities))\n",
    "    \n",
    "    # Plot bars for each network and ground truth\n",
    "    bar_width = 0.8 / (len(all_networks) * 2)  # 2 for two ground truths\n",
    "    \n",
    "    for i, network in enumerate(sorted(all_networks)):\n",
    "        for j, gt in enumerate(['importance', 'doctypebranch']):\n",
    "            counts = [network_counts[network][gt][c] for c in centralities]\n",
    "            offset = bar_width * (i * 2 + j) - (len(all_networks) * bar_width)\n",
    "            \n",
    "            # Get appropriate color based on ground truth\n",
    "            color = network_colors[network][1] if gt == 'doctypebranch' else network_colors[network][0]\n",
    "            \n",
    "            bars = ax.bar(x + offset, counts, bar_width, \n",
    "                         label=f'{network} ({gt})',\n",
    "                         color=color,\n",
    "                         alpha=0.9)\n",
    "            \n",
    "            # Add value labels on top of each bar\n",
    "            for bar in bars:\n",
    "                height = bar.get_height()\n",
    "                if height > 0:  # Only add label if bar has height\n",
    "                    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                           f'{int(height)}',\n",
    "                           ha='center', va='bottom')\n",
    "    \n",
    "    # Customize the plot\n",
    "    ax.set_ylabel('Count')\n",
    "    ax.set_title('Centrality Performance by Network and Ground Truth')\n",
    "    plt.suptitle('Total number of times each centrality has been selected as highest predictor for either low or high scores.', fontsize=10, y=0.95)\n",
    "    \n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(centralities, rotation=45, ha='right')\n",
    "    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    \n",
    "    # Adjust layout and save\n",
    "    plt.tight_layout()\n",
    "    output_file = os.path.join(output_path, 'network_centrality_comparison.png')\n",
    "    plt.savefig(output_file, bbox_inches='tight', dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def create_centrality_correlation_matrix(analysis_results: Dict, output_path: str):\n",
    "    \"\"\"\n",
    "    Create a CSV file showing correlations between centralities and ground truths across networks.\n",
    "    \n",
    "    Args:\n",
    "        analysis_results: Dictionary containing the analysis results with structure:\n",
    "            {'centrality_correlations': {\n",
    "                'network_name': {\n",
    "                    'ground_truth': {\n",
    "                        'centrality_ground_truth': correlation_value,\n",
    "                        ...\n",
    "                    },\n",
    "                    ...\n",
    "                },\n",
    "                ...\n",
    "            }}\n",
    "        output_path: Path to save the CSV file\n",
    "    \n",
    "    The CSV will have:\n",
    "    - Column 1: Centrality measures\n",
    "    - Remaining columns: One column per network\n",
    "    - Values: Ground truth correlations formatted as \"GT1: value | GT2: value\"\n",
    "    \"\"\"\n",
    "    # Get centrality correlations dictionary\n",
    "    network_correlations = analysis_results['centrality_correlations']\n",
    "    print(f\"Found networks: {list(network_correlations.keys())}\")\n",
    "    \n",
    "    # Extract all centralities from the data\n",
    "    all_centralities = set()\n",
    "    for network_data in network_correlations.values():\n",
    "        for centrality, _ in network_data.keys():\n",
    "            all_centralities.add(centrality)\n",
    "    print(f\"\\nFound centralities: {sorted(list(all_centralities))}\")\n",
    "    \n",
    "    # Extract ground truths from the first network (assuming all networks have same ground truths)\n",
    "    ground_truths = set()\n",
    "    for (_, gt) in network_correlations[list(network_correlations.keys())[0]].keys():\n",
    "        ground_truths.add(gt)\n",
    "    print(f\"\\nFound ground truths: {sorted(list(ground_truths))}\")\n",
    "    \n",
    "    # Create DataFrame with just centralities and networks\n",
    "    networks = sorted(network_correlations.keys())\n",
    "    df = pd.DataFrame(index=sorted(list(all_centralities)), \n",
    "                     columns=['Centrality'] + networks)\n",
    "    \n",
    "    # Fill in centrality names\n",
    "    df['Centrality'] = df.index\n",
    "    print(\"\\nInitial DataFrame:\")\n",
    "    print(df)\n",
    "    \n",
    "    # Fill in the correlation values for each network\n",
    "    for network in networks:\n",
    "        print(f\"\\nFilling data for network: {network}\")\n",
    "        network_data = network_correlations[network]\n",
    "        \n",
    "        for centrality in all_centralities:\n",
    "            # Collect all ground truth correlations for this centrality\n",
    "            correlations = []\n",
    "            for gt in sorted(ground_truths):\n",
    "                key = (centrality, gt)\n",
    "                if key in network_data:\n",
    "                    value = network_data[key]\n",
    "                    # Capitalize first letter of ground truth and format value\n",
    "                    gt_display = gt.capitalize()\n",
    "                    correlations.append(f\"{gt_display}: {value:.4f}\")\n",
    "            \n",
    "            # Join all ground truth correlations with \" | \"\n",
    "            if correlations:\n",
    "                df.loc[centrality, network] = \" | \".join(correlations)\n",
    "            else:\n",
    "                df.loc[centrality, network] = \"N/A\"\n",
    "    \n",
    "    print(\"\\nFinal DataFrame:\")\n",
    "    print(df)\n",
    "    \n",
    "    # Save to CSV\n",
    "    output_file = os.path.join(output_path, 'centrality_correlations_matrix.csv')\n",
    "    df.to_csv(output_file, index=False)\n",
    "    \n",
    "    print(f\"\\nCorrelation matrix saved to {output_file}\")\n",
    "\n",
    "def visualize_network_commonalities(analysis_results: Dict, output_path: str, compare_all: bool = False, print_correlations: bool = False):\n",
    "    \"\"\"\n",
    "    Visualize commonalities between networks focusing on centrality performance.\n",
    "    Each visualization is saved separately.\n",
    "    \n",
    "    Args:\n",
    "        analysis_results: Dictionary containing the analysis results from compare_networks()\n",
    "        output_path: Path to save visualization outputs\n",
    "    \"\"\"\n",
    "    # Set the aesthetic style\n",
    "    plt.style.use('default')\n",
    "    colors = {'importance': 'skyblue', 'doctypebranch': 'lightgreen'}\n",
    "    \n",
    "    # Helper function to add value labels\n",
    "    def add_value_labels(ax, rects):\n",
    "        for rect in rects:\n",
    "            height = rect.get_height()\n",
    "            ax.text(rect.get_x() + rect.get_width()/2., height,\n",
    "                    f'{int(height)}',\n",
    "                    ha='center', va='bottom')\n",
    "    \n",
    "    # 1. High Importance Performers Graph\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    for key in analysis_results:\n",
    "        if key != 'centrality_correlations':  # This is a ground truth\n",
    "            high_counts = analysis_results[key]['centrality_counts']['high']\n",
    "            if high_counts:\n",
    "                centralities = list(high_counts.keys())\n",
    "                counts = list(high_counts.values())\n",
    "                bars = ax.bar(centralities, counts, label=key, alpha=0.7, color=colors[key])\n",
    "                add_value_labels(ax, bars)\n",
    "    \n",
    "    plt.ylabel('Times Selected as Best High Predictor')\n",
    "    plt.title('Best Performing Centralities for High Scores')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{output_path}/high_performers.png', bbox_inches='tight', dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    # 2. Low Importance Performers Graph\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    for key in analysis_results:\n",
    "        if key != 'centrality_correlations':  # This is a ground truth\n",
    "            low_counts = analysis_results[key]['centrality_counts']['low']\n",
    "            if low_counts:\n",
    "                centralities = list(low_counts.keys())\n",
    "                counts = list(low_counts.values())\n",
    "                bars = ax.bar(centralities, counts, alpha=0.5, label=key, color=colors[key])\n",
    "                add_value_labels(ax, bars)\n",
    "    \n",
    "    plt.ylabel('Times Selected as Best Low Predictor')\n",
    "    plt.title('Best Performing Centralities for Low Scores')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{output_path}/low_performers.png', bbox_inches='tight', dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    # 3. Best Overall Performers Graph\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    for ground_truth in analysis_results:\n",
    "        if ground_truth != 'centrality_correlations':\n",
    "            best_overall = analysis_results[ground_truth]['centrality_counts']['best_overall']\n",
    "            if best_overall:\n",
    "                centralities = list(best_overall.keys())\n",
    "                counts = list(best_overall.values())\n",
    "                bars = ax.bar(centralities, counts, label=ground_truth, alpha=0.7, color=colors[ground_truth])\n",
    "                add_value_labels(ax, bars)\n",
    "    \n",
    "    plt.ylabel('Times Selected as Highest Predictor')\n",
    "    plt.title('Higest Overall Predicting Centralities')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{output_path}/best_overall_performers.png', bbox_inches='tight', dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    # 4. Combination Performance Graph\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    for key in analysis_results:\n",
    "        if key != 'centrality_correlations':\n",
    "            combinations = analysis_results[key]['combination_counts']\n",
    "            if combinations:\n",
    "                combo_names = list(combinations.keys())\n",
    "                combo_counts = list(combinations.values())\n",
    "                bars = ax.bar(combo_names, combo_counts, label=key, alpha=0.7, color=colors[key])\n",
    "                add_value_labels(ax, bars)\n",
    "\n",
    "    plt.ylabel('Times Selected as Highest Predictor')\n",
    "    plt.title('Best Performing Centrality Combinations')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{output_path}/combination_performance.png', bbox_inches='tight', dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    # 5. Total Best Occurrences Graph\n",
    "    total_counts = {}\n",
    "    for key in analysis_results:\n",
    "        if key != 'centrality_correlations':\n",
    "            for category in ['high', 'low', 'best_overall']:\n",
    "                counts = analysis_results[key]['centrality_counts'][category]\n",
    "                for centrality, count in counts.items():\n",
    "                    if centrality not in total_counts:\n",
    "                        total_counts[centrality] = 0\n",
    "                    total_counts[centrality] += count\n",
    "    \n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    if total_counts:\n",
    "        centralities = list(total_counts.keys())\n",
    "        counts = list(total_counts.values())\n",
    "        bars = ax.bar(centralities, counts, color='lightcoral', alpha=0.7)\n",
    "        add_value_labels(ax, bars)\n",
    "    \n",
    "    plt.ylabel('Total Times Selected as Highest Predictor')\n",
    "    plt.title('Overall Best Performing Centralities (All Categories)')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{output_path}/total_best_occurrences.png', bbox_inches='tight', dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    # 6. Compare across networks\n",
    "    if compare_all:\n",
    "        plot_network_centrality_comparison(analysis_results, output_path) # TODO\n",
    "        \n",
    "    if print_correlations:\n",
    "        create_centrality_correlation_matrix(analysis_results, output_path)\n",
    "        \n",
    "        # Create performance summary\n",
    "    performance_summary = {}\n",
    "    for key in analysis_results:\n",
    "        if key != 'centrality_correlations':\n",
    "            performance_summary[key] = {\n",
    "                'best_performers': {\n",
    "                    'high': None,\n",
    "                    'low': None,\n",
    "                    'combination': None\n",
    "                },\n",
    "                'composite_performance': {}\n",
    "            }\n",
    "            \n",
    "            # Best high performer\n",
    "            high_counts = analysis_results[key]['centrality_counts']['high']\n",
    "            if high_counts:\n",
    "                best_high = max(high_counts.items(), key=lambda x: x[1])\n",
    "                performance_summary[key]['best_performers']['high'] = {\n",
    "                    'centrality': best_high[0],\n",
    "                    'times_selected': best_high[1]\n",
    "                }\n",
    "            \n",
    "            # Best low performer\n",
    "            low_counts = analysis_results[key]['centrality_counts']['low']\n",
    "            if low_counts:\n",
    "                best_low = max(low_counts.items(), key=lambda x: x[1])\n",
    "                performance_summary[key]['best_performers']['low'] = {\n",
    "                    'centrality': best_low[0],\n",
    "                    'times_selected': best_low[1]\n",
    "                }\n",
    "            \n",
    "            # Best combination\n",
    "            combinations = analysis_results[key]['combination_counts']\n",
    "            if combinations:\n",
    "                best_combo = max(combinations.items(), key=lambda x: x[1])\n",
    "                performance_summary[key]['best_performers']['combination'] = {\n",
    "                    'combination': best_combo[0],\n",
    "                    'times_selected': best_combo[1]\n",
    "                }\n",
    "            \n",
    "            # Composite performance\n",
    "            outperformed_count = analysis_results[key]['composite_performance']['outperformed_count']\n",
    "            best_combinations = analysis_results[key]['composite_performance']['best_combinations']\n",
    "            performance_summary[key]['composite_performance'] = {\n",
    "                'outperformed_count': outperformed_count,\n",
    "                'best_combinations': []\n",
    "            }\n",
    "            \n",
    "            if best_combinations:\n",
    "                combo_counts = {}\n",
    "                for entry in best_combinations:\n",
    "                    combo = entry['combination']\n",
    "                    combo_counts[combo] = combo_counts.get(combo, 0) + 1\n",
    "                \n",
    "                best_outperforming = max(combo_counts.items(), key=lambda x: x[1])\n",
    "                best_correlation = max(entry['correlation'] for entry in best_combinations)\n",
    "                \n",
    "                performance_summary[key]['composite_performance'].update({\n",
    "                    'best_outperforming_combo': {\n",
    "                        'combination': best_outperforming[0],\n",
    "                        'times_outperformed': best_outperforming[1]\n",
    "                    },\n",
    "                    'highest_correlation': best_correlation,\n",
    "                    'detailed_combinations': best_combinations\n",
    "                })\n",
    "    \n",
    "        # Add overall best performer\n",
    "    if total_counts:\n",
    "        best_overall = max(total_counts.items(), key=lambda x: x[1])\n",
    "        performance_summary['overall_best_performer'] = {\n",
    "            'centrality': best_overall[0],\n",
    "            'total_times_selected': best_overall[1]\n",
    "        }\n",
    "    \n",
    "    # Save performance summary as JSON\n",
    "    with open(f'{output_path}/performance_summary.json', 'w') as f:\n",
    "        json.dump(performance_summary, f, indent=4)\n",
    "    \n",
    "    # Save detailed numerical results\n",
    "    summary = {\n",
    "        key: {\n",
    "            'high_performers': dict(analysis_results[key]['centrality_counts']['high']),\n",
    "            'low_performers': dict(analysis_results[key]['centrality_counts']['low']),\n",
    "            'best_overall': dict(analysis_results[key]['centrality_counts']['best_overall']),\n",
    "            'combinations': dict(analysis_results[key]['combination_counts']),\n",
    "            'composite_performance': {\n",
    "                'outperformed_count': analysis_results[key]['composite_performance']['outperformed_count'],\n",
    "                'best_combinations': analysis_results[key]['composite_performance']['best_combinations']\n",
    "            },\n",
    "            'best_performers': {\n",
    "                'high': max(analysis_results[key]['centrality_counts']['high'].items(), \n",
    "                           key=lambda x: x[1])[0] if analysis_results[key]['centrality_counts']['high'] else None,\n",
    "                'low': max(analysis_results[key]['centrality_counts']['low'].items(), \n",
    "                          key=lambda x: x[1])[0] if analysis_results[key]['centrality_counts']['low'] else None,\n",
    "                'overall': max(analysis_results[key]['centrality_counts']['best_overall'].items(), \n",
    "                             key=lambda x: x[1])[0] if analysis_results[key]['centrality_counts']['best_overall'] else None,\n",
    "                'combination': max(analysis_results[key]['combination_counts'].items(), \n",
    "                                 key=lambda x: x[1])[0] if analysis_results[key]['combination_counts'] else None\n",
    "            }\n",
    "        }\n",
    "        for key in analysis_results if key != 'centrality_correlations'\n",
    "    }\n",
    "    \n",
    "    # Add total counts to summary\n",
    "    summary['total_counts'] = total_counts\n",
    "    \n",
    "    # Save visualization summary\n",
    "    with open(f'{output_path}/visualization_summary.json', 'w') as f:\n",
    "        json.dump(summary, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_networks(path, max=100, min_nodes=50):\n",
    "    \"\"\"\n",
    "    Load networks from a directory structure. Can handle both:\n",
    "    1. Direct network files (nodes.json and edges.json in the input path)\n",
    "    2. Networks in subdirectories\n",
    "    \n",
    "    Args:\n",
    "        path: Root directory to search for networks or direct path to a network\n",
    "        max: Maximum number of networks to load\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary of networks with structure {network_name: {'nodes': df, 'edges': df}}\n",
    "    \"\"\"\n",
    "    networks = {}\n",
    "    loaded_count = 0\n",
    "    \n",
    "    # Verify root directory exists and print absolute path\n",
    "    abs_root = os.path.abspath(path)\n",
    "    print(f\"Looking for networks in: {abs_root}\")\n",
    "    \n",
    "    if not os.path.exists(abs_root):\n",
    "        raise ValueError(f\"Directory {abs_root} does not exist\")\n",
    "        \n",
    "    def is_network_dir(dir_path):\n",
    "        \"\"\"Check if directory contains exactly nodes.json and edges.json\"\"\"\n",
    "        try:\n",
    "            contents = os.listdir(dir_path)\n",
    "            return ('nodes.json' in contents and 'edges.json' in contents)\n",
    "        except:\n",
    "            return False\n",
    "    \n",
    "    def load_network(dir_path, network_name, min_nodes=50):\n",
    "        \"\"\"Load a single network from a directory\"\"\"\n",
    "        print(f\"\\nChecking network: {network_name}\")\n",
    "        print(f\"  Path: {dir_path}\")\n",
    "        \n",
    "        nodes_file = os.path.join(dir_path, 'nodes.json')\n",
    "        edges_file = os.path.join(dir_path, 'edges.json')\n",
    "        \n",
    "        try:\n",
    "            # Load and validate data\n",
    "            print(\"  Loading data files...\")\n",
    "            nodes_df = pd.read_json(nodes_file)\n",
    "            edges_df = pd.read_json(edges_file)\n",
    "            \n",
    "            # Validate required columns\n",
    "            required_node_cols = ['ecli', 'importance', 'doctypebranch']\n",
    "            missing_cols = [col for col in required_node_cols if col not in nodes_df.columns]\n",
    "            if missing_cols:\n",
    "                print(f\"  Warning: Missing columns in nodes.json: {missing_cols}\")\n",
    "                print(f\"  Available columns: {nodes_df.columns.tolist()}\")\n",
    "                return False\n",
    "            \n",
    "            required_edge_cols = ['ecli', 'references']\n",
    "            missing_edge_cols = [col for col in required_edge_cols if col not in edges_df.columns]\n",
    "            if missing_edge_cols:\n",
    "                print(f\"  Warning: Missing columns in edges.json: {missing_edge_cols}\")\n",
    "                print(f\"  Available columns: {edges_df.columns.tolist()}\")\n",
    "                return False\n",
    "            \n",
    "            if len(nodes_df) < min_nodes:\n",
    "                print(f\"  Warning: Network {network_name} has fewer than {min_nodes} nodes\")\n",
    "                return False\n",
    "            \n",
    "            networks[network_name] = {\n",
    "                'nodes': nodes_df,\n",
    "                'edges': edges_df\n",
    "            }\n",
    "            print(f\"  Successfully loaded network with {len(nodes_df)} nodes and {len(edges_df)} edges\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  Error loading network: {str(e)}\")\n",
    "            return False\n",
    "    \n",
    "    def process_directory(current_path, parent_prefix=\"\"):\n",
    "        \"\"\"Recursively process directories looking for networks\"\"\"\n",
    "        nonlocal loaded_count\n",
    "        if loaded_count >= max:\n",
    "            return\n",
    "            \n",
    "        # First check if current directory is a network directory\n",
    "        if is_network_dir(current_path):\n",
    "            # Generate network name based on path\n",
    "            rel_path = os.path.relpath(current_path, abs_root)\n",
    "            network_name = rel_path.replace(os.sep, '-')\n",
    "            if network_name == '.':  # Handle case where path is direct to network\n",
    "                network_name = os.path.basename(current_path)\n",
    "            \n",
    "            if load_network(current_path, network_name, min_nodes):\n",
    "                loaded_count += 1\n",
    "            return\n",
    "        \n",
    "        # If not a network directory, search subdirectories\n",
    "        try:\n",
    "            for item in os.listdir(current_path):\n",
    "                if loaded_count >= max:\n",
    "                    break\n",
    "                    \n",
    "                item_path = os.path.join(current_path, item)\n",
    "                if not os.path.isdir(item_path):\n",
    "                    continue\n",
    "                    \n",
    "                process_directory(item_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error accessing directory {current_path}: {str(e)}\")\n",
    "    \n",
    "    # Start processing from root directory\n",
    "    process_directory(abs_root)\n",
    "    print(f\"\\nSuccessfully loaded {len(networks)} networks\")\n",
    "    return networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Centralities of interest\n",
    "CENTRALITIES = ['degree_centrality', 'in_degree_centrality', 'out_degree_centrality', \n",
    "                'betweenness_centrality', 'closeness_centrality', 'core_number', \n",
    "                'relative_in_degree_centrality', 'eigenvector_centrality', \n",
    "                'pagerank', 'hits_hub', 'hits_authority', 'harmonic_centrality', 'disruption']\n",
    "\n",
    "# Ground truths of interest\n",
    "GROUND_TRUTHS = ['importance', 'doctypebranch']\n",
    "GROUND_TRUTHS_INVERTED = ['importance_inverted',  'doctypebranch_inverted']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load all networks from data/split directory\n",
    "# network_results = {}\n",
    "\n",
    "# networks = load_networks('networks')\n",
    "\n",
    "# # Analyze Networks\n",
    "# for network_name, data in networks.items():\n",
    "#     print(f\"\\nAnalyzing network: {network_name}\")\n",
    "#     results = analyze_network(\n",
    "#         nodes_df=data['nodes'],\n",
    "#         edges_df=data['edges'],\n",
    "#         ground_truths=GROUND_TRUTHS,\n",
    "#         centralities=CENTRALITIES,\n",
    "#         composite_functions=['weight_composite_ranking',],\n",
    "#         output_path=f'results/{network_name}'\n",
    "#     )\n",
    "#     network_results[network_name] = results\n",
    "\n",
    "\n",
    "# # Before comparison\n",
    "# if not network_results:\n",
    "#     raise ValueError(\"No networks were successfully analyzed. Check if data/split directory contains valid network data.\")\n",
    "\n",
    "\n",
    "# # Compare results across networks\n",
    "# analysis = compare_networks(network_results, 'results/comparisons')\n",
    "\n",
    "# # Visualize results\n",
    "# visualize_network_commonalities(analysis, 'results/comparisons')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "# UNBALANCED\n",
    "# network_results = {}\n",
    "# networks = load_networks('networks/balanced-importance')\n",
    "\n",
    "# # Analyze Networks\n",
    "# for network_name, data in networks.items():\n",
    "#     print(f\"\\nAnalyzing network: {network_name}\")\n",
    "#     results = analyze_network(\n",
    "#         nodes_df=data['nodes'],\n",
    "#         edges_df=data['edges'],\n",
    "#         ground_truths=GROUND_TRUTHS,\n",
    "#         centralities=CENTRALITIES,\n",
    "#         composite_functions=['weight_composite_ranking'],\n",
    "#         output_path=f'results/test/{network_name}'\n",
    "#     )\n",
    "#     network_results[network_name] = results\n",
    "\n",
    "# # Before comparison\n",
    "# if not network_results:\n",
    "#     raise ValueError(\"No networks were successfully analyzed. Check if data/split directory contains valid network data.\")\n",
    "\n",
    "\n",
    "# # Compare results across networks\n",
    "# analysis = compare_networks(network_results, 'results/test/comparisons')\n",
    "\n",
    "# # Visualize results\n",
    "# visualize_network_commonalities(analysis, 'results/test/comparisons')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Analyse whole networks (balanced and unbalanced)\n",
    "# network_results = {}\n",
    "\n",
    "# networks = load_networks('networks/full-balanced-importance')\n",
    "# networks.update(load_networks('networks/full-unbalanced'))\n",
    "# networks.update(load_networks('networks/full-balanced-doctypebranch'))\n",
    "\n",
    "# # Analyze Networks\n",
    "# for network_name, data in networks.items():\n",
    "#     print(f\"\\nAnalyzing network: {network_name}\")\n",
    "#     results = analyze_network(\n",
    "#         nodes_df=data['nodes'],\n",
    "#         edges_df=data['edges'],\n",
    "#         ground_truths=GROUND_TRUTHS,\n",
    "#         centralities=CENTRALITIES,\n",
    "#         composite_functions=['weight_composite_ranking',],\n",
    "#         output_path=f'results/full/{network_name}'\n",
    "#     )\n",
    "#     network_results[network_name] = results\n",
    "\n",
    "\n",
    "\n",
    "# # Before comparison\n",
    "# if not network_results:\n",
    "#     raise ValueError(\"No networks were successfully analyzed. Check if data/split directory contains valid network data.\")\n",
    "\n",
    "\n",
    "# # Compare results across networks\n",
    "# analysis = compare_networks(network_results, 'results/full/comparisons')\n",
    "\n",
    "# # Visualize results\n",
    "# visualize_network_commonalities(analysis, 'results/full/comparisons')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # ANALYSE SPLIT NETWORKS\n",
    "# # UNBALANCED\n",
    "# network_results = {}\n",
    "# networks = load_networks('networks/unbalanced')\n",
    "\n",
    "# # Analyze Networks\n",
    "# for network_name, data in networks.items():\n",
    "#     print(f\"\\nAnalyzing network: {network_name}\")\n",
    "#     results = analyze_network(\n",
    "#         nodes_df=data['nodes'],\n",
    "#         edges_df=data['edges'],\n",
    "#         ground_truths=GROUND_TRUTHS,\n",
    "#         centralities=CENTRALITIES,\n",
    "#         composite_functions=['weight_composite_ranking',],\n",
    "#         output_path=f'results/unbalanced/{network_name}'\n",
    "#     )\n",
    "#     network_results[network_name] = results\n",
    "\n",
    "# # Before comparison\n",
    "# if not network_results:\n",
    "#     raise ValueError(\"No networks were successfully analyzed. Check if data/split directory contains valid network data.\")\n",
    "\n",
    "\n",
    "# # Compare results across networks\n",
    "# analysis = compare_networks(network_results, 'results/unbalanced/comparisons')\n",
    "\n",
    "# # Visualize results\n",
    "# visualize_network_commonalities(analysis, 'results/unbalanced/comparisons')\n",
    "\n",
    "\n",
    "\n",
    "# # BALANCED BY IMPORRTANCE \n",
    "# network_results = {}\n",
    "# networks = load_networks('networks/balanced-importance')\n",
    "\n",
    "# # Analyze Networks\n",
    "# for network_name, data in networks.items():\n",
    "#     print(f\"\\nAnalyzing network: {network_name}\")\n",
    "#     results = analyze_network(\n",
    "#         nodes_df=data['nodes'],\n",
    "#         edges_df=data['edges'],\n",
    "#         ground_truths=GROUND_TRUTHS,\n",
    "#         centralities=CENTRALITIES,\n",
    "#         composite_functions=['weight_composite_ranking',],\n",
    "#         output_path=f'results/balanced-importance/{network_name}'\n",
    "#     )\n",
    "#     network_results[network_name] = results\n",
    "\n",
    "# # Before comparison\n",
    "# if not network_results:\n",
    "#     raise ValueError(\"No networks were successfully analyzed. Check if data/split directory contains valid network data.\")\n",
    "\n",
    "\n",
    "# # Compare results across networks\n",
    "# analysis = compare_networks(network_results, 'results/balanced-importance/comparisons')\n",
    "\n",
    "# # Visualize results\n",
    "# visualize_network_commonalities(analysis, 'results/balanced-importance/comparisons')\n",
    "\n",
    "\n",
    "\n",
    "# # BALANCED BY DOCTYPEBRANCH\n",
    "# network_results = {}\n",
    "# networks = load_networks('networks/balanced-doctypebranch')\n",
    "\n",
    "# # Analyze Networks\n",
    "# for network_name, data in networks.items():\n",
    "#     print(f\"\\nAnalyzing network: {network_name}\")\n",
    "#     results = analyze_network(\n",
    "#         nodes_df=data['nodes'],\n",
    "#         edges_df=data['edges'],\n",
    "#         ground_truths=GROUND_TRUTHS,\n",
    "#         centralities=CENTRALITIES,\n",
    "#         composite_functions=['weight_composite_ranking',],\n",
    "#         output_path=f'results/balanced-doctypebranch/{network_name}'\n",
    "#     )\n",
    "#     network_results[network_name] = results\n",
    "\n",
    "# # Before comparison\n",
    "# if not network_results:\n",
    "#     raise ValueError(\"No networks were successfully analyzed. Check if data/split directory contains valid network data.\")\n",
    "\n",
    "\n",
    "# # Compare results across networks\n",
    "# analysis = compare_networks(network_results, 'results/balanced-doctypebranch/comparisons')\n",
    "\n",
    "# # Visualize results\n",
    "# visualize_network_commonalities(analysis, 'results/balanced-doctypebranch/comparisons')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Analyse whole networks (balanced and unbalanced)\n",
    "# network_results = {}\n",
    "\n",
    "# networks = load_networks('networks/full-balanced-importance')\n",
    "# networks.update(load_networks('networks/full-unbalanced'))\n",
    "# networks.update(load_networks('networks/full-balanced-doctypebranch'))\n",
    "\n",
    "# # Analyze Networks\n",
    "# for network_name, data in networks.items():\n",
    "#     print(f\"\\nAnalyzing network: {network_name}\")\n",
    "#     results = analyze_network(\n",
    "#         nodes_df=data['nodes'],\n",
    "#         edges_df=data['edges'],\n",
    "#         ground_truths=GROUND_TRUTHS,\n",
    "#         centralities=CENTRALITIES,\n",
    "#         composite_functions=['weight_composite_ranking',],\n",
    "#         output_path=f'results/branch-update/full/{network_name}'\n",
    "#     )\n",
    "#     network_results[network_name] = results\n",
    "\n",
    "\n",
    "\n",
    "# # Before comparison\n",
    "# if not network_results:\n",
    "#     raise ValueError(\"No networks were successfully analyzed. Check if data/split directory contains valid network data.\")\n",
    "\n",
    "\n",
    "# # Compare results across networks\n",
    "# analysis = compare_networks(network_results, 'results/branch-update/full/comparisons')\n",
    "\n",
    "# # Visualize results\n",
    "# visualize_network_commonalities(analysis, 'results/branch-update/full/comparisons')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # ANALYSE SPLIT NETWORKS\n",
    "# # UNBALANCED\n",
    "# network_results = {}\n",
    "# networks = load_networks('networks/unbalanced')\n",
    "\n",
    "# # Analyze Networks\n",
    "# for network_name, data in networks.items():\n",
    "#     print(f\"\\nAnalyzing network: {network_name}\")\n",
    "#     results = analyze_network(\n",
    "#         nodes_df=data['nodes'],\n",
    "#         edges_df=data['edges'],\n",
    "#         ground_truths=GROUND_TRUTHS,\n",
    "#         centralities=CENTRALITIES,\n",
    "#         composite_functions=['weight_composite_ranking',],\n",
    "#         output_path=f'results/branch-update/unbalanced/{network_name}'\n",
    "#     )\n",
    "#     network_results[network_name] = results\n",
    "\n",
    "# # Before comparison\n",
    "# if not network_results:\n",
    "#     raise ValueError(\"No networks were successfully analyzed. Check if data/split directory contains valid network data.\")\n",
    "\n",
    "\n",
    "# # Compare results across networks\n",
    "# analysis = compare_networks(network_results, 'results/branch-update/unbalanced/comparisons')\n",
    "\n",
    "# # Visualize results\n",
    "# visualize_network_commonalities(analysis, 'results/branch-update/unbalanced/comparisons')\n",
    "\n",
    "\n",
    "\n",
    "# # BALANCED BY IMPORRTANCE \n",
    "# network_results = {}\n",
    "# networks = load_networks('networks/balanced-importance')\n",
    "\n",
    "# # Analyze Networks\n",
    "# for network_name, data in networks.items():\n",
    "#     print(f\"\\nAnalyzing network: {network_name}\")\n",
    "#     results = analyze_network(\n",
    "#         nodes_df=data['nodes'],\n",
    "#         edges_df=data['edges'],\n",
    "#         ground_truths=GROUND_TRUTHS,\n",
    "#         centralities=CENTRALITIES,\n",
    "#         composite_functions=['weight_composite_ranking',],\n",
    "#         output_path=f'results/branch-update/balanced-importance/{network_name}'\n",
    "#     )\n",
    "#     network_results[network_name] = results\n",
    "\n",
    "# # Before comparison\n",
    "# if not network_results:\n",
    "#     raise ValueError(\"No networks were successfully analyzed. Check if data/split directory contains valid network data.\")\n",
    "\n",
    "\n",
    "# # Compare results across networks\n",
    "# analysis = compare_networks(network_results, 'results/branch-update/balanced-importance/comparisons')\n",
    "\n",
    "# # Visualize results\n",
    "# visualize_network_commonalities(analysis, 'results/branch-update/balanced-importance/comparisons')\n",
    "\n",
    "\n",
    "\n",
    "# # BALANCED BY DOCTYPEBRANCH\n",
    "# network_results = {}\n",
    "# networks = load_networks('networks/balanced-doctypebranch')\n",
    "\n",
    "# # Analyze Networks\n",
    "# for network_name, data in networks.items():\n",
    "#     print(f\"\\nAnalyzing network: {network_name}\")\n",
    "#     results = analyze_network(\n",
    "#         nodes_df=data['nodes'],\n",
    "#         edges_df=data['edges'],\n",
    "#         ground_truths=GROUND_TRUTHS,\n",
    "#         centralities=CENTRALITIES,\n",
    "#         composite_functions=['weight_composite_ranking',],\n",
    "#         output_path=f'results/balanced-doctypebranch/{network_name}'\n",
    "#     )\n",
    "#     network_results[network_name] = results\n",
    "\n",
    "# # Before comparison\n",
    "# if not network_results:\n",
    "#     raise ValueError(\"No networks were successfully analyzed. Check if data/split directory contains valid network data.\")\n",
    "\n",
    "\n",
    "# # Compare results across networks\n",
    "# analysis = compare_networks(network_results, 'results/balanced-doctypebranch/comparisons')\n",
    "\n",
    "# # Visualize results\n",
    "# visualize_network_commonalities(analysis, 'results/balanced-doctypebranch/comparisons')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for networks in: /Users/davidwickerhf/Projects/work/maastrichtuniversity/rankings/networks/merged/split-balanced-importance\n",
      "\n",
      "Checking network: article_2-1\n",
      "  Path: /Users/davidwickerhf/Projects/work/maastrichtuniversity/rankings/networks/merged/split-balanced-importance/article_2-1\n",
      "  Loading data files...\n",
      "  Successfully loaded network with 425 nodes and 75 edges\n",
      "\n",
      "Checking network: article_8-1\n",
      "  Path: /Users/davidwickerhf/Projects/work/maastrichtuniversity/rankings/networks/merged/split-balanced-importance/article_8-1\n",
      "  Loading data files...\n",
      "  Successfully loaded network with 1106 nodes and 180 edges\n",
      "\n",
      "Checking network: article_38-1-b\n",
      "  Path: /Users/davidwickerhf/Projects/work/maastrichtuniversity/rankings/networks/merged/split-balanced-importance/article_38-1-b\n",
      "  Loading data files...\n",
      "  Successfully loaded network with 53 nodes and 1 edges\n",
      "\n",
      "Checking network: article_4-2\n",
      "  Path: /Users/davidwickerhf/Projects/work/maastrichtuniversity/rankings/networks/merged/split-balanced-importance/article_4-2\n",
      "  Loading data files...\n",
      "  Successfully loaded network with 79 nodes and 14 edges\n",
      "\n",
      "Checking network: article_6-1\n",
      "  Path: /Users/davidwickerhf/Projects/work/maastrichtuniversity/rankings/networks/merged/split-balanced-importance/article_6-1\n",
      "  Loading data files...\n",
      "  Successfully loaded network with 2736 nodes and 525 edges\n",
      "\n",
      "Checking network: article_10\n",
      "  Path: /Users/davidwickerhf/Projects/work/maastrichtuniversity/rankings/networks/merged/split-balanced-importance/article_10\n",
      "  Loading data files...\n",
      "  Successfully loaded network with 603 nodes and 104 edges\n",
      "\n",
      "Checking network: article_5-1-e\n",
      "  Path: /Users/davidwickerhf/Projects/work/maastrichtuniversity/rankings/networks/merged/split-balanced-importance/article_5-1-e\n",
      "  Loading data files...\n",
      "  Successfully loaded network with 67 nodes and 11 edges\n",
      "\n",
      "Checking network: article_11-2\n",
      "  Path: /Users/davidwickerhf/Projects/work/maastrichtuniversity/rankings/networks/merged/split-balanced-importance/article_11-2\n",
      "  Loading data files...\n",
      "  Successfully loaded network with 117 nodes and 19 edges\n",
      "\n",
      "Checking network: article_18\n",
      "  Path: /Users/davidwickerhf/Projects/work/maastrichtuniversity/rankings/networks/merged/split-balanced-importance/article_18\n",
      "  Loading data files...\n",
      "  Successfully loaded network with 62 nodes and 4 edges\n",
      "\n",
      "Checking network: article_11\n",
      "  Path: /Users/davidwickerhf/Projects/work/maastrichtuniversity/rankings/networks/merged/split-balanced-importance/article_11\n",
      "  Loading data files...\n",
      "  Successfully loaded network with 213 nodes and 23 edges\n",
      "\n",
      "Checking network: article_29\n",
      "  Path: /Users/davidwickerhf/Projects/work/maastrichtuniversity/rankings/networks/merged/split-balanced-importance/article_29\n",
      "  Loading data files...\n",
      "  Successfully loaded network with 557 nodes and 211 edges\n",
      "\n",
      "Checking network: article_5-1-c\n",
      "  Path: /Users/davidwickerhf/Projects/work/maastrichtuniversity/rankings/networks/merged/split-balanced-importance/article_5-1-c\n",
      "  Loading data files...\n",
      "  Successfully loaded network with 118 nodes and 17 edges\n",
      "\n",
      "Checking network: article_46-2\n",
      "  Path: /Users/davidwickerhf/Projects/work/maastrichtuniversity/rankings/networks/merged/split-balanced-importance/article_46-2\n",
      "  Loading data files...\n",
      "  Successfully loaded network with 172 nodes and 43 edges\n",
      "\n",
      "Checking network: article_34\n",
      "  Path: /Users/davidwickerhf/Projects/work/maastrichtuniversity/rankings/networks/merged/split-balanced-importance/article_34\n",
      "  Loading data files...\n",
      "  Successfully loaded network with 531 nodes and 111 edges\n",
      "\n",
      "Checking network: article_9-2\n",
      "  Path: /Users/davidwickerhf/Projects/work/maastrichtuniversity/rankings/networks/merged/split-balanced-importance/article_9-2\n",
      "  Loading data files...\n",
      "  Successfully loaded network with 57 nodes and 10 edges\n",
      "\n",
      "Checking network: article_29-3\n",
      "  Path: /Users/davidwickerhf/Projects/work/maastrichtuniversity/rankings/networks/merged/split-balanced-importance/article_29-3\n",
      "  Loading data files...\n",
      "  Successfully loaded network with 557 nodes and 211 edges\n",
      "\n",
      "Checking network: article_3+8\n",
      "  Path: /Users/davidwickerhf/Projects/work/maastrichtuniversity/rankings/networks/merged/split-balanced-importance/article_3+8\n",
      "  Loading data files...\n",
      "  Successfully loaded network with 114 nodes and 3 edges\n",
      "\n",
      "Checking network: article_5-1\n",
      "  Path: /Users/davidwickerhf/Projects/work/maastrichtuniversity/rankings/networks/merged/split-balanced-importance/article_5-1\n",
      "  Loading data files...\n",
      "  Successfully loaded network with 1444 nodes and 302 edges\n",
      "\n",
      "Checking network: article_35\n",
      "  Path: /Users/davidwickerhf/Projects/work/maastrichtuniversity/rankings/networks/merged/split-balanced-importance/article_35\n",
      "  Loading data files...\n",
      "  Successfully loaded network with 1585 nodes and 362 edges\n",
      "\n",
      "Checking network: article_3\n",
      "  Path: /Users/davidwickerhf/Projects/work/maastrichtuniversity/rankings/networks/merged/split-balanced-importance/article_3\n",
      "  Loading data files...\n",
      "  Successfully loaded network with 3691 nodes and 696 edges\n",
      "\n",
      "Checking network: article_4\n",
      "  Path: /Users/davidwickerhf/Projects/work/maastrichtuniversity/rankings/networks/merged/split-balanced-importance/article_4\n",
      "  Loading data files...\n",
      "  Successfully loaded network with 4058 nodes and 940 edges\n",
      "\n",
      "Checking network: article_5\n",
      "  Path: /Users/davidwickerhf/Projects/work/maastrichtuniversity/rankings/networks/merged/split-balanced-importance/article_5\n",
      "  Loading data files...\n",
      "  Successfully loaded network with 2319 nodes and 435 edges\n",
      "\n",
      "Checking network: article_2\n",
      "  Path: /Users/davidwickerhf/Projects/work/maastrichtuniversity/rankings/networks/merged/split-balanced-importance/article_2\n",
      "  Loading data files...\n",
      "  Successfully loaded network with 2476 nodes and 542 edges\n",
      "\n",
      "Checking network: article_P1-3\n",
      "  Path: /Users/davidwickerhf/Projects/work/maastrichtuniversity/rankings/networks/merged/split-balanced-importance/article_P1-3\n",
      "  Loading data files...\n",
      "  Successfully loaded network with 79 nodes and 12 edges\n",
      "\n",
      "Checking network: article_P1-1-1\n",
      "  Path: /Users/davidwickerhf/Projects/work/maastrichtuniversity/rankings/networks/merged/split-balanced-importance/article_P1-1-1\n",
      "  Loading data files...\n",
      "  Successfully loaded network with 692 nodes and 153 edges\n",
      "\n",
      "Checking network: article_35-1\n",
      "  Path: /Users/davidwickerhf/Projects/work/maastrichtuniversity/rankings/networks/merged/split-balanced-importance/article_35-1\n",
      "  Loading data files...\n",
      "  Successfully loaded network with 975 nodes and 240 edges\n",
      "\n",
      "Checking network: article_10-1\n",
      "  Path: /Users/davidwickerhf/Projects/work/maastrichtuniversity/rankings/networks/merged/split-balanced-importance/article_10-1\n",
      "  Loading data files...\n",
      "  Successfully loaded network with 525 nodes and 97 edges\n",
      "\n",
      "Checking network: article_6-3\n",
      "  Path: /Users/davidwickerhf/Projects/work/maastrichtuniversity/rankings/networks/merged/split-balanced-importance/article_6-3\n",
      "  Loading data files...\n",
      "  Successfully loaded network with 386 nodes and 77 edges\n",
      "\n",
      "Checking network: article_2-2\n",
      "  Path: /Users/davidwickerhf/Projects/work/maastrichtuniversity/rankings/networks/merged/split-balanced-importance/article_2-2\n",
      "  Loading data files...\n",
      "  Successfully loaded network with 85 nodes and 20 edges\n",
      "\n",
      "Checking network: article_8-2\n",
      "  Path: /Users/davidwickerhf/Projects/work/maastrichtuniversity/rankings/networks/merged/split-balanced-importance/article_8-2\n",
      "  Loading data files...\n",
      "  Successfully loaded network with 551 nodes and 103 edges\n",
      "\n",
      "Checking network: article_6-2\n",
      "  Path: /Users/davidwickerhf/Projects/work/maastrichtuniversity/rankings/networks/merged/split-balanced-importance/article_6-2\n",
      "  Loading data files...\n",
      "  Successfully loaded network with 347 nodes and 69 edges\n",
      "\n",
      "Checking network: article_6-3-c\n",
      "  Path: /Users/davidwickerhf/Projects/work/maastrichtuniversity/rankings/networks/merged/split-balanced-importance/article_6-3-c\n",
      "  Loading data files...\n",
      "  Successfully loaded network with 194 nodes and 36 edges\n",
      "\n",
      "Checking network: article_13\n",
      "  Path: /Users/davidwickerhf/Projects/work/maastrichtuniversity/rankings/networks/merged/split-balanced-importance/article_13\n",
      "  Loading data files...\n",
      "  Successfully loaded network with 949 nodes and 164 edges\n",
      "\n",
      "Checking network: article_5-1-f\n",
      "  Path: /Users/davidwickerhf/Projects/work/maastrichtuniversity/rankings/networks/merged/split-balanced-importance/article_5-1-f\n",
      "  Loading data files...\n",
      "  Successfully loaded network with 73 nodes and 20 edges\n",
      "\n",
      "Checking network: article_14\n",
      "  Path: /Users/davidwickerhf/Projects/work/maastrichtuniversity/rankings/networks/merged/split-balanced-importance/article_14\n",
      "  Loading data files...\n",
      "  Successfully loaded network with 528 nodes and 69 edges\n",
      "\n",
      "Checking network: article_6-3-d\n",
      "  Path: /Users/davidwickerhf/Projects/work/maastrichtuniversity/rankings/networks/merged/split-balanced-importance/article_6-3-d\n",
      "  Loading data files...\n",
      "  Successfully loaded network with 134 nodes and 18 edges\n",
      "\n",
      "Checking network: article_46\n",
      "  Path: /Users/davidwickerhf/Projects/work/maastrichtuniversity/rankings/networks/merged/split-balanced-importance/article_46\n",
      "  Loading data files...\n",
      "  Successfully loaded network with 204 nodes and 49 edges\n",
      "\n",
      "Checking network: article_41\n",
      "  Path: /Users/davidwickerhf/Projects/work/maastrichtuniversity/rankings/networks/merged/split-balanced-importance/article_41\n",
      "  Loading data files...\n",
      "  Successfully loaded network with 3595 nodes and 885 edges\n",
      "\n",
      "Checking network: article_P4-2\n",
      "  Path: /Users/davidwickerhf/Projects/work/maastrichtuniversity/rankings/networks/merged/split-balanced-importance/article_P4-2\n",
      "  Loading data files...\n",
      "  Successfully loaded network with 61 nodes and 12 edges\n",
      "\n",
      "Checking network: article_11-1\n",
      "  Path: /Users/davidwickerhf/Projects/work/maastrichtuniversity/rankings/networks/merged/split-balanced-importance/article_11-1\n",
      "  Loading data files...\n",
      "  Successfully loaded network with 188 nodes and 22 edges\n",
      "\n",
      "Checking network: article_6-3-b\n",
      "  Path: /Users/davidwickerhf/Projects/work/maastrichtuniversity/rankings/networks/merged/split-balanced-importance/article_6-3-b\n",
      "  Loading data files...\n",
      "  Successfully loaded network with 81 nodes and 21 edges\n",
      "\n",
      "Checking network: article_37\n",
      "  Path: /Users/davidwickerhf/Projects/work/maastrichtuniversity/rankings/networks/merged/split-balanced-importance/article_37\n",
      "  Loading data files...\n",
      "  Successfully loaded network with 229 nodes and 27 edges\n",
      "\n",
      "Checking network: article_9-1\n",
      "  Path: /Users/davidwickerhf/Projects/work/maastrichtuniversity/rankings/networks/merged/split-balanced-importance/article_9-1\n",
      "  Loading data files...\n",
      "  Successfully loaded network with 105 nodes and 17 edges\n",
      "\n",
      "Checking network: article_39\n",
      "  Path: /Users/davidwickerhf/Projects/work/maastrichtuniversity/rankings/networks/merged/split-balanced-importance/article_39\n",
      "  Loading data files...\n",
      "  Successfully loaded network with 106 nodes and 6 edges\n",
      "\n",
      "Checking network: article_5-3\n",
      "  Path: /Users/davidwickerhf/Projects/work/maastrichtuniversity/rankings/networks/merged/split-balanced-importance/article_5-3\n",
      "  Loading data files...\n",
      "  Successfully loaded network with 801 nodes and 134 edges\n",
      "\n",
      "Checking network: article_5-4\n",
      "  Path: /Users/davidwickerhf/Projects/work/maastrichtuniversity/rankings/networks/merged/split-balanced-importance/article_5-4\n",
      "  Loading data files...\n",
      "  Successfully loaded network with 373 nodes and 57 edges\n",
      "\n",
      "Checking network: article_38\n",
      "  Path: /Users/davidwickerhf/Projects/work/maastrichtuniversity/rankings/networks/merged/split-balanced-importance/article_38\n",
      "  Loading data files...\n",
      "  Successfully loaded network with 114 nodes and 3 edges\n",
      "\n",
      "Checking network: article_7-1\n",
      "  Path: /Users/davidwickerhf/Projects/work/maastrichtuniversity/rankings/networks/merged/split-balanced-importance/article_7-1\n",
      "  Loading data files...\n",
      "  Successfully loaded network with 319 nodes and 40 edges\n",
      "\n",
      "Checking network: article_5-5\n",
      "  Path: /Users/davidwickerhf/Projects/work/maastrichtuniversity/rankings/networks/merged/split-balanced-importance/article_5-5\n",
      "  Loading data files...\n",
      "  Successfully loaded network with 105 nodes and 19 edges\n",
      "\n",
      "Checking network: article_5-2\n",
      "  Path: /Users/davidwickerhf/Projects/work/maastrichtuniversity/rankings/networks/merged/split-balanced-importance/article_5-2\n",
      "  Loading data files...\n",
      "  Successfully loaded network with 62 nodes and 10 edges\n",
      "\n",
      "Checking network: article_37-1-c\n",
      "  Path: /Users/davidwickerhf/Projects/work/maastrichtuniversity/rankings/networks/merged/split-balanced-importance/article_37-1-c\n",
      "  Loading data files...\n",
      "  Successfully loaded network with 60 nodes and 7 edges\n",
      "\n",
      "Checking network: article_9\n",
      "  Path: /Users/davidwickerhf/Projects/work/maastrichtuniversity/rankings/networks/merged/split-balanced-importance/article_9\n",
      "  Loading data files...\n",
      "  Successfully loaded network with 791 nodes and 235 edges\n",
      "\n",
      "Checking network: article_P1-1\n",
      "  Path: /Users/davidwickerhf/Projects/work/maastrichtuniversity/rankings/networks/merged/split-balanced-importance/article_P1-1\n",
      "  Loading data files...\n",
      "  Successfully loaded network with 907 nodes and 180 edges\n",
      "\n",
      "Checking network: article_35-3\n",
      "  Path: /Users/davidwickerhf/Projects/work/maastrichtuniversity/rankings/networks/merged/split-balanced-importance/article_35-3\n",
      "  Loading data files...\n",
      "  Successfully loaded network with 442 nodes and 88 edges\n",
      "\n",
      "Checking network: article_35-3-a\n",
      "  Path: /Users/davidwickerhf/Projects/work/maastrichtuniversity/rankings/networks/merged/split-balanced-importance/article_35-3-a\n",
      "  Loading data files...\n",
      "  Successfully loaded network with 233 nodes and 45 edges\n",
      "\n",
      "Checking network: article_7\n",
      "  Path: /Users/davidwickerhf/Projects/work/maastrichtuniversity/rankings/networks/merged/split-balanced-importance/article_7\n",
      "  Loading data files...\n",
      "  Successfully loaded network with 446 nodes and 52 edges\n",
      "\n",
      "Checking network: article_10-2\n",
      "  Path: /Users/davidwickerhf/Projects/work/maastrichtuniversity/rankings/networks/merged/split-balanced-importance/article_10-2\n",
      "  Loading data files...\n",
      "  Successfully loaded network with 425 nodes and 97 edges\n",
      "\n",
      "Checking network: article_37-1\n",
      "  Path: /Users/davidwickerhf/Projects/work/maastrichtuniversity/rankings/networks/merged/split-balanced-importance/article_37-1\n",
      "  Loading data files...\n",
      "  Successfully loaded network with 226 nodes and 25 edges\n",
      "\n",
      "Checking network: article_1\n",
      "  Path: /Users/davidwickerhf/Projects/work/maastrichtuniversity/rankings/networks/merged/split-balanced-importance/article_1\n",
      "  Loading data files...\n",
      "  Successfully loaded network with 5938 nodes and 1108 edges\n",
      "\n",
      "Checking network: article_6\n",
      "  Path: /Users/davidwickerhf/Projects/work/maastrichtuniversity/rankings/networks/merged/split-balanced-importance/article_6\n",
      "  Loading data files...\n",
      "  Successfully loaded network with 3147 nodes and 577 edges\n",
      "\n",
      "Checking network: article_8\n",
      "  Path: /Users/davidwickerhf/Projects/work/maastrichtuniversity/rankings/networks/merged/split-balanced-importance/article_8\n",
      "  Loading data files...\n",
      "  Successfully loaded network with 1267 nodes and 190 edges\n",
      "\n",
      "Checking network: article_P1-1-2\n",
      "  Path: /Users/davidwickerhf/Projects/work/maastrichtuniversity/rankings/networks/merged/split-balanced-importance/article_P1-1-2\n",
      "  Loading data files...\n",
      "  Successfully loaded network with 133 nodes and 23 edges\n",
      "\n",
      "Successfully loaded 62 networks\n",
      "\n",
      "Analyzing network: article_1\n",
      "\n",
      "Analyzing ground truth: importance\n",
      "Plotted 13 centrality vs ground truth plots\n",
      "Best high correlation: in_degree_centrality\n",
      "Best low correlation: relative_in_degree_centrality\n",
      "\n",
      "Analyzing ground truth: doctypebranch\n",
      "Plotted 13 centrality vs ground truth plots\n",
      "Best high correlation: in_degree_centrality\n",
      "Best low correlation: relative_in_degree_centrality\n",
      "\n",
      "Analyzing network: article_4\n",
      "\n",
      "Analyzing ground truth: importance\n",
      "Plotted 13 centrality vs ground truth plots\n",
      "Best high correlation: disruption\n",
      "Best low correlation: in_degree_centrality\n",
      "\n",
      "Analyzing ground truth: doctypebranch\n",
      "Plotted 13 centrality vs ground truth plots\n",
      "Best high correlation: in_degree_centrality\n",
      "Best low correlation: relative_in_degree_centrality\n",
      "\n",
      "Analyzing network: article_3\n",
      "\n",
      "Analyzing ground truth: importance\n",
      "Plotted 13 centrality vs ground truth plots\n",
      "Best high correlation: in_degree_centrality\n",
      "Best low correlation: relative_in_degree_centrality\n",
      "\n",
      "Analyzing ground truth: doctypebranch\n",
      "Plotted 13 centrality vs ground truth plots\n",
      "Best high correlation: in_degree_centrality\n",
      "Best low correlation: relative_in_degree_centrality\n",
      "TESTING CROSS-NETWORK CORRELATIONS\n",
      "Found networks: ['article_1', 'article_4', 'article_3']\n",
      "\n",
      "Found centralities: ['betweenness_centrality', 'closeness_centrality', 'core_number', 'degree_centrality', 'disruption', 'eigenvector_centrality', 'harmonic_centrality', 'hits_authority', 'hits_hub', 'in_degree_centrality', 'out_degree_centrality', 'pagerank', 'relative_in_degree_centrality']\n",
      "\n",
      "Found ground truths: ['doctypebranch', 'importance']\n",
      "\n",
      "Initial DataFrame:\n",
      "                                                  Centrality article_1  \\\n",
      "betweenness_centrality                betweenness_centrality       NaN   \n",
      "closeness_centrality                    closeness_centrality       NaN   \n",
      "core_number                                      core_number       NaN   \n",
      "degree_centrality                          degree_centrality       NaN   \n",
      "disruption                                        disruption       NaN   \n",
      "eigenvector_centrality                eigenvector_centrality       NaN   \n",
      "harmonic_centrality                      harmonic_centrality       NaN   \n",
      "hits_authority                                hits_authority       NaN   \n",
      "hits_hub                                            hits_hub       NaN   \n",
      "in_degree_centrality                    in_degree_centrality       NaN   \n",
      "out_degree_centrality                  out_degree_centrality       NaN   \n",
      "pagerank                                            pagerank       NaN   \n",
      "relative_in_degree_centrality  relative_in_degree_centrality       NaN   \n",
      "\n",
      "                              article_3 article_4  \n",
      "betweenness_centrality              NaN       NaN  \n",
      "closeness_centrality                NaN       NaN  \n",
      "core_number                         NaN       NaN  \n",
      "degree_centrality                   NaN       NaN  \n",
      "disruption                          NaN       NaN  \n",
      "eigenvector_centrality              NaN       NaN  \n",
      "harmonic_centrality                 NaN       NaN  \n",
      "hits_authority                      NaN       NaN  \n",
      "hits_hub                            NaN       NaN  \n",
      "in_degree_centrality                NaN       NaN  \n",
      "out_degree_centrality               NaN       NaN  \n",
      "pagerank                            NaN       NaN  \n",
      "relative_in_degree_centrality       NaN       NaN  \n",
      "\n",
      "Filling data for network: article_1\n",
      "\n",
      "Filling data for network: article_3\n",
      "\n",
      "Filling data for network: article_4\n",
      "\n",
      "Final DataFrame:\n",
      "                                                  Centrality  \\\n",
      "betweenness_centrality                betweenness_centrality   \n",
      "closeness_centrality                    closeness_centrality   \n",
      "core_number                                      core_number   \n",
      "degree_centrality                          degree_centrality   \n",
      "disruption                                        disruption   \n",
      "eigenvector_centrality                eigenvector_centrality   \n",
      "harmonic_centrality                      harmonic_centrality   \n",
      "hits_authority                                hits_authority   \n",
      "hits_hub                                            hits_hub   \n",
      "in_degree_centrality                    in_degree_centrality   \n",
      "out_degree_centrality                  out_degree_centrality   \n",
      "pagerank                                            pagerank   \n",
      "relative_in_degree_centrality  relative_in_degree_centrality   \n",
      "\n",
      "                                                                  article_1  \\\n",
      "betweenness_centrality         Doctypebranch: -0.0506 | Importance: -0.0519   \n",
      "closeness_centrality           Doctypebranch: -0.3631 | Importance: -0.4191   \n",
      "core_number                    Doctypebranch: -0.3419 | Importance: -0.3192   \n",
      "degree_centrality              Doctypebranch: -0.3411 | Importance: -0.3185   \n",
      "disruption                     Doctypebranch: -0.3101 | Importance: -0.3708   \n",
      "eigenvector_centrality         Doctypebranch: -0.3491 | Importance: -0.4150   \n",
      "harmonic_centrality            Doctypebranch: -0.3619 | Importance: -0.4186   \n",
      "hits_authority                 Doctypebranch: -0.2769 | Importance: -0.3049   \n",
      "hits_hub                        Doctypebranch: -0.0385 | Importance: 0.0554   \n",
      "in_degree_centrality           Doctypebranch: -0.3701 | Importance: -0.4223   \n",
      "out_degree_centrality           Doctypebranch: -0.0576 | Importance: 0.0246   \n",
      "pagerank                       Doctypebranch: -0.3628 | Importance: -0.4206   \n",
      "relative_in_degree_centrality  Doctypebranch: -0.3701 | Importance: -0.4223   \n",
      "\n",
      "                                                                  article_3  \\\n",
      "betweenness_centrality         Doctypebranch: -0.0308 | Importance: -0.0511   \n",
      "closeness_centrality           Doctypebranch: -0.3267 | Importance: -0.3734   \n",
      "core_number                    Doctypebranch: -0.2926 | Importance: -0.2763   \n",
      "degree_centrality              Doctypebranch: -0.2932 | Importance: -0.2765   \n",
      "disruption                     Doctypebranch: -0.2925 | Importance: -0.3379   \n",
      "eigenvector_centrality         Doctypebranch: -0.3167 | Importance: -0.3691   \n",
      "harmonic_centrality            Doctypebranch: -0.3258 | Importance: -0.3732   \n",
      "hits_authority                 Doctypebranch: -0.2657 | Importance: -0.2738   \n",
      "hits_hub                        Doctypebranch: -0.0235 | Importance: 0.0403   \n",
      "in_degree_centrality           Doctypebranch: -0.3327 | Importance: -0.3760   \n",
      "out_degree_centrality           Doctypebranch: -0.0239 | Importance: 0.0412   \n",
      "pagerank                       Doctypebranch: -0.3278 | Importance: -0.3757   \n",
      "relative_in_degree_centrality  Doctypebranch: -0.3327 | Importance: -0.3760   \n",
      "\n",
      "                                                                  article_4  \n",
      "betweenness_centrality           Doctypebranch: 0.0084 | Importance: 0.0309  \n",
      "closeness_centrality           Doctypebranch: -0.3403 | Importance: -0.3579  \n",
      "core_number                    Doctypebranch: -0.2539 | Importance: -0.1548  \n",
      "degree_centrality              Doctypebranch: -0.2511 | Importance: -0.1536  \n",
      "disruption                     Doctypebranch: -0.3184 | Importance: -0.3658  \n",
      "eigenvector_centrality         Doctypebranch: -0.3172 | Importance: -0.3543  \n",
      "harmonic_centrality            Doctypebranch: -0.3386 | Importance: -0.3575  \n",
      "hits_authority                 Doctypebranch: -0.2758 | Importance: -0.2723  \n",
      "hits_hub                         Doctypebranch: 0.0814 | Importance: 0.2373  \n",
      "in_degree_centrality           Doctypebranch: -0.3514 | Importance: -0.3614  \n",
      "out_degree_centrality            Doctypebranch: 0.0756 | Importance: 0.2066  \n",
      "pagerank                       Doctypebranch: -0.3371 | Importance: -0.3579  \n",
      "relative_in_degree_centrality  Doctypebranch: -0.3514 | Importance: -0.3614  \n",
      "\n",
      "Correlation matrix saved to results/test/full/comparisons/centrality_correlations_matrix.csv\n"
     ]
    }
   ],
   "source": [
    "# TEST\n",
    "network_results = {}\n",
    "\n",
    "splitTest = load_networks('networks/merged/split-balanced-importance')\n",
    "# Sort networks by size and take the 8 biggest ones\n",
    "splitTest = dict(sorted(splitTest.items(), key=lambda x: len(x[1]['nodes']), reverse=True)[:3])\n",
    "\n",
    "\n",
    "\n",
    "# Analyze Networks\n",
    "for network_name, data in splitTest.items():\n",
    "    print(f\"\\nAnalyzing network: {network_name}\")\n",
    "    results = analyze_network(\n",
    "        nodes_df=data['nodes'],\n",
    "        edges_df=data['edges'],\n",
    "        ground_truths=GROUND_TRUTHS,\n",
    "        centralities=CENTRALITIES,\n",
    "        composite_functions=['weight_composite_ranking',],\n",
    "        output_path=f'results/test/split-balanced-importance/{network_name}'\n",
    "    )\n",
    "    network_results[network_name] = results\n",
    "\n",
    "# Before comparison\n",
    "if not network_results:\n",
    "    raise ValueError(\"No networks were successfully analyzed. Check if data/split directory contains valid network data.\")\n",
    "\n",
    "# Compare results across networks\n",
    "analysis = compare_networks(network_results, 'results/test/split-balanced-importance/comparisons')\n",
    "\n",
    "# Visualize results\n",
    "visualize_network_commonalities(analysis, 'results/test/split-balanced-importance/comparisons')\n",
    "\n",
    "\n",
    "\n",
    "# FOR TESTING, SELECT THE 3 BIGGEST NETWORKS\n",
    "print(\"TESTING CROSS-NETWORK CORRELATIONS\")\n",
    "# Sort networks by size and take the 3 biggest ones\n",
    "\n",
    "# Analyze Networks\n",
    "analysis = compare_networks(network_results, 'results/test/full/comparisons')\n",
    "\n",
    "# Visualize results\n",
    "visualize_network_commonalities(analysis, 'results/test/full/comparisons', print_correlations=True, compare_all=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Analyse whole networks (balanced and unbalanced)\n",
    "# network_results = {}\n",
    "\n",
    "# networks = load_networks('networks/full-balanced-importance')\n",
    "# networks.update(load_networks('networks/full-unbalanced'))\n",
    "# networks.update(load_networks('networks/full-balanced-doctypebranch'))\n",
    "\n",
    "# # Analyze Networks\n",
    "# for network_name, data in networks.items():\n",
    "#     print(f\"\\nAnalyzing network: {network_name}\")\n",
    "#     results = analyze_network(\n",
    "#         nodes_df=data['nodes'],\n",
    "#         edges_df=data['edges'],\n",
    "#         ground_truths=GROUND_TRUTHS,\n",
    "#         centralities=CENTRALITIES,\n",
    "#         composite_functions=['weight_composite_ranking',],\n",
    "#         output_path=f'results/branch-update/full/{network_name}'\n",
    "#     )\n",
    "#     network_results[network_name] = results\n",
    "\n",
    "\n",
    "\n",
    "# # Before comparison\n",
    "# if not network_results:\n",
    "#     raise ValueError(\"No networks were successfully analyzed. Check if data/split directory contains valid network data.\")\n",
    "\n",
    "\n",
    "# # Compare results across networks\n",
    "# analysis = compare_networks(network_results, 'results/branch-update/full/comparisons')\n",
    "\n",
    "# # Visualize results\n",
    "# visualize_network_commonalities(analysis, 'results/branch-update/full/comparisons')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # ANALYSE SPLIT NETWORKS\n",
    "# # UNBALANCED\n",
    "# network_results = {}\n",
    "# networks = load_networks('networks/unbalanced')\n",
    "\n",
    "# # Analyze Networks\n",
    "# for network_name, data in networks.items():\n",
    "#     print(f\"\\nAnalyzing network: {network_name}\")\n",
    "#     results = analyze_network(\n",
    "#         nodes_df=data['nodes'],\n",
    "#         edges_df=data['edges'],\n",
    "#         ground_truths=GROUND_TRUTHS,\n",
    "#         centralities=CENTRALITIES,\n",
    "#         composite_functions=['weight_composite_ranking',],\n",
    "#         output_path=f'results/branch-update/unbalanced/{network_name}'\n",
    "#     )\n",
    "#     network_results[network_name] = results\n",
    "\n",
    "# # Before comparison\n",
    "# if not network_results:\n",
    "#     raise ValueError(\"No networks were successfully analyzed. Check if data/split directory contains valid network data.\")\n",
    "\n",
    "\n",
    "# # Compare results across networks\n",
    "# analysis = compare_networks(network_results, 'results/branch-update/unbalanced/comparisons')\n",
    "\n",
    "# # Visualize results\n",
    "# visualize_network_commonalities(analysis, 'results/branch-update/unbalanced/comparisons')\n",
    "\n",
    "\n",
    "\n",
    "# # BALANCED BY IMPORRTANCE \n",
    "# network_results = {}\n",
    "# networks = load_networks('networks/balanced-importance')\n",
    "\n",
    "# # Analyze Networks\n",
    "# for network_name, data in networks.items():\n",
    "#     print(f\"\\nAnalyzing network: {network_name}\")\n",
    "#     results = analyze_network(\n",
    "#         nodes_df=data['nodes'],\n",
    "#         edges_df=data['edges'],\n",
    "#         ground_truths=GROUND_TRUTHS,\n",
    "#         centralities=CENTRALITIES,\n",
    "#         composite_functions=['weight_composite_ranking',],\n",
    "#         output_path=f'results/branch-update/balanced-importance/{network_name}'\n",
    "#     )\n",
    "#     network_results[network_name] = results\n",
    "\n",
    "# # Before comparison\n",
    "# if not network_results:\n",
    "#     raise ValueError(\"No networks were successfully analyzed. Check if data/split directory contains valid network data.\")\n",
    "\n",
    "\n",
    "# # Compare results across networks\n",
    "# analysis = compare_networks(network_results, 'results/branch-update/balanced-importance/comparisons')\n",
    "\n",
    "# # Visualize results\n",
    "# visualize_network_commonalities(analysis, 'results/branch-update/balanced-importance/comparisons')\n",
    "\n",
    "\n",
    "\n",
    "# # BALANCED BY DOCTYPEBRANCH\n",
    "# network_results = {}\n",
    "# networks = load_networks('networks/balanced-doctypebranch')\n",
    "\n",
    "# # Analyze Networks\n",
    "# for network_name, data in networks.items():\n",
    "#     print(f\"\\nAnalyzing network: {network_name}\")\n",
    "#     results = analyze_network(\n",
    "#         nodes_df=data['nodes'],\n",
    "#         edges_df=data['edges'],\n",
    "#         ground_truths=GROUND_TRUTHS,\n",
    "#         centralities=CENTRALITIES,\n",
    "#         composite_functions=['weight_composite_ranking',],\n",
    "#         output_path=f'results/balanced-doctypebranch/{network_name}'\n",
    "#     )\n",
    "#     network_results[network_name] = results\n",
    "\n",
    "# # Before comparison\n",
    "# if not network_results:\n",
    "#     raise ValueError(\"No networks were successfully analyzed. Check if data/split directory contains valid network data.\")\n",
    "\n",
    "\n",
    "# # Compare results across networks\n",
    "# analysis = compare_networks(network_results, 'results/balanced-doctypebranch/comparisons')\n",
    "\n",
    "# # Visualize results\n",
    "# visualize_network_commonalities(analysis, 'results/balanced-doctypebranch/comparisons')  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
